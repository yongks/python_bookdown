[["index.html", "Python Bookdown About This Book Motivation Building The Book", " Python Bookdown Yong Keh Soon 2022-12-29 About This Book Motivation This is a a python cookbook, serving as a quick reference to python and its popular library. The content covers python fundamentals, data manipulation, plotting, machine learning and sentiment analysis. The main motivation of the book is for my personal reference. Therefore, it is written in a manner according to my thinking process. If you are like me who work on python as part-time basis, this book may help you refresh your memories quickly. Building The Book Although this book is written for python code, it was contructed sing Bookdown (R Language). This is made possible with reticulate R library for integrating Python in R. All content pages of the book are written as RMarkdown files, rendered by Bookdown to BS4 (bootstrap 4, with 3 columns layout) output format. Renders output goes into “/doc” folder, which was uploaded to github. **/doc** folder in Github served as Github Pages, which you are reading right now. Ingridients Here are the key ingredients used to build this book. R Language RMarkown Bookdown RStudio IDE Github Pages Environment The book is build on below Python environment: Anaconda with “base” environment Python 3.9.13 pandas (1.4.4) numpy (1.23.4) scipy (1.9.3) scikit-learn (1.1.3) seaborn (0.12.1) matplotlib (3.6.2) plotnine (0.10.1) nltk (3.7) statsmodel (0.13.2) bs4 (4.11.1) "],["fundamentals.html", "Chapter 1 Fundamentals 1.1 Library Management 1.2 Variables Are Objects 1.3 Assignment", " Chapter 1 Fundamentals 1.1 Library Management 1.1.1 Built-In Libraries import string import datetime as dt import os #os.environ[&#39;QT_QPA_PLATFORM_PLUGIN_PATH&#39;] = &quot;C:\\ProgramData\\Anaconda3\\Library\\plugins\\platforms&quot; 1.1.2 External Libraries Here are some of the popular external libraries. numpy - large multi-dimensional array and matrices - High level mathematical funcitons to operate on them - Efficient array computation, modeled after matlab - Support vectorized array math functions (built on C, hence faster than python for loop and list) scipy - Collection of mathematical algorithms and convenience functions built on the numpy extension - Built upon numpy Pandas - Data manipulation and analysis - Offer data structures and operations for manipulating numerical tables and time series - Good for analyzing tabular data - Use for exploratory data analysis, data pre-processing, statistics and visualization - Built upon numpy scikit-learn - Machine learning functions - Built on top of scipy matplotlib - Data Visualization 1.1.3 Package Management 1.1.4 Anaconda Conda Environment Anaconda is a popular package management system for python. Interaction with anaconda is through command prompt “conda”. conda info ## check the installed conda version and directories conda list ## list all installed python modules, and its version Package Installation Conda is recommended distribution. To install from official conda channel: ## Syntax conda install &lt;package_name&gt; # always install latest conda install &lt;package_name=version_number&gt; ## install specific version ## Example conda install scipy ## official channel conda install scipy=1.2.3 ## official channel To install from conda-forge community channel: conda install -c conda-forge &lt;package_name&gt; conda install -c conda-forge &lt;package_name=version_number&gt; ## Example: Install From conda community: conda install -c conda-forge plotnine conda install -c conda-forge plotnine=1.2.3 1.1.5 PIP PIP is python open repository (not part of conda). Use pip if the package is not available in conda. Package Version pip list ## list all installed module 1.1.5.1 Package Installation pip install &lt;package_name&gt; pip install &lt;package_name=version_numner&gt; ## Example: pip install plydata pip install plydata=1.2.3 1.2 Variables Are Objects All variables in python are objects Every variable assginment is reference based, that is, each object value is the reference to memory block of data In the below example, a, b and c refer to the same memory location: Notice when an object assigned to another object, they refer to the same memory location When two variable refers to the same value, they refer to the same memory location a = 123 b = 123 c = a print (&#39;Data of a =&#39;, a, &#39;\\nData of b =&#39;,b, &#39;\\nData of c =&#39;,c, &#39;\\nID of a = &#39;, id(a), &#39;\\nID of b = &#39;, id(b), &#39;\\nID of c = &#39;, id(c) ) ## Data of a = 123 ## Data of b = 123 ## Data of c = 123 ## ID of a = 1510699456688 ## ID of b = 1510699456688 ## ID of c = 1510699456688 Changing data value (using assignment) changes the reference a = 123 b = a a = 456 # reassignemnt changed a memory reference # b memory reference not changed print (&#39;Data of a =&#39;,a, &#39;\\nData of b =&#39;,b, &#39;\\nID of a = &#39;, id(a), &#39;\\nID of b = &#39;, id(b) ) ## Data of a = 456 ## Data of b = 123 ## ID of a = 1511014930160 ## ID of b = 1510699456688 1.3 Assignment 1.3.1 Multiple Assignment Assign multiple variable at the same time with same value. Note that all object created using this method refer to the same memory location. x = y = &#39;same mem loc&#39; print (&#39;x = &#39;, x, &#39;\\ny = &#39;, y, &#39;\\nid(x) = &#39;, id(x), &#39;\\nid(y) = &#39;, id(y) ) ## x = same mem loc ## y = same mem loc ## id(x) = 1511015054512 ## id(y) = 1511015054512 1.3.2 Augmented Assignment x = 1 y = x + 1 y += 1 print (&#39;y = &#39;, y) ## y = 3 1.3.3 Unpacking Assingment Assign multiple value to multiple variabels at the same time. x,y = 1,3 print (x,y) ## 1 3 "],["built-in-data-types.html", "Chapter 2 Built-in Data Types 2.1 Numbers 2.2 String 2.3 Boolean 2.4 None", " Chapter 2 Built-in Data Types 2.1 Numbers Two types of built-in number type, integer and float. 2.1.1 Integer n = 123 type (n) ## &lt;class &#39;int&#39;&gt; 2.1.2 Float f = 123.4 type (f) ## &lt;class &#39;float&#39;&gt; 2.1.3 Number Operators In general, when the operation potentially return float, the result is float type. Otherwise it return integer. Division always return float print(4/2) # return float ## 2.0 type(4/2) ## &lt;class &#39;float&#39;&gt; Integer Division by integer return inter. Integer division by float return float. print (8//3,&#39;\\n&#39;, # return int 8//3.2) # return float ## 2 ## 2.0 Remainder by integer return integer. Remainder by float return float print (8%3, &#39;\\n&#39;, # return int 8%3.2) # return float ## 2 ## 1.5999999999999996 Power return int or float print (2**3) # return int ## 8 print (2.1**3) # return float ## 9.261000000000001 print (2**3.1) # return float ## 8.574187700290345 2.2 String String is an object class ‘str’. It is an ordered collection of letters, an array of object type str import string s = &#39;abcde&#39; print( &#39;\\nvar type = &#39;, type(s), &#39;\\nelems = &#39;,s[0], s[1], s[2], &#39;\\nlen = &#39;, len(s), &#39;\\nelem type = &#39;,type(s[1])) ## ## var type = &lt;class &#39;str&#39;&gt; ## elems = a b c ## len = 5 ## elem type = &lt;class &#39;str&#39;&gt; 2.2.1 Constructor 2.2.1.1 Classical Method class str(object='') my_string = str() ## empty string class str(object=b'', encoding='utf-8', errors='strict') my_string = str(&#39;abc&#39;) 2.2.1.2 Shortcut Method my_string = &#39;abc&#39; 2.2.1.3 Multiline Method my_string = &#39;&#39;&#39; This is me. Yong Keh Soon &#39;&#39;&#39; print(my_string) ## ## This is me. ## Yong Keh Soon Note that the variable contain \\n front and end of the string. my_string ## &#39;\\nThis is me.\\nYong Keh Soon\\n&#39; 2.2.1.4 Immutability String is immuatable. Changing its content will result in error s = &#39;abcde&#39; print (&#39;s : &#39;, id(s)) #s[1] = &#39;z&#39; # immutable, result in error ## s : 1572803622384 Changing the variable completley change the reference (for new object) s = &#39;efgh&#39; print (&#39;s : &#39;, id(s)) ## s : 1572803626672 2.2.2 Class Constants 2.2.2.1 Letters print( &#39;letters = &#39;, string.ascii_letters, &#39;\\nlowercase = &#39;,string.ascii_lowercase, &#39;\\nuppercase = &#39;,string.ascii_uppercase ) ## letters = abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ## lowercase = abcdefghijklmnopqrstuvwxyz ## uppercase = ABCDEFGHIJKLMNOPQRSTUVWXYZ 2.2.2.2 Digits string.digits ## &#39;0123456789&#39; 2.2.2.3 White Spaces string.whitespace ## &#39; \\t\\n\\r\\x0b\\x0c&#39; 2.2.3 Instance Methods 2.2.3.1 Substitution : format() By Positional print( &#39;{} + {} = {}&#39;.format(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), # auto sequence &#39;\\n{0} + {1} = {2}&#39;.format(&#39;aa&#39;, &#39;bb&#39;, &#39;cc&#39;)) # manual sequence ## a + b = c ## aa + bb = cc By Name &#39;Coordinates: {latitude}, {longitude}&#39;.format(latitude=&#39;37.24N&#39;, longitude=&#39;-115.81W&#39;) ## constant ## &#39;Coordinates: 37.24N, -115.81W&#39; By Dictionary Name coord = {&#39;latitude&#39;: &#39;37.24N&#39;, &#39;longitude&#39;: &#39;-115.81W&#39;} ## dictionary key/value &#39;Coordinates: {latitude}, {longitude}&#39;.format(**coord) ## &#39;Coordinates: 37.24N, -115.81W&#39; Formatting Number Float &#39;{:+f}; {:+f}&#39;.format(3.14, -3.14) # show it always ## &#39;+3.140000; -3.140000&#39; &#39;{: f}; {: f}&#39;.format(3.14, -3.14) # show a space for positive numbers ## &#39; 3.140000; -3.140000&#39; &#39;Correct answers: {:.2f}&#39;.format(55676.345345) ## &#39;Correct answers: 55676.35&#39; Integer, Percentage &#39;{0:,} {0:.2%} {0:,.2%}&#39;.format(1234567890.4455) ## &#39;1,234,567,890.4455 123456789044.55% 123,456,789,044.55%&#39; Alignment &#39;{0:&lt;20} {0:&lt;&lt;20}&#39;.format(&#39;left aligned&#39;) ## &#39;left aligned left aligned&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&#39; &#39;{0:&gt;20} {0:$&gt;20}&#39;.format(&#39;right aligned&#39;) ## &#39; right aligned $$$$$$$right aligned&#39; &#39;{:^30}&#39;.format(&#39;centered&#39;) # use &#39;*&#39; as a fill char ## &#39; centered &#39; 2.2.3.2 Substitution : f-string my_name = &#39;Yong Keh Soon&#39; salary = 11123.346 f&#39;Hello, {my_name}, your salary is {salary:,.2f} !&#39; ## &#39;Hello, Yong Keh Soon, your salary is 11,123.35 !&#39; 2.2.3.3 Conversion: upper() lower() &#39;myEXEel.xls&#39;.upper() ## &#39;MYEXEEL.XLS&#39; &#39;myEXEel.xls&#39;.lower() ## &#39;myexeel.xls&#39; 2.2.3.4 find() pattern position string.find() return position of first occurance. -1 if not found s=&#39;I love karaoke, I know you love it oo&#39; print (s.find(&#39;lov&#39;)) ## 2 print (s.find(&#39;kemuning&#39;)) ## -1 2.2.3.5 strip() off blank spaces filename = &#39; myexce l. xls &#39; filename.strip() ## &#39;myexce l. xls&#39; 2.2.3.6 List Related: split() Splitting delimeter is specified. Observe the empty spaces were conserved in result array animals = &#39;a1,a2 ,a3, a4&#39; animals.split(&#39;,&#39;) ## [&#39;a1&#39;, &#39;a2 &#39;, &#39;a3&#39;, &#39; a4&#39;] 2.2.3.7 List Related: join() &#39;-&#39;.join([&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;]) ## &#39;1-2-3-4&#39; 2.2.3.8 Replacement: .replace() string = &quot;geeks for geeks geeks geeks geeks&quot; # Prints the string by replacing geeks by Geeks print(string.replace(&quot;geeks&quot;, &quot;Geeks&quot;)) # Prints the string by replacing only 3 occurrence of Geeks ## Geeks for Geeks Geeks Geeks Geeks print(string.replace(&quot;geeks&quot;, &quot;GeeksforGeeks&quot;, 3)) ## GeeksforGeeks for GeeksforGeeks GeeksforGeeks geeks geeks 2.2.4 Operator 2.2.4.1 % Old Style Substitution https://docs.python.org/3/library/stdtypes.html#old-string-formatting my_name = &#39;Yong Keh Soon&#39; salary = 11123.346 &#39;Hello, %s, your salary is %.2f !&#39; %(my_name, salary) ## &#39;Hello, Yong Keh Soon, your salary is 11123.35 !&#39; 2.2.4.2 + Concatenation &#39;this is &#39; + &#39;awesome&#39; ## &#39;this is awesome&#39; 2.2.4.3 in matching For single string, partial match print( &#39;abc&#39; in &#39;123abcdefg&#39; ) ## True For list of strings, exact match (even though only one element in list). For partial match, workaround is to convert list to single string print( &#39;abc&#39; in [&#39;abcdefg&#39;], # false &#39;abc&#39; in [&#39;abcdefg&#39;,&#39;123&#39;], # fakse &#39;abc&#39; in [&#39;123&#39;,&#39;abc&#39;,&#39;def&#39;], # true &#39;abc&#39; in str([&#39;123&#39;,&#39;abcdefg&#39;])) # true ## False False True True 2.2.4.4 Comparitor Comparitor compares the memory address. a=&#39;abc&#39; b=&#39;abc&#39; print(&#39;id(a) = &#39;, id(a), &#39;\\nid(b) = &#39;, id(b), &#39;\\na == b &#39;, a==b) ## id(a) = 1572551910384 ## id(b) = 1572551910384 ## a == b True 2.2.5 Iterations string[start:end:step] # default start:0, end:last, step:1 If step is negative (reverse), end value must be lower than start value s = &#39;abcdefghijk&#39; print (s[0]) # first later ## a print (s[:3]) # first 3 letters ## abc print (s[2:8 :2]) # stepping ## ceg print (s[-1]) # last letter ## k print (s[-3:]) # last three letters ## ijk print (s[: :-1]) # reverse everything ## kjihgfedcba print (s[8:2 :-1]) ## ihgfed print (s[8:2]) # return NOTHING 2.3 Boolean b = False if (b): print (&#39;It is true&#39;) else: print (&#39;It is fake&#39;) ## It is fake 2.3.1 What is Considered False ? Everything below are false, anything else are true print ( bool(0), # zero bool(None), # none bool(&#39;&#39;), # empty string bool([]), # empty list bool(()), # empty tupple bool(False), # False bool(2-2)) # expression that return any value above ## False False False False False False False 2.3.2 and operator BEWARE ! and can return different data types If evaluated result is True, the last True Value is returned (because python need to evaluate up to the last value) If evaluated result is False, the first False Value will be returned (because python return it immediately when detecting False value) print (123 and 2 and 1, 123 and [] and 2) ## 1 [] 2.3.3 not operator not (True) ## False not (True or False) ## False not (False) ## True not (True and False) ## True ~(False) ## -1 2.3.4 or operator or can return different data type If evaluated result is True, first True Value will be returned (right hand side value need not be evaluated) If evaluated result is False, last Fasle Value will be returned (need to evalute all items before concluding False) print (1 or 2) ## 1 print (0 or 1 or 1) ## 1 print (0 or () or []) ## [] 2.4 None 2.4.1 None is an Object None is a Python object NonType Any operation to None object will result in error For array data with None elements, verification is required to check through iteration to determine if the item is not None. It is very computaionaly heavy type(None) ## &lt;class &#39;NoneType&#39;&gt; import numpy as np t1 = np.array([1, 2, 3, 4, 5]) t2= np.array([1, 2, 3, None, 4, 5]) print( t1.dtype , &#39;\\n\\n&#39;, # it&#39;s an object t2.dtype) ## int32 ## ## object 2.4.2 Comparing None Not Prefered Method null_variable = None print( null_variable == None ) ## True Prefered print( null_variable is None ) ## True print( null_variable is not None ) ## False 2.4.3 Operation on None Any operator (except is) on None results in error. None &amp; None ## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: unsupported operand type(s) for &amp;: &#39;NoneType&#39; and &#39;NoneType&#39; "],["datetime-standard-library.html", "Chapter 3 datetime Standard Library 3.1 ISO8601 3.2 Module Import 3.3 Class 3.4 date 3.5 date and datetime 3.6 time 3.7 timedelta", " Chapter 3 datetime Standard Library This is a built-in library by Python. There is no need to install this library. 3.1 ISO8601 https://en.wikipedia.org/wiki/ISO_8601#Time_zone_designators 3.1.1 Date Time UTC: \"2007-04-05T14:30Z\" #notice Z GMT+8: \"2007-04-05T12:30+08:00 #notice +08:00 GMT+8: \"2007-04-05T12:30+0800 #notice +0800 GMT+8: \"2007-04-05T12:30+08 #notice +08 3.1.2 Date 2019-02-04 #notice no timezone available 3.2 Module Import from datetime import date # module for date object from datetime import time # module for time object from datetime import datetime # module for datetime object from datetime import timedelta 3.3 Class datetime library contain three class of objects: - date (year,month,day) - time (hour,minute,second) - datetime (year,month,day,hour,minute,second) - timedelta: duration between two datetime or date object 3.4 date 3.4.1 Constructor print( date(2000,1,1) ) ## 2000-01-01 print( date(year=2000,month=1,day=1) ) ## 2000-01-01 print( type(date(year=2000,month=1,day=1))) ## &lt;class &#39;datetime.date&#39;&gt; 3.4.2 Class Method 3.4.2.1 today This is local date (not UTC) date.today() ## datetime.date(2022, 12, 29) print( date.today() ) ## 2022-12-29 3.4.2.2 Convert From ISO fromisoformat strptime is not available for date conversion. It is only for datetime conversion date.fromisoformat(&#39;2011-11-11&#39;) ## datetime.date(2011, 11, 11) To convert non-iso format date string to date object, convert to datetime first, then to date 3.4.3 Instance Method 3.4.3.1 replace() Replace year/month/day with specified parameter, non specified params will remain unchange. Example below change only month. You can change year or day in combination print( date.today() ) ## 2022-12-29 print( date.today().replace(month=8) ) ## 2022-08-29 3.4.3.2 weekday(), isoweekday() For weekday(), Zero being Monday For isoweekday(), Zero being Sunday print( date.today().weekday() ) ## 3 print( date.today().isoweekday() ) ## 4 weekdays = [&#39;Mon&#39;,&#39;Tue&#39;,&#39;Wed&#39;,&#39;Thu&#39;,&#39;Fri&#39;,&#39;Sat&#39;,&#39;Sun&#39;] wd = date.today().weekday() print( date.today(), &quot;is day&quot;, wd ,&quot;which is&quot;, weekdays[wd] ) ## 2022-12-29 is day 3 which is Thu 3.4.3.3 Formating with isoformat() isoformat() return ISO 8601 String (YYYY-MM-DD) date.today().isoformat() # return string ## &#39;2022-12-29&#39; 3.4.3.4 Formating with strftime For complete directive, see below: https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior date.today().strftime(&quot;%m/%d&quot;) ## &#39;12/29&#39; 3.4.3.5 isocalendar() isocalendar return a 3-tuple, (ISO year, ISO week number, ISO weekday). date.today().isocalendar() ## return tuple ## datetime.IsoCalendarDate(year=2022, week=52, weekday=4) 3.4.4 Attributes print( date.today().year ) ## 2022 print( date.today().month ) ## 12 print( date.today().day ) ## 29 3.5 date and datetime 3.5.1 Constructor import datetime as dt print( dt.date(2000,1,1,), &#39;\\n&#39;, dt.datetime(2000,1,1,0,0,0), &#39;\\n&#39;, dt.datetime(year=2000,month=1,day=1,hour=23,minute=15,second=55),&#39;\\n&#39;, type(dt.date(2000,1,1)),&#39;\\n&#39;, type(dt.datetime(2000,1,1,0,0,0))) ## 2000-01-01 ## 2000-01-01 00:00:00 ## 2000-01-01 23:15:55 ## &lt;class &#39;datetime.date&#39;&gt; ## &lt;class &#39;datetime.datetime&#39;&gt; 3.5.2 Class Method 3.5.2.1 now and today Both now() and today() return current system local datetime, no timezone print( dt.datetime.now(), &#39;\\n&#39;, dt.datetime.now().date()) ## 2022-12-29 21:00:25.548970 ## 2022-12-29 dt.datetime.today() ## datetime.datetime(2022, 12, 29, 21, 0, 25, 743140) 3.5.2.2 utcnow dt.datetime.utcnow() ## datetime.datetime(2022, 12, 29, 13, 0, 25, 935106) 3.5.2.3 combine() date and time Apply datetime.combine() module method on both date and time object to get datetime now = dt.datetime.now() dt.datetime.combine(now.date(), now.time()) ## datetime.datetime(2022, 12, 29, 21, 0, 26, 130892) 3.5.2.4 Convert from String strptime() Use strptime to convert string into datetime object %I : 12-hour %H : 24-hour %M : Minute %p : AM/PM %y : 18 %Y : 2018 %b : Mar %m : month (1 to 12) %d : day datetime.strptime(&#39;2011-02-25&#39;,&#39;%Y-%m-%d&#39;) ## datetime.datetime(2011, 2, 25, 0, 0) datetime.strptime(&#39;9-01-18&#39;,&#39;%d-%m-%y&#39;) ## datetime.datetime(2018, 1, 9, 0, 0) datetime.strptime(&#39;09-Mar-2018&#39;,&#39;%d-%b-%Y&#39;) ## datetime.datetime(2018, 3, 9, 0, 0) datetime.strptime(&#39;2/5/2018 4:49 PM&#39;, &#39;%m/%d/%Y %I:%M %p&#39;) ## datetime.datetime(2018, 2, 5, 16, 49) 3.5.2.5 Convert from ISO fromisoformat fromisoformat() is intend to be reverse of isoformat() It actually not ISO compliance: when Z or +8 is included at the end of the string, error occur #s = dt.datetime.now().isoformat() dt.datetime.fromisoformat(&quot;2019-02-05T10:22:33&quot;) ## datetime.datetime(2019, 2, 5, 10, 22, 33) 3.5.3 Instance Method 3.5.3.1 weekday datetime.now().weekday() ## 3 3.5.3.2 replace datetime.now().replace(year=1999) ## datetime.datetime(1999, 12, 29, 21, 0, 27, 569874) 3.5.3.3 convert to .time() datetime.now().time() ## datetime.time(21, 0, 27, 780254) 3.5.3.4 Convert to .date() datetime.now().date() ## datetime.date(2022, 12, 29) 3.5.3.5 Convert to String str str( datetime.now() ) ## &#39;2022-12-29 21:00:28.186639&#39; Use strftime() dt.datetime.now().strftime(&#39;%d-%b-%Y&#39;) ## &#39;29-Dec-2022&#39; dt.datetime.utcnow().strftime(&#39;%Y-%m-%dT%H:%M:%S.%fZ&#39;) ## ISO 8601 UTC ## &#39;2022-12-29T13:00:28.598904Z&#39; Use isoformat() dt.datetime.utcnow().isoformat() ## &#39;2022-12-29T13:00:28.804932&#39; 3.5.4 Attributes print( datetime.now().year ) ## 2022 print( datetime.now().month ) ## 12 print( datetime.now().day ) ## 29 print( datetime.now().hour ) ## 21 print( datetime.now().minute ) ## 0 3.6 time 3.6.1 Constructor print( time(2) ) #default single arugement, hour ## 02:00:00 print( time(2,15) ) #default two arguments, hour, minute ## 02:15:00 print( time(hour=2,minute=15,second=30) ) ## 02:15:30 3.6.2 Class Method 3.6.2.1 now() There is unfortunately no single function to extract the current time. Use time() function of an datetime object datetime.now().time() ## datetime.time(21, 0, 29, 471658) 3.6.3 Attributes print( datetime.now().time().hour ) ## 21 print( datetime.now().time().minute ) ## 0 print( datetime.now().time().second ) ## 29 3.7 timedelta years argument is not supported Apply timedelta on datetime object timedelta cannot be applied on time object , because timedelta potentially go beyond single day (24H) delt = timedelta(days=365,minutes=33,seconds=15) now = datetime.now() print (&#39;delt+now : &#39;, now+delt) ## delt+now : 2023-12-29 21:33:45.103617 "],["built-in-data-structure.html", "Chapter 4 Built-In Data Structure 4.1 Tuple 4.2 List 4.3 Dictionaries 4.4 Sets", " Chapter 4 Built-In Data Structure 4.1 Tuple Tuple is an immutable list. Any attempt to change/update tuple will return error. It can contain different types of object just like list. Benefits of tuple against List are: Tuple is Faster than list Tuple Protects your data against accidental change Can be used as key in dictionaries, list can’t 4.1.1 Creation Tuple is created through assignment with or without brackets. To create tuple from list, use tuple() constructor. t1 = (1,2,3,&#39;o&#39;,(4,5,6)) ## with brackets t2 = 1,2,3,&#39;o&#39;,&#39;apple&#39;, (4,5,6) ## without brackets t3 = tuple([1,2,3,&#39;o&#39;,&#39;apple&#39;, (4,5,6)]) ## create from list using constructor print(type(t1), type(t2), type(t3)) ## &lt;class &#39;tuple&#39;&gt; &lt;class &#39;tuple&#39;&gt; &lt;class &#39;tuple&#39;&gt; 4.1.2 Accessor Assessing single element returns the element. Assessing range of elements returns tuple. print( t1[0], t1[1:3] ) ## 1 (2, 3) 4.1.3 Copy and Clone Use normal assignment = to duplicate. Reference of the memory address is copied. Data is actually not duplicated in memory. To clone an tuple (different ID), convert to list then back to tuple again. ## Copy actually points to the same memory location original = (1,2,3,4,5) copy_test = original clone_test = tuple(list(original)) ## convert to list then back to tuple ## The copy refers to the same content print(original) ## (1, 2, 3, 4, 5) print(copy_test) ## (1, 2, 3, 4, 5) print(clone_test) ## Copy and original has the same memory location. ## (1, 2, 3, 4, 5) print(&#39;Original ID: &#39;, id(original)) ## Original ID: 2042309165520 print(&#39;Copy ID: &#39;, id(copy_test)) ## Copy ID: 2042309165520 print(&#39;Clone ID: &#39;, id(clone_test)) ## clone has different ID ## Clone ID: 2042309418256 4.2 List List is a collection of ordered items, where the items can be different data types You can pack list of items by placing them into [] List is mutable 4.2.1 Creation ## Create Empty List empty = [] # literal assignment method empty = list() # constructor method multiple = [123,&#39;abc&#39;,456, None] ## multiple datatypes allowed str_list = list(&#39;hello&#39;) ## [split into h,e,l,l,o] multiple str_list ## [123, &#39;abc&#39;, 456, None] ## [&#39;h&#39;, &#39;e&#39;, &#39;l&#39;, &#39;l&#39;, &#39;o&#39;] 4.2.2 Accessor Use [] to specify single or range of objects to return. Index numner starts from 0. food = [&#39;bread&#39;, &#39;noodle&#39;, &#39;rice&#39;, &#39;biscuit&#39;,&#39;jelly&#39;,&#39;cake&#39;] ## Accessing Single Index, Returns Object food[2] # 3rd item food[-1] # last item ## Accessing Range Of Indexes, Return List food[:4] # first 3 items food[-3:] # last 3 items food[1:5] # item 1 to 4 food[5:2:-1] # item 3 to 5, reverse order food[::-1] # reverse order ## &#39;rice&#39; ## &#39;cake&#39; ## [&#39;bread&#39;, &#39;noodle&#39;, &#39;rice&#39;, &#39;biscuit&#39;] ## [&#39;biscuit&#39;, &#39;jelly&#39;, &#39;cake&#39;] ## [&#39;noodle&#39;, &#39;rice&#39;, &#39;biscuit&#39;, &#39;jelly&#39;] ## [&#39;cake&#39;, &#39;jelly&#39;, &#39;biscuit&#39;] ## [&#39;cake&#39;, &#39;jelly&#39;, &#39;biscuit&#39;, &#39;rice&#39;, &#39;noodle&#39;, &#39;bread&#39;] 4.2.3 Methods All methods shown below is “inplace”, meaning the original data will be changed. Remove Item(s) Removal of non-existence item will result in error food = list([&#39;bread&#39;, &#39;noodle&#39;, &#39;rice&#39;, &#39;biscuit&#39;,&#39;jelly&#39;,&#39;biscuit&#39;,&#39;noodle&#39;]) food.remove(&#39;biscuit&#39;) ## remove first found element food.pop() ## remove last element, and return it food.pop(1) ## remove second element, and return it food ## &#39;noodle&#39; ## &#39;noodle&#39; ## [&#39;bread&#39;, &#39;rice&#39;, &#39;jelly&#39;, &#39;biscuit&#39;] Appending There are two methods to add elements to the tail. append() adds single element. extend() addes multiple elements. food.append(&#39;durian&#39;) ## add single element to the tail food.extend([&#39;nand&#39;,&#39;puff&#39;]) ## add elements to the tail food ## [&#39;bread&#39;, &#39;rice&#39;, &#39;jelly&#39;, &#39;biscuit&#39;, &#39;durian&#39;, &#39;nand&#39;, &#39;puff&#39;] Other Methods ## ordering of elements food.reverse() ## reverse current order food.sort() ## sort ascending food.sort(reverse=True) ## sort descending food ## methods returning number food.index(&#39;biscuit&#39;) ## return the index of first found element food.count(&#39;biscuit&#39;) ## return occurance of element ## [&#39;rice&#39;, &#39;puff&#39;, &#39;nand&#39;, &#39;jelly&#39;, &#39;durian&#39;, &#39;bread&#39;, &#39;biscuit&#39;] ## 6 ## 1 4.2.4 Operators Concatenation Two lists can be concatenated using ‘+’ operator. animals = [&#39;dog&#39;,&#39;cat&#39;,&#39;horse&#39;] + [&#39;elephant&#39;,&#39;tiger&#39;] + [&#39;sheep&#39;] List is Mutable The reference of list variable won’t change after adding/removing its item id(animals) animals += [&#39;chicken&#39;] animals.pop() id(animals) ## ID had not changed ## 2042309434304 ## &#39;chicken&#39; ## 2042309434304 Copy and Clone Assignment to another variable always refers to the same data.Use copy() method if you wish to clone the data, with different ID. original = [1,2,3,4,5] ## original data copy_test = original ## same ID as Original clone_test = original.copy() ## different ID print( id(original), id(copy_test), id(clone_test)) ## 2042309464512 2042309464512 2042309474048 Passing To Function As Reference When passing list to functions, only the reference passed. Meaning all changes to the list within the function will be reflected outside the function. my_list = [1,2,3,4,5] def func(x): print (x) print(&#39;ID in Function: &#39;, id(x)) x.append(6) ## modify the refrence my_list id(my_list) func(my_list) ## passing reference to function my_list ## content was altered id(my_list) ## [1, 2, 3, 4, 5] ## 2042309474112 ## [1, 2, 3, 4, 5] ## ID in Function: 2042309474112 ## [1, 2, 3, 4, 5, 6] ## 2042309474112 4.2.5 Iteration For Loop mylist = [&#39;abc&#39;,&#39;abcd&#39;,&#39;bcde&#39;,&#39;bcdee&#39;,&#39;cdefg&#39;] for x in mylist: if &#39;abc&#39; in x: print (x) ## abc ## abcd List Comprehension This code below is a short-form method of for loop and if. The output of list comprehension is a new list. old_list = [&#39;abc&#39;,&#39;abcd&#39;,&#39;bcde&#39;,&#39;bcdee&#39;,&#39;cdefg&#39;] [x for x in old_list if &#39;abc&#39; in x] ## [&#39;abc&#39;, &#39;abcd&#39;] Code below is a long-version compared to list comprehension aboce. new_list = [] old_list = [&#39;abc&#39;,&#39;abcd&#39;,&#39;bcde&#39;,&#39;bcdee&#39;,&#39;cdefg&#39;] for x in old_list: if &#39;abc&#39; in x: new_list.append(x) new_list ## [&#39;abc&#39;, &#39;abcd&#39;] 4.2.6 Tuple Conversion my_list = [1,2,3] my_tuple = tuple(my_list) ## concert to tuple using constructor my_list my_tuple ## [1, 2, 3] ## (1, 2, 3) 4.2.7 Built-In Functions my_numbers = [1,2,3,5,5,3,2,1] len(my_numbers) ## numner of elements max(my_numbers) ## maximum value of elements sorted(my_numbers) ## sort ascending sorted(my_numbers, reverse=True) ## sort descending ## 8 ## 5 ## [1, 1, 2, 2, 3, 3, 5, 5] ## [5, 5, 3, 3, 2, 2, 1, 1] 4.3 Dictionaries Dictionary is a list of index-value items. 4.3.1 Creation Simple Dictionary empty_dict = {} ## create empty animal_counts = { &#39;cats&#39; : 2, &#39;dogs&#39; : 5, &#39;horses&#39;:4} type(animal_counts) type(empty_dict) ## &lt;class &#39;dict&#39;&gt; ## &lt;class &#39;dict&#39;&gt; Dictionary with list horse_names = [&#39;Sax&#39;,&#39;Jack&#39;,&#39;Ann&#39;,&#39;Jeep&#39;] animal_names = {&#39;cats&#39;: [&#39;Walter&#39;,&#39;Ra&#39;], &#39;dogs&#39;: [&#39;Jim&#39;,&#39;Roy&#39;,&#39;John&#39;,&#39;Lucky&#39;,&#39;Row&#39;], &#39;horses&#39;: horse_names } animal_names ## {&#39;cats&#39;: [&#39;Walter&#39;, &#39;Ra&#39;], &#39;dogs&#39;: [&#39;Jim&#39;, &#39;Roy&#39;, &#39;John&#39;, &#39;Lucky&#39;, &#39;Row&#39;], &#39;horses&#39;: [&#39;Sax&#39;, &#39;Jack&#39;, &#39;Ann&#39;, &#39;Jeep&#39;]} 4.3.2 Accessor Get All Keys animal_name_keys = animal_names.keys() animal_name_keys ## it is a list [x for x in animal_name_keys] ## it is iterable ## dict_keys([&#39;cats&#39;, &#39;dogs&#39;, &#39;horses&#39;]) ## [&#39;cats&#39;, &#39;dogs&#39;, &#39;horses&#39;] Get All Values animal_name_values = animal_names.values() animal_name_values ## it is a list [x for x in animal_name_values] ## values are iterable ## dict_values([[&#39;Walter&#39;, &#39;Ra&#39;], [&#39;Jim&#39;, &#39;Roy&#39;, &#39;John&#39;, &#39;Lucky&#39;, &#39;Row&#39;], [&#39;Sax&#39;, &#39;Jack&#39;, &#39;Ann&#39;, &#39;Jeep&#39;]]) ## [[&#39;Walter&#39;, &#39;Ra&#39;], [&#39;Jim&#39;, &#39;Roy&#39;, &#39;John&#39;, &#39;Lucky&#39;, &#39;Row&#39;], [&#39;Sax&#39;, &#39;Jack&#39;, &#39;Ann&#39;, &#39;Jeep&#39;]] Acceess With Key Use [ key ] notation to get its value. However, this will return Error if key does not exist animal_names[&#39;dogs&#39;] ## [&#39;Jim&#39;, &#39;Roy&#39;, &#39;John&#39;, &#39;Lucky&#39;, &#39;Row&#39;] For a safer approach (not to return error when key doesn’t exist), use get( key ) notation. It will return None if key does not exist animal_names.get(&#39;cow&#39;) ## does not exist, return None animal_names.get(&#39;dogs&#39;) ## [&#39;Jim&#39;, &#39;Roy&#39;, &#39;John&#39;, &#39;Lucky&#39;, &#39;Row&#39;] 4.3.3 MACD Update/Append Use [key] notation to update or append the content of element. Use del to remove a key/value pair. new_world = {} ## create empty new_world[&#39;bacteria&#39;] = [&#39;Ameoba&#39;,&#39;Fractona&#39;] ## add new key/value new_world[&#39;alien&#39;] = [&#39;Ali&#39;,&#39;Abu&#39;] ## add new key/value new_world new_world[&#39;bacteria&#39;] = [&#39;Mutu&#39;, &#39;Aru&#39;] ## Update value del new_world[&#39;alien&#39;] ## delete key/value new_world ## {&#39;bacteria&#39;: [&#39;Ameoba&#39;, &#39;Fractona&#39;], &#39;alien&#39;: [&#39;Ali&#39;, &#39;Abu&#39;]} ## {&#39;bacteria&#39;: [&#39;Mutu&#39;, &#39;Aru&#39;]} Use clear() to erase all elements animal_names.clear() animal_names ## now an empty dict ## {} 4.3.4 Iteration Example below shows how to iterate over keys (.keys()), values (.values()) and both key/values (.items()). animal_dict = { &#39;cats&#39; : 2, &#39;dogs&#39; : 5, &#39;horses&#39;:4} [ (key,val) for key,val in animal_dict.items()] [x for x in animal_dict.values()] ## values are iterable [x for x in animal_dict.keys()] ## keys are iterable ## [(&#39;cats&#39;, 2), (&#39;dogs&#39;, 5), (&#39;horses&#39;, 4)] ## [2, 5, 4] ## [&#39;cats&#39;, &#39;dogs&#39;, &#39;horses&#39;] 4.4 Sets Set is unordered collection of unique items. Set is mutable 4.4.1 Creation Set can be declared with {}, just like list creation uses ‘[]’. myset = {&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;a&#39;,&#39;b&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;} myset # notice no repetition values ## {&#39;g&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;f&#39;, &#39;e&#39;} Set can be created from list, and then converted back to list. This is the perfect way to make a list unique. mylist = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;a&#39;,&#39;b&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;] myset = set(mylist) my_unique_list = list(myset) print ( &#39;Original List : &#39;, mylist, &#39;\\nConvert to set : &#39;, myset, &#39;\\nConvert back to list: &#39;, my_unique_list) # notice no repetition values ## Original List : [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;a&#39;, &#39;b&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;] ## Convert to set : {&#39;b&#39;, &#39;d&#39;, &#39;f&#39;, &#39;g&#39;, &#39;a&#39;, &#39;c&#39;, &#39;e&#39;} ## Convert back to list: [&#39;b&#39;, &#39;d&#39;, &#39;f&#39;, &#39;g&#39;, &#39;a&#39;, &#39;c&#39;, &#39;e&#39;] 4.4.2 Operators Membership Test &#39;a&#39; in myset # is member ? &#39;f&#39; not in myset # is not member ? ## True ## False Subset Test Subset Test : &lt;= Proper Subset Test : &lt; mysubset = {&#39;d&#39;,&#39;g&#39;} mysubset &lt;= myset ## True Proper Subset test that the master set contain at least one element which is not in the subset mysubset = {&#39;b&#39;,&#39;a&#39;,&#39;d&#39;,&#39;c&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;} print (&#39;Is Subset : &#39;, mysubset &lt;= myset) print (&#39;Is Proper Subet : &#39;, mysubset &lt; myset) ## Is Subset : True ## Is Proper Subet : False Union using | {&#39;a&#39;,&#39;b&#39;,&#39;c&#39;} | {&#39;a&#39;,&#39;e&#39;,&#39;f&#39;} ## {&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;f&#39;, &#39;e&#39;} Intersection using &amp; Any elments that exist in both left and right set {&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;} &amp; {&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;} ## {&#39;d&#39;, &#39;c&#39;} Difference using - Remove right from left {&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;} - {&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;} ## {&#39;a&#39;, &#39;b&#39;} "],["control-and-loops.html", "Chapter 5 Control and Loops 5.1 If Statement 5.2 For Loops 5.3 Generators", " Chapter 5 Control and Loops 5.1 If Statement 5.1.1 Multiline If.. Statements price = 102 if price &lt;100: print (&#39;buy&#39;) elif price &lt; 110: print (&#39;hold&#39;) elif price &lt; 120: print (&#39;think about it&#39;) else: print (&#39;sell&#39;) ## hold print(&#39;end of programming&#39;) ## end of programming 5.1.2 Single Line If .. Statement 5.1.2.1 if … In One Statement price = 70 if price&lt;80: print(&#39;buy&#39;) ## buy 5.1.2.2 Ternary Statemnt This statement return a value with simple condition price = 85 &#39;buy&#39; if (price&lt;80) else &#39;dont buy&#39; ## &#39;dont buy&#39; 5.2 For Loops 5.2.1 For .. Else Construct else is only executed when the for loop completed all cycles mylist = [1,2,3,4,5] for i in mylist: print (i) else: print(&#39;Hooray, the loop is completed successfully&#39;) ## 1 ## 2 ## 3 ## 4 ## 5 ## Hooray, the loop is completed successfully In below exmaple, for loop encountered break, hence the else section is not executed. for i in mylist: if i &lt; 4: print (i) else: print(&#39;Oops, I am breaking out half way in the loop&#39;) break else: print(&#39;Hooray, the loop is completed successfully&#39;) ## 1 ## 2 ## 3 ## Oops, I am breaking out half way in the loop 5.2.2 Loop thorugh ‘range’ for i in range (1,10,2): print (&#39;Odds Number : &#39;,i) ## Odds Number : 1 ## Odds Number : 3 ## Odds Number : 5 ## Odds Number : 7 ## Odds Number : 9 5.2.3 Loop through ‘list’ 5.2.3.1 Standard For Loop letters = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;] for e in letters: print (&#39;Letter : &#39;,e) ## Letter : a ## Letter : b ## Letter : c ## Letter : d 5.2.3.2 List Comprehension Iterate through existing list, and build new list based on condition new_list = [expression(i) for i in old_list] s = [&#39;abc&#39;,&#39;abcd&#39;,&#39;bcde&#39;,&#39;bcdee&#39;,&#39;cdefg&#39;] [x.upper() for x in s] ## [&#39;ABC&#39;, &#39;ABCD&#39;, &#39;BCDE&#39;, &#39;BCDEE&#39;, &#39;CDEFG&#39;] Extend list comprehension can be extended with if condition** new_list = [expression(i) for i in old_list if filter(i)] old_list = [&#39;abc&#39;,&#39;abcd&#39;,&#39;bcde&#39;,&#39;bcdee&#39;,&#39;cdefg&#39;] matching = [ x.upper() for x in old_list if &#39;bcd&#39; in x ] print( matching ) ## [&#39;ABCD&#39;, &#39;BCDE&#39;, &#39;BCDEE&#39;] 5.2.4 Loop Through ‘Dictionary’ Looping through dict will picup key d = {&quot;x&quot;: 1, &quot;y&quot;: 2} for key in d: print (key, d[key]) ## x 1 ## y 2 5.3 Generators Generator is lazy, produce items only if asked for, hence more memory efficient Generator is function with ‘yield’ instead of ‘return’ Generator contains one or more yields statement When called, it returns an object (iterator) but does not start execution immediately Methods like iter() and next() are implemented automatically. So we can iterate through the items using next() Once the function yields, the function is paused and the control is transferred to the caller Local variables and their states are remembered between successive calls Finally, when the function terminates, StopIteration is raised automatically on further calls 5.3.1 Basic Generator Function Below example give clear understanding of how generator works def my_gen(): n = 1 print(&#39;This is printed first&#39;) # Generator function contains yield statements yield n n += 1 print(&#39;This is printed second&#39;) yield n n += 1 print(&#39;This is printed at last&#39;) yield n a = my_gen() type(a) ## &lt;class &#39;generator&#39;&gt; next(a) ## This is printed first ## 1 next(a) ## This is printed second ## 2 5.3.2 Useful Generator Fuction Generator is only useful when it uses for-loop - for-loop within generator - for-loop to iterate through a generator def rev_str(my_str): length = len(my_str) for i in range(length - 1,-1,-1): yield my_str[i] for c in rev_str(&quot;hello&quot;): print(c) ## o ## l ## l ## e ## h 5.3.3 Generator Expression Use () to create an annonymous generator function my_list = [1, 3, 6, 10] a = (x**2 for x in my_list) next(a) ## 1 next(a) ## 9 sum(a) # sum the power of 6,10 ## 136 5.3.4 Compare to Iterator Class class PowTwo: def __init__(self, max = 0): self.max = max def __iter__(self): self.n = 0 return self def __next__(self): if self.n &gt; self.max: raise StopIteration result = 2 ** self.n self.n += 1 return result Obviously, Generator is more concise and cleaner def PowTwoGen(max = 0): n = 0 while n &lt; max: yield 2 ** n n += 1 "],["library-and-functions.html", "Chapter 6 Library and Functions 6.1 Package Source 6.2 Importing Library 6.3 Functions", " Chapter 6 Library and Functions Library are group of functions 6.1 Package Source 6.1.1 Conda Package manager for any language Install binaries 6.1.2 PIP Package manager python only Compile from source Stands for Pip Installs Packages Python’s officially-sanctioned package manager, and is most commonly used to install packages published on the Python Package Index (PyPI) Both pip and PyPI are governed and supported by the Python Packaging Authority (PyPA). 6.2 Importing Library There are two methods to import library functions: 6.2.1 Standalone Namespace # access function through: libName.functionName - import &lt;libName&gt; # access function through: shortName.functionName - import &lt;libName&gt; as &lt;shortName&gt; Use as for aliasing library name. This is useful if you have conflicting library name import math math.sqrt(9) import math as m ## use library shortname m.sqrt(9) ## 3.0 ## 3.0 6.2.2 Global Namespace # all functions available at global namespace - from &lt;libName&gt; import * # access function through original names - from &lt;libName&gt; import &lt;functionName&gt; # access function through custom names - from &lt;libName&gt; import &lt;functionName&gt; as &lt;shortFunctionName&gt; from math import sqrt sqrt(9) from math import sqrt as sq sq(9) ## 3.0 ## 3.0 6.3 Functions 6.3.1 Define By default, arguments are assigned to function left to right. However, you can also specify the argument assignment during function call. Function can have default args value. def myfun(x,y=3): print (&#39;x:&#39;,x) print (&#39;y:&#39;,y) myfun(5,8) ## default function read args from left to right myfun (y=8,x=5) ## but we can specific as well myfun (x=5) ## args x not specified, function assume default= ## x: 5 ## y: 8 ## x: 5 ## y: 8 ## x: 5 ## y: 3 6.3.2 List Within Function Consider a function is an object, its variable (some_list) is a reference. Hence its value is remembered when no parameter is passed over. it is counter intuitive, always ensure to pass the arguments to function, even though it is a an empty list. def spam (elem, some_list=[]): some_list.append(elem) return some_list print (spam(1)) print (spam(2,[6])) print (spam(3)) print (spam(2,[7])) print (spam(4, [])) ## [1] ## [6, 2] ## [1, 3] ## [7, 2] ## [4] 6.3.3 Return Statement Multiple value is returned as tuple. Use multiple assignment to assign to multiple variable def bigger(x,y): if (x&gt;y): return x else: return y def minmax(x,y,z): return min(x,y,z), max(x,y,z) big = bigger(5,8) a,b = minmax(7,8,9) # multiple assignment c = minmax(7,8,9) # tuple print (big) ## noolean print (a,b) ## integer print (c) ## tuple ## 8 ## 7 9 ## (7, 9) if no return statement, python return None def dummy(): print (&#39;This is a dummy function, return no value&#39;) dummy() == None ## This is a dummy function, return no value ## True 6.3.4 Argument Passing too many arguments will result in an error. Passing too little arguments without default value defined will also result in error. For dynamic number of arguments, see args in the next section. def myfun(x,y): print (x) print (y) myfun(1,2,3,4,5) myfun(1) ## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: myfun() takes 2 positional arguments but 5 were given ## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: myfun() missing 1 required positional argument: &#39;y&#39; Function as Argument def myfun(x,y,f): f(x,y) myfun(&#39;hello&#39;,54,print) ## hello 54 Breaking Dict as Argument Below example break dict to a function. However, be careful not to pass extra key to the function, which will result in error. def foo(a,b,c,d=1): print(a, b, c, d) foo(**{&quot;a&quot;:2,&quot;b&quot;:3,&quot;c&quot;:4}) ## ok foo(**{&quot;a&quot;:2,&quot;b&quot;:3,&quot;c&quot;:4, &quot;z&quot;:100}) ## Error, &#39;z&#39; not recognized ## 2 3 4 1 ## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: foo() got an unexpected keyword argument &#39;z&#39; 6.3.5 Argument: args All dynamic length of argumens not captured in the defined argument will overflow into args, which is a tuple. Example 1 - All tails overflow to args First argument goes to x, second argument goes to y, remaining overflow to args. def myfun(x,y,*args): print (x) print (y) print (args) #tuple myfun(1,2,3,4,5,&#39;any&#39;) ## 1 ## 2 ## (3, 4, 5, &#39;any&#39;) Example 2 - Middle overflow to args def myfun(x,*args, y=9): print (x) print (y) print (args) #tuple myfun(1,2,3,4,5) ## 1 ## 9 ## (2, 3, 4, 5) Example 3 - All goes to args def myfun(*args): print (args) #tuple myfun(1,2,3,4,5) ## (1, 2, 3, 4, 5) Example 4 - Empty args def myfun(x,y,*args): print (x) print (y) print (args) myfun(1,2) ## 1 ## 2 ## () 6.3.6 Argument: kwargs kwargs is a dictionary Example 1 - All goes into kwargs def foo(**kwargs): print(kwargs) foo(a=1,b=2,c=3) ## {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3} Example 2 - Tails overflow to kwargs First param goes to x, the rest goes to kwargs. def foo(x,**kwargs): print(x) print(kwargs) foo(9,a=1,b=2,c=3) foo(9) ## empty kwargs dictionary ## 9 ## {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3} ## 9 ## {} Mixing args, kwargs Always put args before kwargs, otherwise there will be an Error. def foo(x,y=2,*args,**kwargs): print (x,y, args, kwargs) foo(1,2,3,4,5,c=6,d=7) ## ok foo(1,2,3,c=6,4,5,d=7) ## ERROR, always puts args before kwargs ## positional argument follows keyword argument (&lt;string&gt;, line 5) "],["built-in-functions-1.html", "Chapter 7 Built-in Functions 7.1 range", " Chapter 7 Built-in Functions 7.1 range range(X) generates sequence of integer object range (lower_bound, upper_bound, step_size) # lower bound is optional, default = 0 # upper bound is not included in result # step is optional, default = 1 Use list() to convert in order to view actual sequence of data r = range(10) # default lower bound =0, step =1 print (type (r)) ## &lt;class &#39;range&#39;&gt; print (r) ## range(0, 10) print (list(r)) ## [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] More Examples print (list(range(2,8))) # step not specified, default 1 ## [2, 3, 4, 5, 6, 7] print (&#39;Odds Number : &#39; , list(range(1,10,2))) # generate odds number ## Odds Number : [1, 3, 5, 7, 9] "],["exception-handling.html", "Chapter 8 Exception Handling 8.1 Catching Error 8.2 Custom Exception", " Chapter 8 Exception Handling The try statement works as follows: First, the try clause (the statement(s) between the try and except keywords) is executed If no exception occurs, the except clause is skipped and execution of the try statement is finished If an exception occurs during execution of the try clause, the rest of the clause is skipped. Then if its type matches the exception named after the except keyword, the except clause is executed, and then execution continues after the try statement If an exception occurs which does not match the exception named in the except clause, it is passed on to outer try statements; if no handler is found, it is an unhandled exception and execution stops with a message as shown above A try statement may have more than one except clause, to specify handlers for different exceptions. Try-Except-Finally try: # code that may cause exceptions except: # code that handle exceptions finally: # code that clean up # this block optional 8.1 Catching Error Different exception object has different attributes. try: a = 1 + &#39;a&#39; ## catch specific error except TypeError as err: print(&#39;I know this error !!!!&#39;, &#39;\\n Error: &#39;, err, &#39;\\n Args: &#39;, err.args, &#39;\\n Type: &#39;, type(err)) ## Catch all other error except Exception as err: print( &#39;Error: &#39;, err, &#39;\\nArgs: &#39;, err.args, &#39;\\nType: &#39;, type(err)) ## I know this error !!!! ## Error: unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39; ## Args: (&quot;unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39;&quot;,) ## Type: &lt;class &#39;TypeError&#39;&gt; 8.2 Custom Exception try: raise Exception(&#39;bloody&#39;, &#39;hell&#39;) #simulate exception except Exception as err: print( &#39;Error: &#39;, err, &#39;\\nArgs: &#39;, err.args, &#39;\\nType: &#39;, type(err)) ## Error: (&#39;bloody&#39;, &#39;hell&#39;) ## Args: (&#39;bloody&#39;, &#39;hell&#39;) ## Type: &lt;class &#39;Exception&#39;&gt; "],["object-oriented-programming.html", "Chapter 9 Object Oriented Programming 9.1 Defining Class 9.2 Constructor 9.3 Calling Method 9.4 Getting Property 9.5 Setting Property", " Chapter 9 Object Oriented Programming 9.1 Defining Class Every function within a class must have at least one parameter - self Use init as the constructor function. init is optional class Person: wallet = 0 # def __init__(self, myname,money=0): # constructor self.name = myname self.wallet=money print(&#39;I\\&#39;m in Person Constructor: {}&#39;.format(myname)) def say_hi(self): print(&#39;Hello, my name is : &#39;, self.name) def say_bye(self): print(&#39;Goodbye&#39;, Person.ID) def take(self,amount): self.wallet+=amount def balance(self): print(&#39;Wallet Balance:&#39;,self.wallet) def MakeCry(self): self.Cry() class Kid(Person): def __init__(self, myname, money=0): print(&#39;I\\&#39;m in Kid Constructor: {}&#39;.format(myname)) super().__init__(myname=myname, money=money) def Cry(self): print(&#39;Kid is crying&#39;) 9.2 Constructor p1 = Person(&#39;Yong&#39;) ## I&#39;m in Person Constructor: Yong p2 = Person(&#39;Gan&#39;,200) ## I&#39;m in Person Constructor: Gan p3 = Kid(&#39;Jolin&#39;,50) ## I&#39;m in Kid Constructor: Jolin ## I&#39;m in Person Constructor: Jolin 9.3 Calling Method p1.say_hi() ## Hello, my name is : Yong p1.balance() ## Wallet Balance: 0 p3.Cry() ## Kid is crying p3.MakeCry() ## Kid is crying p2.say_hi() ## Hello, my name is : Gan p2.balance() ## Wallet Balance: 200 9.4 Getting Property p1.wallet ## 0 p2.wallet ## 200 9.5 Setting Property p1.wallet = 900 p1.wallet ## 900 "],["decorator.html", "Chapter 10 Decorator 10.1 Definition 10.2 Examples", " Chapter 10 Decorator 10.1 Definition Decorator is a function that accept callable as the only argument The main purpose of decarator is to enhance the program of the decorated function It returns a callable 10.2 Examples 10.2.1 Example 1 - Plain decorator function Many times, it is useful to register a function elsewhere - for example, registering a task in a task runner, or a functin with signal handler register is a decarator, it accept decorated as the only argument foo() and bar() are the decorated function of register registry = [] def register(decorated): registry.append(decorated) return decorated @register def foo(): return 3 @register def bar(): return 5 registry ## [&lt;function foo at 0x0000029E37BF8700&gt;, &lt;function bar at 0x0000029E37C365E0&gt;] registry[0]() ## 3 registry[1]() ## 5 10.2.2 Example 2 - Decorator with Class Extending the use case above register is the decarator, it has only one argument class Registry(object): def __init__(self): self._functions = [] def register(self,decorated): self._functions.append(decorated) return decorated def run_all(self,*args,**kwargs): return_values = [] for func in self._functions: return_values.append(func(*args,**kwargs)) return return_values The decorator will decorate two functions, for both object a and b a = Registry() b = Registry() @a.register def foo(x=3): return x @b.register def bar(x=5): return x @a.register @b.register def bax(x=7): return x Observe the result print (a._functions) ## [&lt;function foo at 0x0000029E37C36670&gt;, &lt;function bax at 0x0000029E37C36940&gt;] print (b._functions) ## [&lt;function bar at 0x0000029E37C368B0&gt;, &lt;function bax at 0x0000029E37C36940&gt;] print (a.run_all()) ## [3, 7] print (b.run_all()) ## [5, 7] print ( a.run_all(x=9) ) ## [9, 9] print ( b.run_all(x=9) ) ## [9, 9] "],["matplotlib.html", "Chapter 11 matplotlib 11.1 Library 11.2 Sample Data 11.3 MATLAB-like API 11.4 Object-Oriented API 11.5 Histogram 11.6 Scatter Plot 11.7 Bar Chart", " Chapter 11 matplotlib 11.1 Library import matplotlib import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np from plydata import define, query, select, group_by, summarize, arrange, head, rename import plotnine from plotnine import * import os os.environ[&#39;QT_QPA_PLATFORM_PLUGIN_PATH&#39;] = &quot;C:\\ProgramData\\Anaconda3\\Library\\plugins\\platforms&quot; 11.2 Sample Data This chapter uses the sample data generate with below code. The idea is to simulate two categorical-alike feature, and two numeric value feature: com is random character between ?C1?, ?C2? and ?C3? dept is random character between ?D1?, ?D2?, ?D3?, ?D4? and ?D5? grp is random character with randomly generated ?G1?, ?G2? value1 represents numeric value, normally distributed at mean 50 value2 is numeric value, normally distributed at mean 25 n = 200 comp = [&#39;C&#39; + i for i in np.random.randint( 1,4, size = n).astype(str)] # 3x Company dept = [&#39;D&#39; + i for i in np.random.randint( 1,6, size = n).astype(str)] # 5x Department grp = [&#39;G&#39; + i for i in np.random.randint( 1,3, size = n).astype(str)] # 2x Groups value1 = np.random.normal( loc=50 , scale=5 , size = n) value2 = np.random.normal( loc=20 , scale=3 , size = n) value3 = np.random.normal( loc=5 , scale=30 , size = n) mydf = pd.DataFrame({ &#39;comp&#39;:comp, &#39;dept&#39;:dept, &#39;grp&#39;: grp, &#39;value1&#39;:value1, &#39;value2&#39;:value2, &#39;value3&#39;:value3 }) mydf.head() ## comp dept grp value1 value2 value3 ## 0 C1 D5 G1 52.750091 22.587541 0.344088 ## 1 C1 D5 G1 60.749514 15.821748 42.410972 ## 2 C1 D5 G2 53.056143 20.607832 8.812354 ## 3 C1 D5 G1 53.427367 25.386136 18.590399 ## 4 C3 D2 G1 57.248056 16.325665 -27.302194 mydf.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 200 entries, 0 to 199 ## Data columns (total 6 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 comp 200 non-null object ## 1 dept 200 non-null object ## 2 grp 200 non-null object ## 3 value1 200 non-null float64 ## 4 value2 200 non-null float64 ## 5 value3 200 non-null float64 ## dtypes: float64(3), object(3) ## memory usage: 9.5+ KB 11.3 MATLAB-like API The good thing about the pylab MATLAB-style API is that it is easy to get started with if you are familiar with MATLAB, and it has a minumum of coding overhead for simple plots. However, I’d encourrage not using the MATLAB compatible API for anything but the simplest figures. Instead, I recommend learning and using matplotlib’s object-oriented plotting API. It is remarkably powerful. For advanced figures with subplots, insets and other components it is very nice to work with. 11.3.1 Sample Data # Sample Data x = np.linspace(0,5,10) y = x ** 2 11.3.2 Single Plot plt.figure() plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.plot(x,y,&#39;red&#39;) plt.title(&#39;My Good Data&#39;) plt.show() 11.3.3 Multiple Subplots Each call lto subplot() will create a new container for subsequent plot command plt.figure() plt.subplot(1,2,1) # 1 row, 2 cols, at first box plt.plot(x,y,&#39;r--&#39;) plt.subplot(1,2,2) # 1 row, 2 cols, at second box plt.plot(y,x,&#39;g*-&#39;) plt.show() 11.4 Object-Oriented API 11.4.1 Sample Data # Sample Data x = np.linspace(0,5,10) y = x ** 2 11.4.2 Single Plot One figure, one axes fig = plt.figure() axes = fig.add_axes([0,0,1,1]) # left, bottom, width, height (range 0 to 1) axes.plot(x, y, &#39;r&#39;) axes.set_xlabel(&#39;x&#39;) axes.set_ylabel(&#39;y&#39;) axes.set_title(&#39;title&#39;) plt.show() 11.4.3 Multiple Axes In One Plot This is still considered a single plot, but with multiple axes fig = plt.figure() ax1 = fig.add_axes([0, 0, 1, 1]) # main axes ax2 = fig.add_axes([0.2, 0.5, 0.4, 0.3]) # inset axes ax1.plot(x,y,&#39;r&#39;) ax1.set_xlabel(&#39;x&#39;) ax1.set_ylabel(&#39;y&#39;) ax2.plot(y, x, &#39;g&#39;) ax2.set_xlabel(&#39;y&#39;) ax2.set_ylabel(&#39;x&#39;) ax2.set_title(&#39;insert title&#39;) plt.show() 11.4.4 Multiple Subplots One figure can contain multiple subplots Each subplot has one axes 11.4.4.1 Simple Subplots - all same size subplots() function return axes object that is iterable. Single Row Grid Single row grid means axes is an 1-D array. Hence can use for to iterate through axes fig, axes = plt.subplots( nrows=1,ncols=3 ) print (axes.shape) ## (3,) for ax in axes: ax.plot(x, y, &#39;r&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_title(&#39;title&#39;) ax.text(0.2,0.5,&#39;One&#39;) plt.show() Multiple Row Grid Multile row grid means axes is an 2-D array. Hence can use two levels of for loop to iterate through each row and column fig, axes = plt.subplots(2, 3, sharex=&#39;col&#39;, sharey=&#39;row&#39;) print (axes.shape) ## (2, 3) for i in range(axes.shape[0]): for j in range(axes.shape[1]): axes[i, j].text(0.5, 0.5, str((i, j)), fontsize=18, ha=&#39;center&#39;) plt.show() 11.4.4.2 Complicated Subplots - different size GridSpec specify grid size of the figure Manually specify each subplot and their relevant grid position and size plt.figure(figsize=(5,5)) grid = plt.GridSpec(2, 3, hspace=0.4, wspace=0.4) plt.subplot(grid[0, 0]) #row 0, col 0 plt.subplot(grid[0, 1:]) #row 0, col 1 to : plt.subplot(grid[1, :2]) #row 1, col 0:2 plt.subplot(grid[1, 2]); #row 1, col 2 plt.show() plt.figure(figsize=(5,5)) grid = plt.GridSpec(4, 4, hspace=0.8, wspace=0.4) plt.subplot(grid[:3, 0]) # row 0:3, col 0 plt.subplot(grid[:3, 1: ]) # row 0:3, col 1: plt.subplot(grid[3, 1: ]); # row 3, col 1: plt.show() -1 means last row or column plt.figure(figsize=(6,6)) grid = plt.GridSpec(4, 4, hspace=0.4, wspace=1.2) plt.subplot(grid[:-1, 0 ]) # row 0 till last row (not including last row), col 0 plt.subplot(grid[:-1, 1:]) # row 0 till last row (not including last row), col 1 till end plt.subplot(grid[-1, 1: ]); # row last row, col 1 till end plt.show() 11.4.5 Figure Customization 11.4.5.1 Avoid Overlap - Use tight_layout() Sometimes when the figure size is too small, plots will overlap each other. - tight_layout() will introduce extra white space in between the subplots to avoid overlap. - The figure became wider. fig, axes = plt.subplots( nrows=1,ncols=2) for ax in axes: ax.plot(x, y, &#39;r&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_title(&#39;title&#39;) fig.tight_layout() # adjust the positions of axes so that there is no overlap plt.show() 11.4.5.2 Avoid Overlap - Change Figure Size fig, axes = plt.subplots( nrows=1,ncols=2,figsize=(12,3)) for ax in axes: ax.plot(x, y, &#39;r&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_title(&#39;title&#39;) plt.show() 11.4.5.3 Text Within Figure fig = plt.figure() fig.text(0.5, 0.5, &#39;This Is A Sample&#39;,fontsize=18, ha=&#39;center&#39;); axes = fig.add_axes([0,0,1,1]) # left, bottom, width, height (range 0 to 1) plt.show() 11.4.6 Axes Customization 11.4.6.1 Y-Axis Limit fig = plt.figure() fig.add_axes([0,0,1,1], ylim=(-2,5)); plt.show() 11.4.6.2 Text Within Axes fig, ax = plt.subplots(2, 3, sharex=&#39;col&#39;, sharey=&#39;row&#39;) for i in range(2): for j in range(3): ax[i, j].text(0.5, 0.5, str((i, j)), fontsize=18, ha=&#39;center&#39;) plt.show() plt.text(0.5, 0.5, &#39;one&#39;,fontsize=18, ha=&#39;center&#39;) plt.show() 11.4.6.3 Share Y Axis Label fig, ax = plt.subplots(2, 3, sharex=&#39;col&#39;, sharey=&#39;row&#39;) # removed inner label plt.show() 11.4.6.4 Create Subplot Individually Each call lto subplot() will create a new container for subsequent plot command plt.subplot(2,4,1) plt.text(0.5, 0.5, &#39;one&#39;,fontsize=18, ha=&#39;center&#39;) plt.subplot(2,4,8) plt.text(0.5, 0.5, &#39;eight&#39;,fontsize=18, ha=&#39;center&#39;) plt.show() Iterate through subplots (ax) to populate them fig, ax = plt.subplots(2, 3, sharex=&#39;col&#39;, sharey=&#39;row&#39;) for i in range(2): for j in range(3): ax[i, j].text(0.5, 0.5, str((i, j)), fontsize=18, ha=&#39;center&#39;) plt.show() 11.5 Histogram plt.hist(mydf.value1, bins=12); plt.show() 11.6 Scatter Plot plt.scatter(mydf.value1, mydf.value2) plt.show() 11.7 Bar Chart com_grp = mydf.groupby(&#39;comp&#39;) grpdf = com_grp[&#39;value1&#39;].sum().reset_index() grpdf ## comp value1 ## 0 C1 3502.403644 ## 1 C2 3346.418329 ## 2 C3 3244.615460 plt.bar(grpdf.comp, grpdf.value1); plt.xlabel(&#39;Company&#39;) plt.ylabel(&#39;Sum of Value 1&#39;) plt.show() "],["seaborn.html", "Chapter 12 seaborn 12.1 Seaborn and Matplotlib 12.2 Sample Data 12.3 Scatter Plot 12.4 Histogram 12.5 Bar Chart 12.6 Faceting 12.7 Pair Grid", " Chapter 12 seaborn 12.1 Seaborn and Matplotlib seaborn returns a matplotlib object that can be modified by the options in the pyplot module Often, these options are wrapped by seaborn and .plot() in pandas and available as arguments 12.2 Sample Data n = 100 comp = [&#39;C&#39; + i for i in np.random.randint( 1,4, size = n).astype(str)] # 3x Company dept = [&#39;D&#39; + i for i in np.random.randint( 1,4, size = n).astype(str)] # 5x Department grp = [&#39;G&#39; + i for i in np.random.randint( 1,4, size = n).astype(str)] # 2x Groups value1 = np.random.normal( loc=50 , scale=5 , size = n) value2 = np.random.normal( loc=20 , scale=3 , size = n) value3 = np.random.normal( loc=5 , scale=30 , size = n) mydf = pd.DataFrame({ &#39;comp&#39;:comp, &#39;dept&#39;:dept, &#39;grp&#39;: grp, &#39;value1&#39;:value1, &#39;value2&#39;:value2, &#39;value3&#39;:value3 }) mydf.head() ## comp dept grp value1 value2 value3 ## 0 C1 D3 G3 47.742844 14.624240 -8.103716 ## 1 C1 D2 G2 52.990104 20.737877 -51.266869 ## 2 C1 D3 G2 51.561362 24.878603 34.559852 ## 3 C2 D1 G1 57.031746 22.056130 34.011182 ## 4 C3 D2 G3 48.080655 22.649980 27.315605 12.3 Scatter Plot 12.3.1 2x Numeric sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, data=mydf) plt.show() sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, fit_reg=False, data=mydf); #hide regresion line plt.show() 12.3.2 2xNumeric + 1x Categorical Use hue to represent additional categorical feature sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, data=mydf, hue=&#39;comp&#39;, fit_reg=False); plt.show() 12.3.3 2xNumeric + 2x Categorical Use col and hue to represent two categorical features sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, col=&#39;comp&#39;,hue=&#39;grp&#39;, fit_reg=False, data=mydf); plt.show() 12.3.4 2xNumeric + 3x Categorical Use row, col and hue to represent three categorical features sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, row=&#39;dept&#39;,col=&#39;comp&#39;, hue=&#39;grp&#39;, fit_reg=False, data=mydf); ## C:\\PROGRA~3\\Anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py:447: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`. plt.show() 12.3.5 Customization 12.3.5.1 size size: height in inch for each facet sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, col=&#39;comp&#39;,hue=&#39;grp&#39;, fit_reg=False, data=mydf) plt.show() Observe that even size is very large, lmplot will fit (shrink) everything into one row by deafult. See example below. sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, col=&#39;comp&#39;,hue=&#39;grp&#39;,fit_reg=False, data=mydf) plt.show() 12.3.5.2 col_wrap To avoid lmplot from shrinking the chart, we use col_wrap=&lt;col_number to wrap the output. Compare the size (height of each facet) with the above without col_wrap. Below chart is larger. sns.lmplot(x=&#39;value1&#39;, y=&#39;value2&#39;, col=&#39;comp&#39;,hue=&#39;grp&#39;, col_wrap=2, fit_reg=False, data=mydf) plt.show() 12.4 Histogram seaborn.distplot( a, # Series, 1D Array or List bins=None, hist=True, rug = False, vertical=False ) 12.4.1 1x Numeric sns.distplot(mydf.value1) plt.show() sns.distplot(mydf.value1,hist=True,rug=True,vertical=True, bins=30,color=&#39;g&#39;) plt.show() 12.5 Bar Chart com_grp = mydf.groupby(&#39;comp&#39;) grpdf = com_grp[&#39;value1&#39;].sum().reset_index() grpdf ## comp value1 ## 0 C1 2019.650686 ## 1 C2 1686.583374 ## 2 C3 1254.738173 12.5.1 1x Categorical, 1x Numeric sns.barplot(x=&#39;comp&#39;,y=&#39;value1&#39;,data=grpdf) plt.show() 12.5.2 Customization 12.5.2.1 Ordering sns.barplot(x=&#39;comp&#39;,y=&#39;value2&#39;, hue=&#39;grp&#39;, order=[&#39;C3&#39;,&#39;C2&#39;,&#39;C1&#39;], hue_order=[&#39;G1&#39;,&#39;G2&#39;,&#39;G3&#39;], data=mydf ) plt.show() 12.5.2.2 Flipping X/Y Axis sns.barplot(x=&#39;value2&#39;,y=&#39;comp&#39;, hue=&#39;grp&#39;,data=mydf) plt.show() 12.6 Faceting Faceting in Seaborn is a generic function that works with matplotlib various plot utility. It support matplotlib as well as seaborn plotting utility. 12.6.1 Faceting Histogram g = sns.FacetGrid(mydf, col=&quot;comp&quot;, row=&#39;dept&#39;) g.map(plt.hist, &quot;value1&quot;) plt.show() g = sns.FacetGrid(mydf, col=&quot;comp&quot;, row=&#39;dept&#39;) g.map(plt.hist, &quot;value1&quot;) plt.show() 12.6.2 Faceting Scatter Plot g = sns.FacetGrid(mydf, col=&quot;comp&quot;, row=&#39;dept&#39;,hue=&#39;grp&#39;) g.map(plt.scatter, &quot;value1&quot;,&quot;value2&quot;,alpha=0.7); g.add_legend() plt.show() 12.7 Pair Grid 12.7.1 Simple Pair Grid g = sns.PairGrid(mydf, hue=&#39;comp&#39;) g.map(plt.scatter); g.add_legend() plt.show() 12.7.2 Different Diag and OffDiag g = sns.PairGrid(mydf, hue=&#39;comp&#39;) g.map_diag(plt.hist, bins=15) g.map_offdiag(plt.scatter) g.add_legend() plt.show() "],["plotnine.html", "Chapter 13 plotnine 13.1 Histogram 13.2 Scatter Plot 13.3 Line Chart 13.4 Bar Chart", " Chapter 13 plotnine 13.1 Histogram 13.1.1 1xNumeric plotnine.ggplot( dataframe, aex(x=&#39;colName&#39;)) + geom_histogram( bins=10 ) plotnine.ggplot( dataframe, aex(x=&#39;colName&#39;)) + geom_histogram( binwidth=? ) plotnine.options.figure_size = (3, 3) ggplot(mydf, aes(x=&#39;value1&#39;)) + geom_histogram() # default bins = 10 ## &lt;ggplot: (154362947307)&gt; ## ## C:\\PROGRA~3\\Anaconda3\\lib\\site-packages\\plotnine\\stats\\stat_bin.py:95: PlotnineWarning: &#39;stat_bin()&#39; using &#39;bins = 7&#39;. Pick better value with &#39;binwidth&#39;. ggplot(mydf, aes(x=&#39;value1&#39;)) + geom_histogram(bins = 15) ## &lt;ggplot: (154369300547)&gt; ggplot(mydf, aes(x=&#39;value1&#39;)) + geom_histogram(binwidth = 3) ## &lt;ggplot: (154369435191)&gt; 13.1.2 1xNumeric + 1xCategorical plotnine.ggplot( dataframe, aes(x=&#39;colName&#39;), fill=&#39;categorical-alike-colName&#39;) + geom_histogram() ggplot(mydf, aes(x=&#39;value1&#39;, fill=&#39;grp&#39;)) + geom_histogram(bins=15) ## &lt;ggplot: (154369378663)&gt; 13.2 Scatter Plot 13.2.1 2x Numeric ggplot(mydf, aes(x=&#39;value1&#39;,y=&#39;value2&#39;)) + geom_point() ## &lt;ggplot: (154369419376)&gt; 13.2.2 2x Numeric + 1x Categorical ggplot( DataFrame, aes(x=&#39;colName1&#39;,y=&#39;colName2&#39;)) + geom_point( aes( color=&#39;categorical-alike-colName&#39;, size=&#39;numberColName&#39; )) ggplot(mydf, aes(x=&#39;value1&#39;,y=&#39;value2&#39;)) + geom_point(aes(color=&#39;grp&#39;)) ## &lt;ggplot: (154369360824)&gt; ggplot(mydf, aes(x=&#39;value1&#39;,y=&#39;value2&#39;,color=&#39;grp&#39;)) + geom_point() ## &lt;ggplot: (154362849771)&gt; ggplot(mydf, aes(x=&#39;value1&#39;,y=&#39;value2&#39;)) + \\ geom_point(aes( color=&#39;grp&#39; )) ## &lt;ggplot: (154366153575)&gt; 13.2.3 2x Numeric + 1x Numeric + 1x Categorical ggplot(mydf, aes(x=&#39;value1&#39;,y=&#39;value2&#39;)) + \\ geom_point(aes( color=&#39;grp&#39;, size=&#39;value3&#39; )) ## &lt;ggplot: (154367888125)&gt; 13.2.4 Overlay Smooth Line ggplot(mydf, aes(x=&#39;value1&#39;, y=&#39;value2&#39;)) + \\ geom_point() + \\ geom_smooth() # default method=&#39;loess&#39; ## &lt;ggplot: (154369450463)&gt; ## ## C:\\PROGRA~3\\Anaconda3\\lib\\site-packages\\plotnine\\stats\\smoothers.py:321: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings. ggplot(mydf, aes(x=&#39;value1&#39;, y=&#39;value2&#39;,fill=&#39;grp&#39;)) + \\ geom_point() + \\ geom_smooth( se=True, color=&#39;red&#39;, method=&#39;lm&#39;, level=0.75) ## &lt;ggplot: (154367841268)&gt; 13.3 Line Chart 13.3.1 2x Numeric Data ggplot (mydf.head(15), aes(x=&#39;value1&#39;, y=&#39;value2&#39;)) + geom_line() ## &lt;ggplot: (154367832471)&gt; 13.3.2 1x Numeric, 1x Categorical ggplot (mydf.head(15), aes(x=&#39;dept&#39;, y=&#39;value1&#39;)) + geom_line() ## &lt;ggplot: (154367733405)&gt; ggplot (mydf.head(30), aes(x=&#39;dept&#39;, y=&#39;value1&#39;)) + geom_line( aes(group=1)) ## &lt;ggplot: (154366153497)&gt; 13.3.3 2x Numeric, 1x Categorical ggplot (mydf.head(15), aes(x=&#39;value1&#39;, y=&#39;value2&#39;)) + geom_line( aes(color=&#39;grp&#39;),size=2) ## &lt;ggplot: (154362857301)&gt; 13.4 Bar Chart 13.4.0.1 1x Categorical Single categorical variable produces frequency chart. tmpdf = mydf.groupby([&#39;comp&#39;],as_index=False).count() tmpdf ## comp dept grp value1 value2 value3 ## 0 C1 41 41 41 41 41 ## 1 C2 33 33 33 33 33 ## 2 C3 26 26 26 26 26 tmpdf.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 3 entries, 0 to 2 ## Data columns (total 6 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 comp 3 non-null object ## 1 dept 3 non-null int64 ## 2 grp 3 non-null int64 ## 3 value1 3 non-null int64 ## 4 value2 3 non-null int64 ## 5 value3 3 non-null int64 ## dtypes: int64(5), object(1) ## memory usage: 272.0+ bytes ggplot (tmpdf, aes(x=&#39;comp&#39;, y=&#39;grp&#39;)) +geom_col() ## &lt;ggplot: (154363527349)&gt; "],["numpy.html", "Chapter 14 numpy 14.1 Environment Setup 14.2 Module Import 14.3 Data Types 14.4 Numpy Array 14.5 Random Numbers 14.6 Sampling (Integer) 14.7 NaN : Missing Numerical Data", " Chapter 14 numpy Best array data manipulation, fast numpy array allows only single data type, unlike list Support matrix operation 14.1 Environment Setup from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:75% !important; margin-left:350px; }&lt;/style&gt;&quot;)) #%matplotlib inline ## &lt;IPython.core.display.HTML object&gt; import pandas as pd import matplotlib.pyplot as plt import math pd.set_option( &#39;display.notebook_repr_html&#39;, False) # render Series and DataFrame as text, not HTML pd.set_option( &#39;display.max_column&#39;, 10) # number of columns pd.set_option( &#39;display.max_rows&#39;, 10) # number of rows pd.set_option( &#39;display.width&#39;, 90) # number of characters per row 14.2 Module Import import numpy as np np.__version__ ## other modules ## &#39;1.23.4&#39; from datetime import datetime from datetime import date from datetime import time 14.3 Data Types 14.3.1 NumPy Data Types NumPy supports a much greater variety of numerical types than Python does. This makes numpy much more powerful https://www.numpy.org/devdocs/user/basics.types.html Integer: np.int8, np.int16, np.int32, np.uint8, np.uint16, np.uint32 Float: np.float32, np.float64 14.3.2 int32/64 np.int is actually python standard int x = np.int(13) ## &lt;string&gt;:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information. ## Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations y = int(13) print( type(x) ) ## &lt;class &#39;int&#39;&gt; print( type(y) ) ## &lt;class &#39;int&#39;&gt; np.int32/64 are NumPy specific x = np.int32(13) y = np.int64(13) print( type(x) ) ## &lt;class &#39;numpy.int32&#39;&gt; print( type(y) ) ## &lt;class &#39;numpy.int64&#39;&gt; 14.3.3 float32/64 x = np.float(13) ## &lt;string&gt;:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. ## Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations y = float(13) print( type(x) ) ## &lt;class &#39;float&#39;&gt; print( type(y) ) ## &lt;class &#39;float&#39;&gt; x = np.float32(13) y = np.float64(13) print( type(x) ) ## &lt;class &#39;numpy.float32&#39;&gt; print( type(y) ) ## &lt;class &#39;numpy.float64&#39;&gt; 14.3.4 bool np.bool is actually python standard bool x = np.bool(True) ## &lt;string&gt;:1: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here. ## Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations print( type(x) ) ## &lt;class &#39;bool&#39;&gt; print( type(True) ) ## &lt;class &#39;bool&#39;&gt; 14.3.5 str np.str is actually python standard str x = np.str(&quot;ali&quot;) ## &lt;string&gt;:1: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here. ## Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations print( type(x) ) ## &lt;class &#39;str&#39;&gt; x = np.str_(&quot;ali&quot;) print( type(x) ) ## &lt;class &#39;numpy.str_&#39;&gt; 14.3.6 datetime64 Unlike python standard datetime library, there is no seperation of date, datetime and time. There is no time equivalent object NumPy only has one object: datetime64 object . 14.3.6.1 Constructor From String Note that the input string cannot be ISO8601 compliance, meaning any timezone related information at the end of the string (such as Z or +8) will result in error. np.datetime64(&#39;2005-02&#39;) ## numpy.datetime64(&#39;2005-02&#39;) np.datetime64(&#39;2005-02-25&#39;) ## numpy.datetime64(&#39;2005-02-25&#39;) np.datetime64(&#39;2005-02-25T03:30&#39;) ## numpy.datetime64(&#39;2005-02-25T03:30&#39;) From datetime np.datetime64( date.today() ) ## numpy.datetime64(&#39;2022-12-29&#39;) np.datetime64( datetime.now() ) ## numpy.datetime64(&#39;2022-12-29T21:13:49.796757&#39;) 14.3.6.2 Instance Method Convert to datetime using astype() dt64 = np.datetime64(&quot;2019-01-31&quot; ) dt64.astype(datetime) ## datetime.date(2019, 1, 31) 14.3.7 nan 14.3.7.1 Creating NaN NaN is NOT A BUILT-IN datatype. It means not a number, a numpy float object type. Can be created using two methods below. import numpy as np import pandas as pd import math kosong1 = float(&#39;NaN&#39;) kosong2 = np.nan print(&#39;Type: &#39;, type(kosong1), &#39;\\n&#39;, &#39;Value: &#39;, kosong1) ## Type: &lt;class &#39;float&#39;&gt; ## Value: nan print(&#39;Type: &#39;, type(kosong2), &#39;\\n&#39;, &#39;Value: &#39;, kosong2) ## Type: &lt;class &#39;float&#39;&gt; ## Value: nan 14.3.7.2 Detecting NaN Detect nan using various function from panda, numpy and math. print(pd.isna(kosong1), &#39;\\n&#39;, pd.isna(kosong2), &#39;\\n&#39;, np.isnan(kosong1),&#39;\\n&#39;, math.isnan(kosong2)) ## True ## True ## True ## True 14.3.7.3 Operation 14.3.7.3.1 Logical Operator print( True and kosong1, kosong1 and True) ## nan True print( True or kosong1, False or kosong1) ## True nan 14.3.7.3.2 Comparing Compare nan with anything results in False, including itself. print(kosong1 &gt; 0, kosong1==0, kosong1&lt;0, kosong1 ==1, kosong1==kosong1, kosong1==False, kosong1==True) ## False False False False False False False 14.3.7.3.3 Casting nan is numpy floating value. It is not a zero value, therefore casting to boolean returns True. bool(kosong1) ## True 14.4 Numpy Array 14.4.1 Concept Structure - NumPy provides an N-dimensional array type, the ndarray - ndarray is homogenous: every item takes up the same size block of memory, and all blocks - For each ndarray, there is a seperate dtype object, which describe ndarray data type - An item extracted from an array, e.g., by indexing, is represented by a Python object whose type is one of the array scalar types built in NumPy. The array scalars allow easy manipulation of also more complicated arrangements of data. 14.4.2 Constructor By default, numpy.array autodetect its data types based on most common denominator 14.4.2.1 dType: int, float Notice example below auto detected as int32 data type x = np.array( (1,2,3,4,5) ) print(x) ## [1 2 3 4 5] print(&#39;Type: &#39;, type(x)) ## Type: &lt;class &#39;numpy.ndarray&#39;&gt; print(&#39;dType:&#39;, x.dtype) ## dType: int32 Notice example below auto detected as float64 data type x = np.array( (1,2,3,4.5,5) ) print(x) ## [1. 2. 3. 4.5 5. ] print(&#39;Type: &#39;, type(x)) ## Type: &lt;class &#39;numpy.ndarray&#39;&gt; print(&#39;dType:&#39;, x.dtype) ## dType: float64 You can specify dtype to specify desired data types. NumPy will auto convert the data into specifeid types. Observe below that we convert float into integer x = np.array( (1,2,3,4.5,5), dtype=&#39;int&#39; ) print(x) ## [1 2 3 4 5] print(&#39;Type: &#39;, type(x)) ## Type: &lt;class &#39;numpy.ndarray&#39;&gt; print(&#39;dType:&#39;, x.dtype) ## dType: int32 14.4.2.2 dType: datetime64 Specify dtype is necessary to ensure output is datetime type. If not, output is generic object type. From str x = np.array([&#39;2007-07-13&#39;, &#39;2006-01-13&#39;, &#39;2010-08-13&#39;], dtype=&#39;datetime64&#39;) print(x) ## [&#39;2007-07-13&#39; &#39;2006-01-13&#39; &#39;2010-08-13&#39;] print(&#39;Type: &#39;, type(x)) ## Type: &lt;class &#39;numpy.ndarray&#39;&gt; print(&#39;dType:&#39;, x.dtype) ## dType: datetime64[D] From datetime x = np.array([datetime(2019,1,12), datetime(2019,1,14),datetime(2019,3,3)], dtype=&#39;datetime64&#39;) print(x) ## [&#39;2019-01-12T00:00:00.000000&#39; &#39;2019-01-14T00:00:00.000000&#39; ## &#39;2019-03-03T00:00:00.000000&#39;] print(&#39;Type: &#39;, type(x)) ## Type: &lt;class &#39;numpy.ndarray&#39;&gt; print(&#39;dType:&#39;, x.dtype) ## dType: datetime64[us] print(&#39;\\nElement Type:&#39;,type(x[1])) ## ## Element Type: &lt;class &#39;numpy.datetime64&#39;&gt; 14.4.2.3 2D Array x = np.array([range(10),np.arange(10)]) x ## array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], ## [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]) 14.4.3 Dimensions 14.4.3.1 Differentiating Dimensions 1-D array is array of single list 2-D array is array made of list containing lists (each row is a list) 2-D single row array is array with list containing just one list 14.4.3.2 1-D Array Observe that the shape of the array is (5,). It seems like an array with 5 rows, empty columns ! What it really means is 5 items single dimension. arr = np.array(range(5)) print (arr) ## [0 1 2 3 4] print (arr.shape) ## (5,) print (arr.ndim) ## 1 14.4.3.3 2-D Array arr = np.array([range(5),range(5,10),range(10,15)]) print (arr) ## [[ 0 1 2 3 4] ## [ 5 6 7 8 9] ## [10 11 12 13 14]] print (arr.shape) ## (3, 5) print (arr.ndim) ## 2 14.4.3.4 2-D Array - Single Row arr = np.array([range(5)]) print (arr) ## [[0 1 2 3 4]] print (arr.shape) ## (1, 5) print (arr.ndim) ## 2 14.4.3.5 2-D Array : Single Column Using array slicing method with newaxis at COLUMN, will turn 1D array into 2D of single column arr = np.arange(5)[:, np.newaxis] print (arr) ## [[0] ## [1] ## [2] ## [3] ## [4]] print (arr.shape) ## (5, 1) print (arr.ndim) ## 2 Using array slicing method with newaxis at ROW, will turn 1D array into 2D of single row arr = np.arange(5)[np.newaxis,:] print (arr) ## [[0 1 2 3 4]] print (arr.shape) ## (1, 5) print (arr.ndim) ## 2 14.4.4 Class Method 14.4.4.1 arange() Generate array with a sequence of numbers np.arange(10) ## array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 14.4.4.2 ones() np.ones(10) # One dimension, default is float ## array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) np.ones((2,5),&#39;int&#39;) #Two dimensions ## array([[1, 1, 1, 1, 1], ## [1, 1, 1, 1, 1]]) 14.4.4.3 zeros() np.zeros( 10 ) # One dimension, default is float ## array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) np.zeros((2,5),&#39;int&#39;) # 2 rows, 5 columns of ZERO ## array([[0, 0, 0, 0, 0], ## [0, 0, 0, 0, 0]]) 14.4.4.4 where() On 1D array numpy.where() returns the items matching the criteria ar1 = np.array(range(10)) print( ar1 ) ## [0 1 2 3 4 5 6 7 8 9] print( np.where(ar1&gt;5) ) ## (array([6, 7, 8, 9], dtype=int64),) On 2D array, where() return array of row index and col index for matching elements ar = np.array([(1,2,3,4,5),(11,12,13,14,15),(21,22,23,24,25)]) print (&#39;Data : \\n&#39;, ar) ## Data : ## [[ 1 2 3 4 5] ## [11 12 13 14 15] ## [21 22 23 24 25]] np.where(ar&gt;13) ## (array([1, 1, 2, 2, 2, 2, 2], dtype=int64), array([3, 4, 0, 1, 2, 3, 4], dtype=int64)) 14.4.4.5 Logical Methods numpy.logical_or Perform or operation on two boolean array, generate new resulting boolean arrays ar = np.arange(10) print( ar==3 ) # boolean array 1 ## [False False False True False False False False False False] print( ar==6 ) # boolean array 2 ## [False False False False False False True False False False] print( np.logical_or(ar==3,ar==6 ) ) # resulting boolean ## [False False False True False False True False False False] numpy.logical_and Perform and operation on two boolean array, generate new resulting boolean arrays ar = np.arange(10) print( ar==3 ) # boolean array 1 ## [False False False True False False False False False False] print( ar==6 ) # boolean array 2 ## [False False False False False False True False False False] print( np.logical_and(ar==3,ar==6 ) ) # resulting boolean ## [False False False False False False False False False False] 14.4.5 Instance Method 14.4.5.1 astype() conversion Convert to from datetime64 to datetime ar1 = np.array([&#39;2007-07-13&#39;, &#39;2006-01-13&#39;, &#39;2010-08-13&#39;], dtype=&#39;datetime64&#39;) print( type(ar1) ) ## a numpy array ## &lt;class &#39;numpy.ndarray&#39;&gt; print( ar1.dtype ) ## dtype is a numpy data type ## datetime64[D] After convert to datetime (non-numpy object, the dtype becomes generic ‘object’. ar2 = ar1.astype(datetime) print( type(ar2) ) ## still a numpy array ## &lt;class &#39;numpy.ndarray&#39;&gt; print( ar2.dtype ) ## dtype becomes generic &#39;object&#39; ## object 14.4.5.2 reshape() reshape ( row numbers, col numbers ) Sample Data a = np.array([range(5), range(10,15), range(20,25), range(30,35)]) a ## array([[ 0, 1, 2, 3, 4], ## [10, 11, 12, 13, 14], ## [20, 21, 22, 23, 24], ## [30, 31, 32, 33, 34]]) Resphepe 1-Dim to 2-Dim np.arange(12) # 1-D Array ## array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) np.arange(12).reshape(3,4) # 2-D Array ## array([[ 0, 1, 2, 3], ## [ 4, 5, 6, 7], ## [ 8, 9, 10, 11]]) Respahe 2-Dim to 2-Dim np.array([range(5), range(10,15)]) # 2-D Array ## array([[ 0, 1, 2, 3, 4], ## [10, 11, 12, 13, 14]]) np.array([range(5), range(10,15)]).reshape(5,2) # 2-D Array ## array([[ 0, 1], ## [ 2, 3], ## [ 4, 10], ## [11, 12], ## [13, 14]]) Reshape 2-Dimension to 2-Dim (of single row) - Change 2x10 to 1x10 - Observe [[ ]], and the number of dimension is stll 2, don’t be fooled np.array( [range(0,5), range(5,10)]) # 2-D Array ## array([[0, 1, 2, 3, 4], ## [5, 6, 7, 8, 9]]) np.array( [range(0,5), range(5,10)]).reshape(1,10) # 2-D Array ## array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]) Reshape 1-Dim Array to 2-Dim Array (single column) np.arange(8) ## array([0, 1, 2, 3, 4, 5, 6, 7]) np.arange(8).reshape(8,1) ## array([[0], ## [1], ## [2], ## [3], ## [4], ## [5], ## [6], ## [7]]) A better method, use newaxis, easier because no need to input row number as parameter np.arange(8)[:,np.newaxis] ## array([[0], ## [1], ## [2], ## [3], ## [4], ## [5], ## [6], ## [7]]) Reshape 1-Dim Array to 2-Dim Array (single row) np.arange(8) ## array([0, 1, 2, 3, 4, 5, 6, 7]) np.arange(8)[np.newaxis,:] ## array([[0, 1, 2, 3, 4, 5, 6, 7]]) 14.4.6 Element Selection 14.4.6.1 Sample Data x1 = np.array( (0,1,2,3,4,5,6,7,8)) x2 = np.array(( (1,2,3,4,5), (11,12,13,14,15), (21,22,23,24,25))) print(x1) ## [0 1 2 3 4 5 6 7 8] print(x2) ## [[ 1 2 3 4 5] ## [11 12 13 14 15] ## [21 22 23 24 25]] 14.4.6.2 1-Dimension All indexing starts from 0 (not 1) Choosing Single Element does not return array print( x1[0] ) ## first element ## 0 print( x1[-1] ) ## last element ## 8 print( x1[3] ) ## third element from start 3 ## 3 print( x1[-3] ) ## third element from end ## 6 Selecting multiple elments return ndarray print( x1[:3] ) ## first 3 elements ## [0 1 2] print( x1[-3:]) ## last 3 elements ## [6 7 8] print( x1[3:] ) ## all except first 3 elements ## [3 4 5 6 7 8] print( x1[:-3] ) ## all except last 3 elements ## [0 1 2 3 4 5] print( x1[1:4] ) ## elemnt 1 to 4 (not including 4) ## [1 2 3] 14.4.6.3 2-Dimension Indexing with [ row_positoins, row_positions ], index starts with 0 x[1:3, 1:4] # row 1 to 2 column 1 to 3 ## array([[1, 2, 3]]) 14.4.7 Attributes 14.4.7.1 dtype ndarray contain a property called dtype, whcih tell us the type of underlying items a = np.array( (1,2,3,4,5), dtype=&#39;float&#39; ) a.dtype ## dtype(&#39;float64&#39;) print(a.dtype) ## float64 print( type(a[1])) ## &lt;class &#39;numpy.float64&#39;&gt; 14.4.7.2 dim dim returns the number of dimensions of the NumPy array. Example below shows 2-D array x = np.array(( (1,2,3,4,5), (11,12,13,14,15), (21,22,23,24,25))) x.ndim ## 2 14.4.7.3 shape shape return a type of (rows, cols) x = np.array(( (1,2,3,4,5), (11,12,13,14,15), (21,22,23,24,25))) x.shape ## (3, 5) np.identity(5) ## array([[1., 0., 0., 0., 0.], ## [0., 1., 0., 0., 0.], ## [0., 0., 1., 0., 0.], ## [0., 0., 0., 1., 0.], ## [0., 0., 0., 0., 1.]]) 14.4.8 Operations 14.4.8.1 Arithmetic Sample Date ar = np.arange(10) print( ar ) ## [0 1 2 3 4 5 6 7 8 9] * ar = np.arange(10) print (ar) ## [0 1 2 3 4 5 6 7 8 9] print (ar*2) ## [ 0 2 4 6 8 10 12 14 16 18] **+ and -** ar = np.arange(10) print (ar+2) ## [ 2 3 4 5 6 7 8 9 10 11] print (ar-2) ## [-2 -1 0 1 2 3 4 5 6 7] 14.4.8.2 Comparison Sample Data ar = np.arange(10) print( ar ) ## [0 1 2 3 4 5 6 7 8 9] == print( ar==3 ) ## [False False False True False False False False False False] &gt;, &gt;=, &lt;, &lt;= print( ar&gt;3 ) ## [False False False False True True True True True True] print( ar&lt;=3 ) ## [ True True True True False False False False False False] 14.5 Random Numbers 14.5.1 Uniform Distribution 14.5.1.1 Random Integer (with Replacement) randint() Return random integers from low (inclusive) to high (exclusive) np.random.randint( low ) # generate an integer, i, which i &lt; low np.random.randint( low, high ) # generate an integer, i, which low &lt;= i &lt; high np.random.randint( low, high, size=1) # generate an ndarray of integer, single dimension np.random.randint( low, high, size=(r,c)) # generate an ndarray of integer, two dimensions np.random.randint( 10 ) ## 5 np.random.randint( 10, 20 ) ## 18 np.random.randint( 10, high=20, size=5) # single dimension ## array([11, 18, 14, 10, 13]) np.random.randint( 10, 20, (3,5) ) # two dimensions ## array([[13, 13, 15, 11, 19], ## [17, 17, 12, 15, 19], ## [12, 13, 16, 15, 16]]) 14.5.1.2 Random Integer (with or without replacement) numpy.random .choice( a, size, replace=True) # sampling from a, # if a is integer, then it is assumed sampling from arange(a) # if a is an 1-D array, then sampling from this array np.random.choice(10,5, replace=False) # take 5 samples from 0:19, without replacement ## array([5, 3, 1, 8, 7]) np.random.choice( np.arange(10,20), 5, replace=False) ## array([18, 12, 16, 14, 11]) 14.5.1.3 Random Float randf() Generate float numbers in between 0.0 and 1.0 np.random.ranf(size=None) np.random.ranf(4) ## array([0.81429055, 0.90130078, 0.72435389, 0.71842983]) uniform() Return random float from low (inclusive) to high (exclusive) np.random.uniform( low ) # generate an float, i, which f &lt; low np.random.uniform( low, high ) # generate an float, i, which low &lt;= f &lt; high np.random.uniform( low, high, size=1) # generate an array of float, single dimension np.random.uniform( low, high, size=(r,c)) # generate an array of float, two dimensions np.random.uniform( 2 ) ## 1.4578172537977627 np.random.uniform( 2,5, size=(4,4) ) ## array([[4.17676629, 2.42225008, 2.84446176, 2.81707751], ## [3.15004448, 4.90847589, 2.72634139, 2.32142379], ## [3.82884429, 3.83189594, 3.23311778, 4.60748045], ## [4.83846875, 3.16579808, 4.37576259, 2.15626565]]) 14.5.2 Normal Distribution numpy. random.randn (n_items) # 1-D standard normal (mean=0, stdev=1) numpy. random.randn (nrows, ncols) # 2-D standard normal (mean=0, stdev=1) numpy. random.standard_normal( size=None ) # default to mean = 0, stdev = 1, non-configurable numpy. random.normal ( loc=0, scale=1, size=None) # loc = mean, scale = stdev, size = dimension 14.5.2.1 Standard Normal Distribution Generate random normal numbers with gaussion distribution (mean=0, stdev=1) One Dimension np.random.standard_normal(3) ## array([2.19796076, 0.52798048, 0.47388021]) np.random.randn(3) ## array([-0.2716829, -0.4833686, -0.6173721]) Two Dimensions np.random.randn(2,4) ## array([[-0.31618364, 0.14265918, -0.16929468, 0.49368487], ## [ 0.19196495, -0.35481338, -0.39458486, -1.52829915]]) np.random.standard_normal((2,4)) ## array([[ 1.28308953, 0.20073435, 2.353611 , -0.08875257], ## [ 0.22223767, 0.33641296, 1.1124219 , 0.85285548]]) Observe: randn(), standard_normal() and normal() are able to generate standard normal numbers np.random.seed(15) print (np.random.randn(5)) ## [-0.31232848 0.33928471 -0.15590853 -0.50178967 0.23556889] np.random.seed(15) print (np.random.normal ( size = 5 )) # stdev and mean not specified, default to standard normal ## [-0.31232848 0.33928471 -0.15590853 -0.50178967 0.23556889] np.random.seed(15) print (np.random.standard_normal (size=5)) ## [-0.31232848 0.33928471 -0.15590853 -0.50178967 0.23556889] 14.5.2.2 Normal Distribution (Non-Standard) np.random.seed(125) np.random.normal( loc = 12, scale=1.25, size=(3,3)) ## array([[11.12645382, 12.01327885, 10.81651695], ## [12.41091248, 12.39383072, 11.49647195], ## [ 8.70837035, 12.25246312, 11.49084235]]) 14.5.2.3 Linear Spacing numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None) # endpoint: If True, stop is the last sample, otherwise it is not included Include Endpoint Step = Gap divide by (number of elements minus 1) (2/(10-1)) np.linspace(1,3,10) #default endpont=True ## array([1. , 1.22222222, 1.44444444, 1.66666667, 1.88888889, ## 2.11111111, 2.33333333, 2.55555556, 2.77777778, 3. ]) Does Not Include Endpoint Step = Gap divide by (number of elements minus 1) (2/(101)) np.linspace(1,3,10,endpoint=False) ## array([1. , 1.2, 1.4, 1.6, 1.8, 2. , 2.2, 2.4, 2.6, 2.8]) 14.6 Sampling (Integer) random.choice( a, size=None, replace=True, p=None) # a=integer, return &lt;size&gt; integers &lt; a random.choice( a, size=None, replace=True, p=None) # a=array-like, return &lt;size&gt; integers picked from list a np.random.choice (100, size=10) ## array([58, 0, 84, 50, 89, 32, 87, 30, 66, 92]) np.random.choice( [1,3,5,7,9,11,13,15,17,19,21,23], size=10, replace=False) ## array([ 5, 1, 23, 17, 3, 13, 15, 9, 21, 7]) 14.7 NaN : Missing Numerical Data You should be aware that NaN is a bit like a data virus?it infects any other object it touches t = np.array([1, np.nan, 3, 4]) t.dtype ## dtype(&#39;float64&#39;) Regardless of the operation, the result of arithmetic with NaN will be another NaN 1 + np.nan ## nan t.sum(), t.mean(), t.max() ## (nan, nan, nan) np.nansum(t), np.nanmean(t), np.nanmax(t) ## (8.0, 2.6666666666666665, 4.0) "],["pandas.html", "Chapter 15 pandas 15.1 Modules Import 15.2 Pandas Objects 15.3 Class Method 15.4 Class: Timestamp 15.5 Class: DateTimeIndex 15.6 Class: Series 15.7 Class: DataFrame 15.8 Class: MultiIndex 15.9 Class: Categories 15.10 Dummies 15.11 DataFrameGroupBy 15.12 Fundamental Analysis 15.13 Missing Data", " Chapter 15 pandas 15.1 Modules Import import pandas as pd ## Other Libraries import numpy as np import datetime as dt from datetime import datetime from datetime import date 15.2 Pandas Objects 15.2.1 Types of Objects pandas.Timestamp pandas.Timedelta pandas.Period pandas.Interval pandas.DateTimeIndex pandas.DataFrame pandas.Series 15.2.2 Series and DataFrame Type Dimension Size Value Constructor Series 1 Immutable Mutable pandas.DataFrame( data, index, dtype, copy) DataFrame 2 Mutable Mutable pandas.DataFrame( data, index, columns, dtype, copy) Panel 3 Mutable Mutable data can be ndarray, list, constants index must be unique and same length as data. Can be integer or string dtype if none, it will be inferred copy copy data. Default false 15.3 Class Method 15.3.1 Convertion 15.3.1.1 Convert To Timestamp Use to_datetime() to create various Timestamp related objects: Convert list of dates to DateTimeIndex Convert list of dates to Series of Timestamps Convert single date into Timestamp Source can be string, date, datetime object Create Single Timestamp object pd.to_datetime(&#39;2011-01-03&#39;) pd.to_datetime(date(2018,4,13)) pd.to_datetime(datetime(2018,3,1,7,30)) ## Timestamp(&#39;2011-01-03 00:00:00&#39;) ## Timestamp(&#39;2018-04-13 00:00:00&#39;) ## Timestamp(&#39;2018-03-01 07:30:00&#39;) 15.3.1.2 Convert To DateTimeIndex data = [&#39;2011-01-03&#39;, # from string date(2018,4,13), # from date datetime(2018,3,1,7,30)] # from datetime dti = pd.to_datetime(data) ## Convert into DateTimeIndex type(dti) ## DateTimeIndex dti.dtype ## &lt;M8[ns] type(dti[1]) ## Elements are Timestamp objects ## &lt;class &#39;pandas.core.indexes.datetimes.DatetimeIndex&#39;&gt; ## dtype(&#39;&lt;M8[ns]&#39;) ## &lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; 15.3.2 Creating DateTimeIndex User date_range() to automatically generate range of Timestamps based on start/end/interval. The result is DateTimeIndex object which contains the range of Timestamps. 15.3.2.1 Hourly If start time not specified, default to 00:00:00. If start time specified, it will be honored on all subsequent Timestamp elements. Specify start and end, sequence will automatically distribute Timestamp according to frequency. print( pd.date_range(&#39;2018-01-01&#39;, periods=3, freq=&#39;H&#39;), pd.date_range(datetime(2018,1,1,12,30), periods=3, freq=&#39;H&#39;), pd.date_range(start=&#39;2018-01-03-1230&#39;, end=&#39;2018-01-03-18:30&#39;, freq=&#39;H&#39;)) ## DatetimeIndex([&#39;2018-01-01 00:00:00&#39;, &#39;2018-01-01 01:00:00&#39;, &#39;2018-01-01 02:00:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;H&#39;) DatetimeIndex([&#39;2018-01-01 12:30:00&#39;, &#39;2018-01-01 13:30:00&#39;, &#39;2018-01-01 14:30:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;H&#39;) DatetimeIndex([&#39;2018-01-03 12:30:00&#39;, &#39;2018-01-03 13:30:00&#39;, &#39;2018-01-03 14:30:00&#39;, ## &#39;2018-01-03 15:30:00&#39;, &#39;2018-01-03 16:30:00&#39;, &#39;2018-01-03 17:30:00&#39;, ## &#39;2018-01-03 18:30:00&#39;], ## dtype=&#39;datetime64[ns]&#39;, freq=&#39;H&#39;) 15.3.2.2 Daily When the frequency is Day and time is not specified, output is date distributed. When time is specified, output will honor the time. pd.date_range(date(2018,1,2), periods=3, freq=&#39;D&#39;), pd.date_range(&#39;2018-01-01-1230&#39;, periods=4, freq=&#39;D&#39;) ## (DatetimeIndex([&#39;2018-01-02&#39;, &#39;2018-01-03&#39;, &#39;2018-01-04&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;),) ## DatetimeIndex([&#39;2018-01-01 12:30:00&#39;, &#39;2018-01-02 12:30:00&#39;, &#39;2018-01-03 12:30:00&#39;, ## &#39;2018-01-04 12:30:00&#39;], ## dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) 15.3.2.3 First Day Of Month Use freq=MS, M stands for montly, S stand for Start. If the day specified, the sequence start from first day of following month. pd.date_range(&#39;2018-01&#39;, periods=4, freq=&#39;MS&#39;), pd.date_range(&#39;2018-01-09&#39;, periods=4, freq=&#39;MS&#39;), pd.date_range(&#39;2018-01-09 12:30:00&#39;, periods=4, freq=&#39;MS&#39;) ## (DatetimeIndex([&#39;2018-01-01&#39;, &#39;2018-02-01&#39;, &#39;2018-03-01&#39;, &#39;2018-04-01&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;MS&#39;),) ## (DatetimeIndex([&#39;2018-02-01&#39;, &#39;2018-03-01&#39;, &#39;2018-04-01&#39;, &#39;2018-05-01&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;MS&#39;),) ## DatetimeIndex([&#39;2018-02-01 12:30:00&#39;, &#39;2018-03-01 12:30:00&#39;, &#39;2018-04-01 12:30:00&#39;, ## &#39;2018-05-01 12:30:00&#39;], ## dtype=&#39;datetime64[ns]&#39;, freq=&#39;MS&#39;) 15.3.2.4 Last Day of Month Sequence always starts from the end of the specified month. pd.date_range(&#39;2018-01&#39;, periods=4, freq=&#39;M&#39;) pd.date_range(&#39;2018-01-09&#39;, periods=4, freq=&#39;M&#39;) pd.date_range(&#39;2018-01-09 12:30:00&#39;, periods=4, freq=&#39;M&#39;) ## DatetimeIndex([&#39;2018-01-31&#39;, &#39;2018-02-28&#39;, &#39;2018-03-31&#39;, &#39;2018-04-30&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;M&#39;) ## DatetimeIndex([&#39;2018-01-31&#39;, &#39;2018-02-28&#39;, &#39;2018-03-31&#39;, &#39;2018-04-30&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;M&#39;) ## DatetimeIndex([&#39;2018-01-31 12:30:00&#39;, &#39;2018-02-28 12:30:00&#39;, &#39;2018-03-31 12:30:00&#39;, ## &#39;2018-04-30 12:30:00&#39;], ## dtype=&#39;datetime64[ns]&#39;, freq=&#39;M&#39;) 15.3.3 Frequency Table (crosstab) crosstab returns Dataframe Object crosstab( index = &lt;SeriesObj&gt;, columns = &lt;new_colName&gt; ) # one dimension table crosstab( index = &lt;SeriesObj&gt;, columns = &lt;SeriesObj&gt; ) # two dimension table crosstab( index = &lt;SeriesObj&gt;, columns = [&lt;SeriesObj1&gt;, &lt;SeriesObj2&gt;] ) # multi dimension table crosstab( index = &lt;SeriesObj&gt;, columns = &lt;SeriesObj&gt;, margines=True ) # add column and row margins 15.3.3.1 Sample Data n = 200 comp = [&#39;C&#39; + i for i in np.random.randint( 1,4, size = n).astype(str)] # 3x Company dept = [&#39;D&#39; + i for i in np.random.randint( 1,6, size = n).astype(str)] # 5x Department grp = [&#39;G&#39; + i for i in np.random.randint( 1,3, size = n).astype(str)] # 2x Groups value1 = np.random.normal( loc=50 , scale=5 , size = n) value2 = np.random.normal( loc=20 , scale=3 , size = n) value3 = np.random.normal( loc=5 , scale=30 , size = n) mydf = pd.DataFrame({ &#39;comp&#39;:comp, &#39;dept&#39;:dept, &#39;grp&#39;: grp, &#39;value1&#39;:value1, &#39;value2&#39;:value2, &#39;value3&#39;:value3 }) mydf.head() ## comp dept grp value1 value2 value3 ## 0 C2 D2 G2 47.723523 24.694660 27.166640 ## 1 C2 D1 G2 45.539901 18.245128 2.576584 ## 2 C1 D4 G1 52.129810 24.189825 7.390868 ## 3 C3 D5 G2 55.028834 20.497327 -5.182554 ## 4 C2 D4 G1 42.908300 20.260859 37.540052 15.3.3.2 One Dimension Frequency Table pd.crosstab(index=mydf.comp, columns=&#39;counter&#39;) ## col_0 counter ## comp ## C1 72 ## C2 68 ## C3 60 15.3.3.3 Two Dimension Frequency Table Row dimension uses comp variable, Column dimension uses dept variable. pd.crosstab(index=mydf.comp, columns=mydf.dept) ## dept D1 D2 D3 D4 D5 ## comp ## C1 12 13 18 19 10 ## C2 21 15 13 9 10 ## C3 8 9 15 12 16 15.3.3.4 Higher Dimension Table Crosstab header is multi-levels index when more than one column specified. The multilevel index consists of all permuation of of grp and dept variables. tb = pd.crosstab(index=mydf.comp, columns=[mydf.dept, mydf.grp]) tb.columns ## observe two levels of headers generated tb ## two levels of headers ## MultiIndex([(&#39;D1&#39;, &#39;G1&#39;), ## (&#39;D1&#39;, &#39;G2&#39;), ## (&#39;D2&#39;, &#39;G1&#39;), ## (&#39;D2&#39;, &#39;G2&#39;), ## (&#39;D3&#39;, &#39;G1&#39;), ## (&#39;D3&#39;, &#39;G2&#39;), ## (&#39;D4&#39;, &#39;G1&#39;), ## (&#39;D4&#39;, &#39;G2&#39;), ## (&#39;D5&#39;, &#39;G1&#39;), ## (&#39;D5&#39;, &#39;G2&#39;)], ## names=[&#39;dept&#39;, &#39;grp&#39;]) ## dept D1 D2 D3 D4 D5 ## grp G1 G2 G1 G2 G1 G2 G1 G2 G1 G2 ## comp ## C1 5 7 6 7 12 6 11 8 7 3 ## C2 9 12 13 2 7 6 7 2 5 5 ## C3 2 6 3 6 10 5 5 7 8 8 Select sub-dataframe using multi-level referencing. print( &#39;Under D2:\\n&#39;, tb[&#39;D2&#39;], &#39;\\n\\n&#39;, &#39;Under D2-G2:\\n&#39;,tb[&#39;D2&#39;,&#39;G1&#39;]) ## Under D2: ## grp G1 G2 ## comp ## C1 6 7 ## C2 13 2 ## C3 3 6 ## ## Under D2-G2: ## comp ## C1 6 ## C2 13 ## C3 3 ## Name: (D2, G1), dtype: int64 15.3.3.5 Getting Margin Extend the crosstab with ‘margin=True’ to have sum of rows/columns, presented in new column/row named ‘All’. tb = pd.crosstab(index=mydf.dept, columns=mydf.grp, margins=True) tb ## grp G1 G2 All ## dept ## D1 16 25 41 ## D2 22 15 37 ## D3 29 17 46 ## D4 23 17 40 ## D5 20 16 36 ## All 110 90 200 print( &#39;Row Sums: \\n&#39;, tb.loc[:,&#39;All&#39;], &#39;\\n\\nColumn Sums:\\n&#39;, tb.loc[&#39;All&#39;]) ## Row Sums: ## dept ## D1 41 ## D2 37 ## D3 46 ## D4 40 ## D5 36 ## All 200 ## Name: All, dtype: int64 ## ## Column Sums: ## grp ## G1 110 ## G2 90 ## All 200 ## Name: All, dtype: int64 15.3.3.6 Getting Proportion Use matrix operation divide each row with its respective column sum. tb/tb.loc[&#39;All&#39;] ## grp G1 G2 All ## dept ## D1 0.145455 0.277778 0.205 ## D2 0.200000 0.166667 0.185 ## D3 0.263636 0.188889 0.230 ## D4 0.209091 0.188889 0.200 ## D5 0.181818 0.177778 0.180 ## All 1.000000 1.000000 1.000 15.3.4 Concatenation 15.3.4.1 Sample Data s1 = pd.Series([&#39;A1&#39;,&#39;A2&#39;,&#39;A3&#39;,&#39;A4&#39;]) s2 = pd.Series([&#39;B1&#39;,&#39;B2&#39;,&#39;B3&#39;,&#39;B4&#39;], name=&#39;B&#39;) s3 = pd.Series([&#39;C1&#39;,&#39;C2&#39;,&#39;C3&#39;,&#39;C4&#39;], name=&#39;C&#39;) 15.3.4.2 Column-Wise Combining Multiple Series Into A New DataFrame Series name will become column name in DataFrame. If Series has no name, default column names in DataFrame will be 0,1,2,3 axis=1 means column-wise pd.concat([s1,s2,s3, None], axis=1) ## observed that None is ignored ## 0 B C ## 0 A1 B1 C1 ## 1 A2 B2 C2 ## 2 A3 B3 C3 ## 3 A4 B4 C4 Add Multiple Series Into An Existing DataFrame No change to original DataFrame column name Added columns from series will have 0,1,2,3,.. column name df = pd.DataFrame({ &#39;A&#39;: s1, &#39;B&#39;: s2}) df ## A B ## 0 A1 B1 ## 1 A2 B2 ## 2 A3 B3 ## 3 A4 B4 pd.concat([df,s3,s2,s1, None],axis=1) ## A B C B 0 ## 0 A1 B1 C1 B1 A1 ## 1 A2 B2 C2 B2 A2 ## 2 A3 B3 C3 B3 A3 ## 3 A4 B4 C4 B4 A4 15.3.4.3 Row-Wise 15.3.5 External Data 15.3.5.1 html_table Parser This method require html5lib library. - Read the web page, create a list: which contain one or more dataframes that maps to each html table found - Scrap all detectable html tables - Auto detect column header - Auto create index using number starting from 0 read_html(url) # return list of dataframe(s) that maps to web table(s) structure df_list = pd.read_html(&#39;https://www.malaysiastock.biz/Listed-Companies.aspx?type=S&amp;s1=18&#39;) ## read all tables df = df_list[6] ## get the specific table print (&#39;Total Table(s) Found : &#39;, len(df_list), &#39;\\n&#39;, &#39;First Table Found: &#39;,df) ## Total Table(s) Found : 14 ## First Table Found: 0 \\ ## 0 0 - 9 A B C D E F G H I J K L M ... ## ## 1 ## 0 Bursa Malaysia Market Watch Top Articles 1. ... 15.3.5.2 CSV Writing Syntax DataFrame.to_csv( path_or_buf=None, ## if not provided, result is returned as string sep=&#39;, &#39;, na_rep=&#39;&#39;, float_format=None, columns=None, ## list of columns name to write, if not provided, all columns are written header=True, ## write out column names index=True, ## write row label index_label=None, mode=&#39;w&#39;, encoding=None, ## if not provided, default to &#39;utf-8&#39; quoting=None, quotechar=&#39;&quot;&#39;, line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal=&#39;.&#39;) Example below shows column value containing different special character. Note that pandas handles these very well by default. mydf = pd.DataFrame({&#39;Id&#39;:[10,20,30,40], &#39;Name&#39;: [&#39;Aaa&#39;,&#39;Bbb&#39;,&#39;Ccc&#39;,&#39;Ddd&#39;], &#39;Funny&#39;: [&quot;world&#39;s most \\clever&quot;, &quot;Bloody, damn, good&quot;, &quot;many\\nmany\\nline&quot;, &#39;Quoting &quot;is&quot; tough&#39;]}) mydf.set_index(&#39;Id&#39;, inplace=True) mydf.to_csv(&#39;data/csv_test.csv&#39;, index=True) mydf ## Name Funny ## Id ## 10 Aaa world&#39;s most \\clever ## 20 Bbb Bloody, damn, good ## 30 Ccc many\\nmany\\nline ## 40 Ddd Quoting &quot;is&quot; tough This is the file saved system(&#39;more data\\\\csv_test.csv&#39;) ## [1] 0 All content retained when reading back by Pandas pd.read_csv(&#39;data/csv_test.csv&#39;, index_col=&#39;Id&#39;) ## Name Funny ## Id ## 10 Aaa world&#39;s most \\clever ## 20 Bbb Bloody, damn, good ## 30 Ccc many\\nmany\\nline ## 40 Ddd Quoting &quot;is&quot; tough 15.3.5.3 CSV Reading Syntax pandas.read_csv( &#39;url or filePath&#39;, # path to file or url encoding = &#39;utf_8&#39;, # optional: default is &#39;utf_8&#39; index_col = [&#39;colName1&#39;, ...], # optional: specify one or more index column parse_dates = [&#39;dateCol1&#39;, ...], # optional: specify multiple string column to convert to date na_values = [&#39;.&#39;,&#39;na&#39;,&#39;NA&#39;,&#39;N/A&#39;], # optional: values that is considered NA names = [&#39;newColName1&#39;, ... ], # optional: overwrite column names thousands = &#39;.&#39;, # optional: thousand seperator symbol nrows = n, # optional: load only first n rows skiprows = 0, # optional: don&#39;t load first n rows parse_dates = False, # List of date column names infer_datetime_format = False # automatically parse dates ) Refer to full codec Python Codec. Default Import index is sequence of integer 0,1,2… only two data types detection; number (float64/int64) and string (object) date is not parsed, hence stayed as string goo = pd.read_csv(&#39;data/goog.csv&#39;, encoding=&#39;utf_8&#39;) print(goo.head(), &#39;\\n\\n&#39;, goo.info()) ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 61 entries, 0 to 60 ## Data columns (total 6 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Date 61 non-null object ## 1 Open 61 non-null float64 ## 2 High 61 non-null float64 ## 3 Low 61 non-null float64 ## 4 Close 61 non-null float64 ## 5 Volume 61 non-null int64 ## dtypes: float64(4), int64(1), object(1) ## memory usage: 3.0+ KB ## Date Open High Low Close Volume ## 0 12/19/2016 790.219971 797.659973 786.270020 794.200012 1225900 ## 1 12/20/2016 796.760010 798.650024 793.270020 796.419983 925100 ## 2 12/21/2016 795.840027 796.676025 787.099976 794.559998 1208700 ## 3 12/22/2016 792.359985 793.320007 788.580017 791.260010 969100 ## 4 12/23/2016 790.900024 792.739990 787.280029 789.909973 623400 ## ## None Specify Data Types To customize the data type, use dtype parameter with a dict of definition. d_types = {&#39;Volume&#39;: str} pd.read_csv(&#39;data/goog.csv&#39;, dtype=d_types).info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 61 entries, 0 to 60 ## Data columns (total 6 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Date 61 non-null object ## 1 Open 61 non-null float64 ## 2 High 61 non-null float64 ## 3 Low 61 non-null float64 ## 4 Close 61 non-null float64 ## 5 Volume 61 non-null object ## dtypes: float64(4), object(2) ## memory usage: 3.0+ KB Parse Datetime You can specify multiple date-alike column for parsing pd.read_csv(&#39;data/goog.csv&#39;, parse_dates=[&#39;Date&#39;]).info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 61 entries, 0 to 60 ## Data columns (total 6 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Date 61 non-null datetime64[ns] ## 1 Open 61 non-null float64 ## 2 High 61 non-null float64 ## 3 Low 61 non-null float64 ## 4 Close 61 non-null float64 ## 5 Volume 61 non-null int64 ## dtypes: datetime64[ns](1), float64(4), int64(1) ## memory usage: 3.0 KB Parse Datetime, Then Set as Index - Specify names of date column in parse_dates= - When date is set as index, the type is DateTimeIndex goo3 = pd.read_csv(&#39;data/goog.csv&#39;,index_col=&#39;Date&#39;, parse_dates=[&#39;Date&#39;]) goo3.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## DatetimeIndex: 61 entries, 2016-12-19 to 2017-03-17 ## Data columns (total 5 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Open 61 non-null float64 ## 1 High 61 non-null float64 ## 2 Low 61 non-null float64 ## 3 Close 61 non-null float64 ## 4 Volume 61 non-null int64 ## dtypes: float64(4), int64(1) ## memory usage: 2.9 KB 15.3.6 Inspection 15.3.6.1 Structure info info() is a function that print information to screen. It doesn’t return any object dataframe.info() # display columns and number of rows (that has no missing data) goo.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 61 entries, 0 to 60 ## Data columns (total 6 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Date 61 non-null object ## 1 Open 61 non-null float64 ## 2 High 61 non-null float64 ## 3 Low 61 non-null float64 ## 4 Close 61 non-null float64 ## 5 Volume 61 non-null int64 ## dtypes: float64(4), int64(1), object(1) ## memory usage: 3.0+ KB 15.3.6.2 head goo.head() ## Date Open High Low Close Volume ## 0 12/19/2016 790.219971 797.659973 786.270020 794.200012 1225900 ## 1 12/20/2016 796.760010 798.650024 793.270020 796.419983 925100 ## 2 12/21/2016 795.840027 796.676025 787.099976 794.559998 1208700 ## 3 12/22/2016 792.359985 793.320007 788.580017 791.260010 969100 ## 4 12/23/2016 790.900024 792.739990 787.280029 789.909973 623400 15.4 Class: Timestamp This is an enhanced version to datetime standard library. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html#pandas.Timestamp 15.4.1 Constructor 15.4.1.1 From Number type(pd.Timestamp(2017,12,11)) ## &lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; pd.Timestamp(year=2017, month=1, day=1) # date-like numbers pd.Timestamp(2017,1,1) # date-like numbers pd.Timestamp(2017,12,11,5,45) # datetime-like numbers pd.Timestamp(2017,12,11,5,45,55,999) # + microseconds pd.Timestamp(2017,12,11,5,45,55,999,8) # + nanoseconds ## Timestamp(&#39;2017-01-01 00:00:00&#39;) ## Timestamp(&#39;2017-01-01 00:00:00&#39;) ## Timestamp(&#39;2017-12-11 05:45:00&#39;) ## Timestamp(&#39;2017-12-11 05:45:55.000999&#39;) ## Timestamp(&#39;2017-12-11 05:45:55.000999008&#39;) 15.4.1.2 From String Observe that pandas support many string input format Year Month Day pd.Timestamp(&#39;2017-12-11&#39;) # date-like string: year-month-day pd.Timestamp(&#39;2017 12 11&#39;) # date-like string: year-month-day pd.Timestamp(&#39;2017 Dec 11&#39;) # date-like string: year-month-day pd.Timestamp(&#39;Dec 11, 2017&#39;) # date-like string: year-month-day ## Timestamp(&#39;2017-12-11 00:00:00&#39;) ## Timestamp(&#39;2017-12-11 00:00:00&#39;) ## Timestamp(&#39;2017-12-11 00:00:00&#39;) ## Timestamp(&#39;2017-12-11 00:00:00&#39;) YMD Hour Minute Second Ms pd.Timestamp(&#39;2017-12-11 0545&#39;) ## hour minute pd.Timestamp(&#39;2017-12-11-05:45&#39;) pd.Timestamp(&#39;2017-12-11T0545&#39;) pd.Timestamp(&#39;2017-12-11 054533&#39;) ## hour minute seconds pd.Timestamp(&#39;2017-12-11 05:45:33&#39;) ## Timestamp(&#39;2017-12-11 05:45:00&#39;) ## Timestamp(&#39;2017-12-11 05:45:00&#39;) ## Timestamp(&#39;2017-12-11 05:45:00&#39;) ## Timestamp(&#39;2017-12-11 05:45:33&#39;) ## Timestamp(&#39;2017-12-11 05:45:33&#39;) With Timezone can be included in various ways. pd.Timestamp(&#39;2017-01-01T0545Z&#39;), # GMT pd.Timestamp(&#39;2017-01-01T0545+9&#39;) # GMT+9 pd.Timestamp(&#39;2017-01-01T0545+0800&#39;) # GMT+0800 pd.Timestamp(&#39;2017-01-01 0545&#39;, tz=&#39;Asia/Singapore&#39;) ## (Timestamp(&#39;2017-01-01 05:45:00+0000&#39;, tz=&#39;UTC&#39;),) ## Timestamp(&#39;2017-01-01 05:45:00+0900&#39;, tz=&#39;pytz.FixedOffset(540)&#39;) ## Timestamp(&#39;2017-01-01 05:45:00+0800&#39;, tz=&#39;pytz.FixedOffset(480)&#39;) ## Timestamp(&#39;2017-01-01 05:45:00+0800&#39;, tz=&#39;Asia/Singapore&#39;) 15.4.1.3 From datetime / date pd.Timestamp(date(2017,3,5)) # from date pd.Timestamp(datetime(2017,3,5,4,30)) # from datetime pd.Timestamp(datetime(2017,3,5,4,30), tz=&#39;Asia/Kuala_Lumpur&#39;) # from datetime, + tz ## Timestamp(&#39;2017-03-05 00:00:00&#39;) ## Timestamp(&#39;2017-03-05 04:30:00&#39;) ## Timestamp(&#39;2017-03-05 04:30:00+0800&#39;, tz=&#39;Asia/Kuala_Lumpur&#39;) 15.4.2 Attributes We can tell many things about a Timestamp object. Note that timezone (tz) is a pytz object. ts = pd.Timestamp(&#39;2017-01-01T054533+0800&#39;) # GMT+0800 ts.month ts.day ts.year ts.hour ts.minute ts.second ts.microsecond ts.nanosecond ts.dayofweek ts.dayofyear ts.daysinmonth ts.is_leap_year ts.is_month_end ts.is_month_start ts.dayofweek ts.tz ## pytz object type(ts.tz) ## 1 ## 1 ## 2017 ## 5 ## 45 ## 33 ## 0 ## 0 ## 6 ## 1 ## 31 ## False ## False ## True ## 6 ## pytz.FixedOffset(480) ## &lt;class &#39;pytz._FixedOffset&#39;&gt; 15.4.3 Timezones Adding Timezones and Clock Shifting Use tz_localize to make a timestamp zone aware. It will not shift the clock. Once gotten a timezone, you can easily shift the clock to another timezone using tz_convert(). ts = pd.Timestamp(2017,1,10,10,34) ## No timezone ts ts1 = ts.tz_localize(&#39;Asia/Kuala_Lumpur&#39;) ## Add timezone, retain time ts1 ts1.tz_convert(&#39;UTC&#39;) ## Convert to diff timezone ## Timestamp(&#39;2017-01-10 10:34:00&#39;) ## Timestamp(&#39;2017-01-10 10:34:00+0800&#39;, tz=&#39;Asia/Kuala_Lumpur&#39;) ## Timestamp(&#39;2017-01-10 02:34:00+0000&#39;, tz=&#39;UTC&#39;) Removing Timezone Just apply None with tz_localize to remove TZ info. ts = pd.Timestamp(2017,1,10,10,34).tz_localize(&#39;Asia/Kuala_Lumpur&#39;) ts ts.tz_localize(None) ## no longer TZ aware ## Timestamp(&#39;2017-01-10 10:34:00+0800&#39;, tz=&#39;Asia/Kuala_Lumpur&#39;) ## Timestamp(&#39;2017-01-10 10:34:00&#39;) 15.4.4 Formatting strftime Use strftime() to customize string format. For complete directive, see here. ts = pd.Timestamp(2017,1,10,15,34,10).tz_localize(&#39;Asia/Kuala_Lumpur&#39;) ts ts.strftime(&quot;%Y-%m-%d %b %I:%M:%S %p&quot;) ## Timestamp(&#39;2017-01-10 15:34:10+0800&#39;, tz=&#39;Asia/Kuala_Lumpur&#39;) ## &#39;2017-01-10 Jan 03:34:10 PM&#39; isoformat Use isoformat() to format ISO string (without timezone) ts = pd.Timestamp(2017,1,10,15,34,10) ts1 = ts.tz_localize(&#39;Asia/Kuala_Lumpur&#39;) ts.isoformat() ## without TZ ts1.isoformat() ## with Tz ## &#39;2017-01-10T15:34:10&#39; ## &#39;2017-01-10T15:34:10+08:00&#39; 15.4.5 Conversion 15.4.5.1 Convert To datetime Use to_pydatetime() to convert into standard library datetime.datetime. From the ‘datetime’ object, apply date() to get datetime.date ts = pd.Timestamp(2017,1,10,7,30,52).tz_localize(&#39;Asia/Kuala_Lumpur&#39;) ts ## Timestamp, wtih TZ ts.to_pydatetime() ## datetime, with TZ ts.to_pydatetime().date() ## datetime.date (no TZ) ts.date() ## same as above (no TZ) ## Timestamp(&#39;2017-01-10 07:30:52+0800&#39;, tz=&#39;Asia/Kuala_Lumpur&#39;) ## datetime.datetime(2017, 1, 10, 7, 30, 52, tzinfo=&lt;DstTzInfo &#39;Asia/Kuala_Lumpur&#39; +08+8:00:00 STD&gt;) ## datetime.date(2017, 1, 10) ## datetime.date(2017, 1, 10) 15.4.5.2 Convert To numpy.datetime64 Use to_datetime64() to convert into numpy.datetime64 ts = pd.Timestamp(2017,1,10,7,30,52) ts.to_datetime64() ## numpy.datetime64(&#39;2017-01-10T07:30:52.000000000&#39;) 15.4.6 Updating replace() ts.replace(year=2000, month=1,day=1) ## Timestamp(&#39;2000-01-01 07:30:52&#39;) 15.4.7 Other Utilities 15.4.7.1 Attribute Like Methods ts = pd.Timestamp(&#39;2017-01-01T054533+0800&#39;) # GMT+0800 ts.day_name() ts.month_name() ts.date() ## convert to datetime.date object ts.time() ## convert to datetime.time object ## &#39;Sunday&#39; ## &#39;January&#39; ## datetime.date(2017, 1, 1) ## datetime.time(5, 45, 33) 15.4.7.2 celing - ceil ts ts.ceil(freq=&#39;D&#39;) # ceiling to day ## Timestamp(&#39;2017-01-01 05:45:33+0800&#39;, tz=&#39;pytz.FixedOffset(480)&#39;) ## Timestamp(&#39;2017-01-02 00:00:00+0800&#39;, tz=&#39;pytz.FixedOffset(480)&#39;) 15.5 Class: DateTimeIndex 15.5.1 Creating Refer to Pandas class method above. 15.5.2 Co Conversion Convert To datetime.datetime Use to_pydatetime to convert into python standard datetime.datetime object print(&#39;Converted to List:&#39;, dti.to_pydatetime(), &#39;\\n\\n&#39;, &#39;Converted Type:&#39;, type(dti.to_pydatetime())) ## Converted to List: [datetime.datetime(2011, 1, 3, 0, 0) datetime.datetime(2018, 4, 13, 0, 0) ## datetime.datetime(2018, 3, 1, 7, 30)] ## ## Converted Type: &lt;class &#39;numpy.ndarray&#39;&gt; 15.5.3 Structure Conversion Convert To Series: to_series This creates a Series where index and data with the same value #dti = pd.date_range(&#39;2018-02&#39;, periods=4, freq=&#39;M&#39;) dti.to_series() ## 2011-01-03 00:00:00 2011-01-03 00:00:00 ## 2018-04-13 00:00:00 2018-04-13 00:00:00 ## 2018-03-01 07:30:00 2018-03-01 07:30:00 ## dtype: datetime64[ns] Convert To DataFrame: to_frame() This convert to single column DataFrame with index as the same value dti.to_frame() ## 0 ## 2011-01-03 00:00:00 2011-01-03 00:00:00 ## 2018-04-13 00:00:00 2018-04-13 00:00:00 ## 2018-03-01 07:30:00 2018-03-01 07:30:00 15.5.4 Attributes All Timestamp Attributes can be used upon DateTimeIndex. print( dti.weekday, &#39;\\n&#39;, dti.month, &#39;\\n&#39;, dti.daysinmonth) ## Int64Index([0, 4, 3], dtype=&#39;int64&#39;) ## Int64Index([1, 4, 3], dtype=&#39;int64&#39;) ## Int64Index([31, 30, 31], dtype=&#39;int64&#39;) 15.6 Class: Series Series allows different data types (object class) as its element pandas.Series(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False) - data array-like, iterable, dict or scalar - If dtype not specified, it will infer from data. 15.6.1 Constructor 15.6.1.1 Empty Series Passing no data to constructor will result in empty series. By default, empty series dtype is float. s = pd.Series(dtype=&#39;object&#39;) print (s, &#39;\\n&#39;, type(s)) ## Series([], dtype: object) ## &lt;class &#39;pandas.core.series.Series&#39;&gt; 15.6.1.2 From Scalar If data is a scalar value, an index must be provided. The value will be repeated to match the length of index pd.Series( 99, index = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;]) ## a 99 ## b 99 ## c 99 ## d 99 ## dtype: int64 15.6.1.3 From array-like From list pd.Series([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;]) # from Python list ## 0 a ## 1 b ## 2 c ## 3 d ## 4 e ## dtype: object From numpy.array If index is not specified, default to 0 and continue incrementally pd.Series(np.array([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;])) ## 0 a ## 1 b ## 2 c ## 3 d ## 4 e ## dtype: object From DateTimeIndex pd.Series(pd.date_range(&#39;2011-1-1&#39;,&#39;2011-1-3&#39;)) ## 0 2011-01-01 ## 1 2011-01-02 ## 2 2011-01-03 ## dtype: datetime64[ns] 15.6.1.4 From Dictionary The dictionary key will be the index. Order is not sorted. pd.Series({&#39;a&#39; : 0., &#39;c&#39; : 5., &#39;b&#39; : 2.}) ## a 0.0 ## c 5.0 ## b 2.0 ## dtype: float64 If index sequence is specifeid, then Series will forllow the index order Objerve that missing data (index without value) will be marked as NaN pd.Series({&#39;a&#39; : 0., &#39;c&#39; : 1., &#39;b&#39; : 2.},index = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;]) ## a 0.0 ## b 2.0 ## c 1.0 ## d NaN ## dtype: float64 15.6.1.5 Specify Index pd.Series([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;], index=[10,20,30,40,50]) ## 10 a ## 20 b ## 30 c ## 40 d ## 50 e ## dtype: object 15.6.1.6 Mix Element Types dType will be ‘object’ when there were mixture of classes ser = pd.Series([&#39;a&#39;,1,2,3]) print(&#39;Object Type : &#39;, type(ser),&#39;\\n&#39;, &#39;Object dType: &#39;, ser.dtype,&#39;\\n&#39;, &#39;Element 1 Type: &#39;,type(ser[0]),&#39;\\n&#39;, &#39;Elmeent 2 Type: &#39;,type(ser[1])) ## Object Type : &lt;class &#39;pandas.core.series.Series&#39;&gt; ## Object dType: object ## Element 1 Type: &lt;class &#39;str&#39;&gt; ## Elmeent 2 Type: &lt;class &#39;int&#39;&gt; 15.6.1.7 Specify Data Types By default, dtype is inferred from data. ser1 = pd.Series([1,2,3]) ser2 = pd.Series([1,2,3], dtype=&quot;int8&quot;) ser3 = pd.Series([1,2,3], dtype=&quot;object&quot;) print(&#39; Inferred: &#39;,ser1.dtype, &#39;\\n&#39;, &#39;Specified int8: &#39;,ser2.dtype, &#39;\\n&#39;, &#39;Specified object:&#39;,ser3.dtype) ## Inferred: int64 ## Specified int8: int8 ## Specified object: object 15.6.2 Accessing Series series ( single/list/range_of_row_label/number ) # can cause confusion series.loc ( single/list/range_of_row_label ) series.iloc( single/list/range_of_row_number ) 15.6.2.1 Sample Data s = pd.Series([1,2,3,4,5],index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;]) s ## a 1 ## b 2 ## c 3 ## d 4 ## e 5 ## dtype: int64 15.6.2.2 by Row Number(s) Single Item. Notice that inputing a number and list of number give different result. print( &#39;Referencing by number:&#39;,s.iloc[1],&#39;\\n\\n&#39;, &#39;\\nReferencing by list of number:\\n&#39;,s.iloc[[1]]) ## Referencing by number: 2 ## ## ## Referencing by list of number: ## b 2 ## dtype: int64 Multiple Items s.iloc[[1,3]] ## b 2 ## d 4 ## dtype: int64 Range (First 3) s.iloc[:3] ## a 1 ## b 2 ## c 3 ## dtype: int64 Range (Last 3) s.iloc[-3:] ## c 3 ## d 4 ## e 5 ## dtype: int64 Range (in between) s.iloc[2:3] ## c 3 ## dtype: int64 15.6.2.3 by Index(es) Single Label. Notice the difference referencing input: single index and list of index. Warning: if index is invalid, this will result in error. print( s.loc[&#39;c&#39;], &#39;\\n&#39;, s[[&#39;c&#39;]]) ## 3 ## c 3 ## dtype: int64 Multiple Labels If index is not found, it will return NaN s.loc[[&#39;k&#39;,&#39;c&#39;]] ## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: &quot;[&#39;k&#39;] not in index&quot; ** Range of Labels ** s.loc[&#39;b&#39;:&#39;d&#39;] ## b 2 ## c 3 ## d 4 ## dtype: int64 15.6.2.4 Filtering Use logical array to filter s = pd.Series(range(1,8)) s[s&lt;5] ## 0 1 ## 1 2 ## 2 3 ## 3 4 ## dtype: int64 Use where The where method is an application of the if-then idiom. For each element in the calling Series, if cond is True the element is used; otherwise other is used. .where(cond, other=nan, inplace=False) print(s.where(s&lt;4),&#39;\\n\\n&#39;, s.where(s&lt;4,other=None) ) ## 0 1.0 ## 1 2.0 ## 2 3.0 ## 3 NaN ## 4 NaN ## 5 NaN ## 6 NaN ## dtype: float64 ## ## 0 1.0 ## 1 2.0 ## 2 3.0 ## 3 NaN ## 4 NaN ## 5 NaN ## 6 NaN ## dtype: float64 15.6.3 Updating Series 15.6.3.1 by Row Number(s) s = pd.Series(range(1,7), index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;]) s[2] = 999 s[[3,4]] = 888,777 s ## a 1 ## b 2 ## c 999 ## d 888 ## e 777 ## f 6 ## dtype: int64 15.6.3.2 by Index(es) s = pd.Series(range(1,7), index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;]) s[&#39;e&#39;] = 888 s[[&#39;c&#39;,&#39;d&#39;]] = 777,888 s ## a 1 ## b 2 ## c 777 ## d 888 ## e 888 ## f 6 ## dtype: int64 15.6.4 Series Attributes 15.6.4.1 The Data s = pd.Series([1,2,3,4,5],index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;],name=&#39;SuperHero&#39;) s ## a 1 ## b 2 ## c 3 ## d 4 ## e 5 ## Name: SuperHero, dtype: int64 15.6.4.2 The Attributes print( &#39; Series Index: &#39;,s.index, &#39;\\n&#39;, &#39;Series dType: &#39;, s.dtype, &#39;\\n&#39;, &#39;Series Size: &#39;, s.size, &#39;\\n&#39;, &#39;Series Shape: &#39;, s.shape, &#39;\\n&#39;, &#39;Series Dimension:&#39;, s.ndim) ## Series Index: Index([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], dtype=&#39;object&#39;) ## Series dType: int64 ## Series Size: 5 ## Series Shape: (5,) ## Series Dimension: 1 15.6.5 Instance Methods 15.6.5.1 Index Manipulation .rename_axis() s.rename_axis(&#39;haribulan&#39;) ## haribulan ## a 1 ## b 2 ## c 3 ## d 4 ## e 5 ## Name: SuperHero, dtype: int64 .reset_index() Resetting index will: - Convert index to a normal column, with column named as ‘index’ - Index renumbered to 1,2,3 - Return DataFrame (became two columns) s.reset_index() ## index SuperHero ## 0 a 1 ## 1 b 2 ## 2 c 3 ## 3 d 4 ## 4 e 5 15.6.5.2 Structure Conversion A series structure contain value (in numpy array), its dtype (data type of the numpy array). Use values to retrieve into `numpy.ndarray. Use dtype to understand the data type. s = pd.Series([1,2,3,4,5]) print(&#39; Series value: &#39;, s.values, &#39;\\n&#39;, &#39;Series value type: &#39;, type(s.values), &#39;\\n&#39;, &#39;Series dtype: &#39;, s.dtype) ## Series value: [1 2 3 4 5] ## Series value type: &lt;class &#39;numpy.ndarray&#39;&gt; ## Series dtype: int64 Convert To List using .tolist() pd.Series.tolist(s) ## [1, 2, 3, 4, 5] 15.6.5.3 DataType Conversion Use astype() to convert to another numpy supproted datatypes, results in a new Series. Warning: casting to incompatible type will result in error s.astype(&#39;int8&#39;) ## 0 1 ## 1 2 ## 2 3 ## 3 4 ## 4 5 ## dtype: int8 15.6.6 Operators The result of applying operator (arithmetic or logic) to Series object returns a new Series object 15.6.6.1 Arithmetic Operator s1 = pd.Series( [100,200,300,400,500] ) s2 = pd.Series( [10, 20, 30, 40, 50] ) Apply To One Series Object s1 - 100 ## 0 0 ## 1 100 ## 2 200 ## 3 300 ## 4 400 ## dtype: int64 Apply To Two Series Objects s1 - s2 ## 0 90 ## 1 180 ## 2 270 ## 3 360 ## 4 450 ## dtype: int64 15.6.6.2 Logic Operator Apply logic operator to a Series return a new Series of boolean result This can be used for Series or DataFrame filtering bs = pd.Series(range(0,10)) bs&gt;3 ## 0 False ## 1 False ## 2 False ## 3 False ## 4 True ## 5 True ## 6 True ## 7 True ## 8 True ## 9 True ## dtype: bool ~((bs&gt;3) &amp; (bs&lt;8) | (bs&gt;7)) ## 0 True ## 1 True ## 2 True ## 3 True ## 4 False ## 5 False ## 6 False ## 7 False ## 8 False ## 9 False ## dtype: bool 15.6.7 String Accesor .str If the underlying data is str type, then pandas exposed various properties and methos through str accessor. SeriesObj.str.operatorFunction() Available Functions Nearly all Python’s built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas str methods that mirror Python string methods: len() lower() translate() islower() ljust() upper() startswith() isupper() rjust() find() endswith() isnumeric() center() rfind() isalnum() isdecimal() zfill() index() isalpha() split() strip() rindex() isdigit() rsplit() rstrip() capitalize() isspace() partition() lstrip() swapcase() istitle() rpartition() 15.6.7.1 Regex Extractor Extract capture groups in the regex pattern, by default in DataFrame (expand=True). Series.str.extract(self, pat, flags=0, expand=True) - expand=True: if result is single column, make it a Series instead of Dataframe. s = pd.Series([&#39;a1&#39;, &#39;b2&#39;, &#39;c3&#39;]) print( &#39; Extracted Dataframe:\\n&#39;, s.str.extract(r&#39;([ab])(\\d)&#39;),&#39;\\n\\n&#39;, &#39;Extracted Dataframe witn Names:\\n&#39;, s.str.extract(r&#39;(?P&lt;Letter&gt;[ab])(\\d)&#39;)) ## Extracted Dataframe: ## 0 1 ## 0 a 1 ## 1 b 2 ## 2 NaN NaN ## ## Extracted Dataframe witn Names: ## Letter 1 ## 0 a 1 ## 1 b 2 ## 2 NaN NaN Below ouptut single columne, use expand=False to make the result a Series, instead of DataFrame. r = s.str.extract(r&#39;[ab](\\d)&#39;, expand=False) print( r, &#39;\\n\\n&#39;, type(r) ) ## 0 1 ## 1 2 ## 2 NaN ## dtype: object ## ## &lt;class &#39;pandas.core.series.Series&#39;&gt; 15.6.7.2 Character Extractor monte = pd.Series([&#39;Graham Chapman&#39;, &#39;John Cleese&#39;, &#39;Terry Gilliam&#39;, &#39;Eric Idle&#39;, &#39;Terry Jones&#39;, &#39;Michael Palin&#39;]) monte ## 0 Graham Chapman ## 1 John Cleese ## 2 Terry Gilliam ## 3 Eric Idle ## 4 Terry Jones ## 5 Michael Palin ## dtype: object startwith monte.str.startswith(&#39;T&#39;) ## 0 False ## 1 False ## 2 True ## 3 False ## 4 True ## 5 False ## dtype: bool Slicing monte.str[0:3] ## 0 Gra ## 1 Joh ## 2 Ter ## 3 Eri ## 4 Ter ## 5 Mic ## dtype: object 15.6.7.3 Splitting Split strings around given separator/delimiter in either string or regex. Series.str.split(self, pat=None, n=-1, expand=False) - pat: can be string or regex s = pd.Series([&#39;a_b_c&#39;, &#39;c_d_e&#39;, np.nan, &#39;f_g_h_i_j&#39;]) s ## 0 a_b_c ## 1 c_d_e ## 2 NaN ## 3 f_g_h_i_j ## dtype: object str.split() by default, split will split each item into array s.str.split(&#39;_&#39;) ## 0 [a, b, c] ## 1 [c, d, e] ## 2 NaN ## 3 [f, g, h, i, j] ## dtype: object expand=True will return a dataframe instead of series. By default, expand split into all possible columns. print( s.str.split(&#39;_&#39;, expand=True) ) ## 0 1 2 3 4 ## 0 a b c None None ## 1 c d e None None ## 2 NaN NaN NaN NaN NaN ## 3 f g h i j It is possible to limit the number of columns splitted print( s.str.split(&#39;_&#39;, expand=True, n=1) ) ## 0 1 ## 0 a b_c ## 1 c d_e ## 2 NaN NaN ## 3 f g_h_i_j str.rsplit() rsplit stands for reverse split, it works the same way, except it is reversed print( s.str.rsplit(&#39;_&#39;, expand=True, n=1) ) ## 0 1 ## 0 a_b c ## 1 c_d e ## 2 NaN NaN ## 3 f_g_h_i j 15.6.7.4 Case Conversion SeriesObj.str.upper() SeriesObj.str.lower() SeriesObj.str.capitalize() s = pd.Series([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;aAba&#39;, &#39;bBaca&#39;, np.nan, &#39;cCABA&#39;, &#39;dog&#39;, &#39;cat&#39;]) print( s.str.upper(), &#39;\\n&#39;, s.str.capitalize()) ## 0 A ## 1 B ## 2 C ## 3 AABA ## 4 BBACA ## 5 NaN ## 6 CCABA ## 7 DOG ## 8 CAT ## dtype: object ## 0 A ## 1 B ## 2 C ## 3 Aaba ## 4 Bbaca ## 5 NaN ## 6 Ccaba ## 7 Dog ## 8 Cat ## dtype: object 15.6.7.5 Number of Characters s.str.len() ## 0 1.0 ## 1 1.0 ## 2 1.0 ## 3 4.0 ## 4 5.0 ## 5 NaN ## 6 5.0 ## 7 3.0 ## 8 3.0 ## dtype: float64 15.6.7.6 String Indexing This return specified character from each item. s = pd.Series([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;Aaba&#39;, &#39;Baca&#39;, np.nan,&#39;CABA&#39;, &#39;dog&#39;, &#39;cat&#39;]) s.str[0].values # first char ## array([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;A&#39;, &#39;B&#39;, nan, &#39;C&#39;, &#39;d&#39;, &#39;c&#39;], dtype=object) s.str[0:2].values # first and second char ## array([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;Aa&#39;, &#39;Ba&#39;, nan, &#39;CA&#39;, &#39;do&#39;, &#39;ca&#39;], dtype=object) 15.6.7.7 Substring Extraction Sample Data s = pd.Series([&#39;a1&#39;, &#39;b2&#39;, &#39;c3&#39;]) s ## 0 a1 ## 1 b2 ## 2 c3 ## dtype: object Extract absed on regex matching … to improve … type(s.str.extract(&#39;([ab])(\\d)&#39;, expand=False)) ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; 15.6.8 Date Accessor .dt If the underlying data is datetime64 type, then pandas exposed various properties and methos through dt accessor. 15.6.8.1 Sample Data s = pd.Series([ datetime(2000,1,1,0,0,0), datetime(1999,12,15,12,34,55), datetime(2020,3,8,5,7,12), datetime(2018,1,1,0,0,0), datetime(2003,3,4,5,6,7) ]) s ## 0 2000-01-01 00:00:00 ## 1 1999-12-15 12:34:55 ## 2 2020-03-08 05:07:12 ## 3 2018-01-01 00:00:00 ## 4 2003-03-04 05:06:07 ## dtype: datetime64[ns] 15.6.8.2 Convert To datetime.datetime Use to_pydatetime() to convert into numpy.array of standard library datetime.datetime pdt = s.dt.to_pydatetime() print( type(pdt) ) ## &lt;class &#39;numpy.ndarray&#39;&gt; pdt ## array([datetime.datetime(2000, 1, 1, 0, 0), ## datetime.datetime(1999, 12, 15, 12, 34, 55), ## datetime.datetime(2020, 3, 8, 5, 7, 12), ## datetime.datetime(2018, 1, 1, 0, 0), ## datetime.datetime(2003, 3, 4, 5, 6, 7)], dtype=object) datetime.date Use dt.date to convert into pandas.Series of standard library datetime.date Is it possible to have a pandas.Series of datetime.datetime ? No, because Pandas want it as its own Timestamp. sdt = s.dt.date print( type(sdt[1] )) ## &lt;class &#39;datetime.date&#39;&gt; print( type(sdt)) ## &lt;class &#39;pandas.core.series.Series&#39;&gt; sdt ## 0 2000-01-01 ## 1 1999-12-15 ## 2 2020-03-08 ## 3 2018-01-01 ## 4 2003-03-04 ## dtype: object 15.6.8.3 Timestamp Attributes A Series::DateTime object support below properties: - date - month - day - year - dayofweek - dayofyear - weekday - weekday_name - quarter - daysinmonth - hour - minute Full list below: https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties s.dt.date ## 0 2000-01-01 ## 1 1999-12-15 ## 2 2020-03-08 ## 3 2018-01-01 ## 4 2003-03-04 ## dtype: object s.dt.month ## 0 1 ## 1 12 ## 2 3 ## 3 1 ## 4 3 ## dtype: int64 s.dt.dayofweek ## 0 5 ## 1 2 ## 2 6 ## 3 0 ## 4 1 ## dtype: int64 s.dt.weekday ## 0 5 ## 1 2 ## 2 6 ## 3 0 ## 4 1 ## dtype: int64 s.dt.quarter ## 0 1 ## 1 4 ## 2 1 ## 3 1 ## 4 1 ## dtype: int64 s.dt.daysinmonth ## 0 31 ## 1 31 ## 2 31 ## 3 31 ## 4 31 ## dtype: int64 s.dt.time # extract time as time Object ## 0 00:00:00 ## 1 12:34:55 ## 2 05:07:12 ## 3 00:00:00 ## 4 05:06:07 ## dtype: object s.dt.hour # extract hour as integer ## 0 0 ## 1 12 ## 2 5 ## 3 0 ## 4 5 ## dtype: int64 s.dt.minute # extract minute as integer ## 0 0 ## 1 34 ## 2 7 ## 3 0 ## 4 6 ## dtype: int64 15.7 Class: DataFrame 15.7.1 Constructor pd.DataFrame() constructor creates DataFrame object. When index (row label) and column names are not specified (or cannot be induced from data source), it will default to 0,1,2,3… Specify Row Label and Column Header with columns and index parameters in constructor. When columns is specified AND data source has keys that can be used as column names, pandas will select those column names and order specified in columns parameter. index and row label are used interchangeably in this book 15.7.1.1 Empty DataFrame By default, An empty dataframe contain no columns and index. empty_df1 = pd.DataFrame() empty_df2 = pd.DataFrame() empty_df1 id(empty_df1) id(empty_df2) ## Empty DataFrame ## Columns: [] ## Index: [] ## 2282692804768 ## 2282692807984 Even though it is empty, you can still initialize it with Index Columns. ## empty dataframe with columns empty_df = pd.DataFrame(columns=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;]) empty_df ## index is empty ## empty dataframe with columns and index empty_df = pd.DataFrame(columns=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;], index=[1,2,3]) empty_df ## Empty DataFrame ## Columns: [A, B, C] ## Index: [] ## A B C ## 1 NaN NaN NaN ## 2 NaN NaN NaN ## 3 NaN NaN NaN When initializing multiple empty DataFrame at once (in one line), all the empty DataFrame refers to same memory location. Meaning they contain similar data. empty_df1 = empty_df2 = pd.DataFrame() id(empty_df1) id(empty_df2) ## 2282692805776 ## 2282692805776 15.7.1.2 From Row Oriented Data (List of Lists) DataFrame( [row_list1, row_list2, row_list3] ) DataFrame( [row_list1, row_list2, row_list3], column = columnName_list ) DataFrame( [row_list1, row_list2, row_list3], index = row_label_list ) Create DataFrame object from List of Lists Use `columns’ to select the columns and preferred order. data = [ [101, &#39;Alice&#39;, 40000, 2017], [102, &#39;Bob&#39;, 24000, 2017], [103, &#39;Charles&#39;,31000, 2017]] ## Initialize with default row label and column names pd.DataFrame(data) ## 0 1 2 3 ## 0 101 Alice 40000 2017 ## 1 102 Bob 24000 2017 ## 2 103 Charles 31000 2017 ## Initialize with column names and row index col_names = [&#39;empID&#39;,&#39;name&#39;,&#39;salary&#39;,&#39;year&#39;] row_labels = [&#39;r1&#39;,&#39;r2&#39;,&#39;r3&#39;] pd.DataFrame(data, columns=col_names, index=row_labels) ## empID name salary year ## r1 101 Alice 40000 2017 ## r2 102 Bob 24000 2017 ## r3 103 Charles 31000 2017 15.7.1.3 From Record Oriented Data (List of Dict) DataFrame( [dict1, dict2, dict3] ) DataFrame( [dict1, dict2, dict3], column = column_list ) DataFrame( [dict1, dict2, dict3], index = row_label_list ) Create DataFrame object from List of Dicts. By default, Column Name will follow Dictionary Key, unless manually specified. Certain dict object may have missing keys, these are assumed as NaN by pandas. Use `columns’ to select the columns and preferred order. data = [ {&quot;name&quot;:&quot;Yong&quot;, &quot;id&quot;:1,&quot;zkey&quot;:101}, {&quot;name&quot;:&quot;Gan&quot;, &quot;id&quot;:2 }] ## missing zkey in this record ## Default Index and include all detected columns pd.DataFrame(data) ## name id zkey ## 0 Yong 1 101.0 ## 1 Gan 2 NaN ## specify Index, must be same length of DataFrame pd.DataFrame(data, index=[&#39;row1&#39;,&#39;row2&#39;]) ## name id zkey ## row1 Yong 1 101.0 ## row2 Gan 2 NaN ## Filter Which Columns To Include, in the prefered order pd.DataFrame(data, columns=[&#39;zkey&#39;,&#39;name&#39;]) ## zkey name ## 0 101.0 Yong ## 1 NaN Gan 15.7.1.4 From Column Oriented Data (Dict of Lists) DataFrame( data = {&#39;column1&#39;: list1, &#39;column2&#39;: list2, &#39;column3&#39;: list3 } , index = row_label_list, columns = column_list) Create DataFrame object from Dict of Lists. Constructor will arrange the columns according to the dict keys order by default, unless columns is specified. Use `columns’ to select the columns in preferred order. NaN column is returned if not exist. ## columns arrangement default to dict&#39;s keys data = {&#39;empID&#39;: [100, 101, 102, 103, 104], &#39;year1&#39;: [2017, 2017, 2017, 2018, 2018], &#39;year2&#39;: [2027, 2027, 2027, 2028, 2028], &#39;salary&#39;: [40000, 24000, 31000, 20000, 30000], &#39;name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charles&#39;, &#39;David&#39;, &#39;Eric&#39;]} ## Default Index and Include All Columns pd.DataFrame(data) ## empID year1 year2 salary name ## 0 100 2017 2027 40000 Alice ## 1 101 2017 2027 24000 Bob ## 2 102 2017 2027 31000 Charles ## 3 103 2018 2028 20000 David ## 4 104 2018 2028 30000 Eric ## Select What Columns To Show (and its order) Specify row label. pd.DataFrame(data, columns=[&#39;name&#39;,&#39;salary&#39;,&#39;year1&#39;,&#39;year2&#39;,&#39;not_exist&#39;], index=data.get(&#39;empID&#39;)) ## name salary year1 year2 not_exist ## 100 Alice 40000 2017 2027 NaN ## 101 Bob 24000 2017 2027 NaN ## 102 Charles 31000 2017 2027 NaN ## 103 David 20000 2018 2028 NaN ## 104 Eric 30000 2018 2028 NaN 15.7.2 Operators 15.7.2.1 Sample Data Two dataframes are created, each with 3 columns and 3 rows. However, there are only two matching column and row names. We shall notice that the operator will perform cell-wise, honoring the row/column name. df1 = pd.DataFrame(data= {&#39;idx&#39;: [&#39;row1&#39;,&#39;row2&#39;,&#39;row3&#39;], &#39;x&#39;: [10, 20, 30], &#39;y&#39;: [1,2,3], &#39;z&#39;: [0.1, 0.2, 0.3]}).set_index(&#39;idx&#39;) df2 = pd.DataFrame(data= {&#39;idx&#39;: [&#39;row1&#39;,&#39;row2&#39;,&#39;row4&#39;], &#39;x&#39;: [13, 23, 33], &#39;z&#39;: [0.1, 0.2, 0.3], &#39;k&#39;: [11,21,31]}).set_index(&#39;idx&#39;) 15.7.2.2 Addition Adding Two DataFrame When Using + operator, non-matching row/column names will result in NA. .add(), has the same result as + by default. However, add supports fill_value= parameter. When specified, ONE-SIDED none matching cells is assumed to have value specified in fill_value. Only BOTH-SIDED non matching cells will results in NaN. df1 + df2 df1.add(df2) df1.add(df2, fill_value=1) ## k x y z ## idx ## row1 NaN 23.0 NaN 0.2 ## row2 NaN 43.0 NaN 0.4 ## row3 NaN NaN NaN NaN ## row4 NaN NaN NaN NaN ## k x y z ## idx ## row1 NaN 23.0 NaN 0.2 ## row2 NaN 43.0 NaN 0.4 ## row3 NaN NaN NaN NaN ## row4 NaN NaN NaN NaN ## k x y z ## idx ## row1 12.0 23.0 2.0 0.2 ## row2 22.0 43.0 3.0 0.4 ## row3 NaN 31.0 4.0 1.3 ## row4 32.0 34.0 NaN 1.3 Adding Series To DataFrame Specify the appropriate axis depending on the orientation of the series data. Column and Row names are respected in this operation. However, fill_value is not applicable is not available on Series. Columns or rows in Series that are non-matching in DataFrame, they will be created as the result. This behavior is similar to adding two DataFrames. ## Series to add into Rows s3 = pd.Series([1,1,1], index=[&#39;row1&#39;,&#39;row2&#39;,&#39;row4&#39;]) ## Series to add into Columns s4 = pd.Series([3,3,3], index=[&#39;x&#39;,&#39;y&#39;,&#39;s&#39;]) print(&#39;Original DataFrame:\\n&#39;,df1,&#39;\\n\\n&#39;, &#39;Add Series as Rows: \\n&#39;, df1.add(s3, axis=0), &#39;\\n\\n&#39;, &#39;Add Series as Columns: \\n&#39;, df1.add(s4, axis=1)) ## Original DataFrame: ## x y z ## idx ## row1 10 1 0.1 ## row2 20 2 0.2 ## row3 30 3 0.3 ## ## Add Series as Rows: ## x y z ## row1 11.0 2.0 1.1 ## row2 21.0 3.0 1.2 ## row3 NaN NaN NaN ## row4 NaN NaN NaN ## ## Add Series as Columns: ## s x y z ## idx ## row1 NaN 13.0 4.0 NaN ## row2 NaN 23.0 5.0 NaN ## row3 NaN 33.0 6.0 NaN 15.7.2.3 Substraction Refer to .add() above for fill_value explanation. df2 - df1 df2.sub(df1,fill_value=1000) ## k x y z ## idx ## row1 NaN 3.0 NaN 0.0 ## row2 NaN 3.0 NaN 0.0 ## row3 NaN NaN NaN NaN ## row4 NaN NaN NaN NaN ## k x y z ## idx ## row1 -989.0 3.0 999.0 0.0 ## row2 -979.0 3.0 998.0 0.0 ## row3 NaN 970.0 997.0 999.7 ## row4 -969.0 -967.0 NaN -999.7 15.7.2.4 Operator &amp; Non matching columns returns NaN. Matching columns with both True value returns True Matching columns with both True and False returns False df1&gt;5 df2&lt;20 (df1&gt;5) &amp; (df2&lt;20) ## x y z ## idx ## row1 True False False ## row2 True False False ## row3 True False False ## x z k ## idx ## row1 True True True ## row2 False True False ## row4 False True False ## k x y z ## idx ## row1 NaN True NaN False ## row2 NaN False NaN False ## row3 NaN False NaN False ## row4 NaN False NaN False 15.7.3 Attributes df = pd.DataFrame(data) df ## empID year1 year2 salary name ## 0 100 2017 2027 40000 Alice ## 1 101 2017 2027 24000 Bob ## 2 102 2017 2027 31000 Charles ## 3 103 2018 2028 20000 David ## 4 104 2018 2028 30000 Eric df.shape ## tuple (rows, columns) df.index ## default index is RangeIndex df.index.values ## array of integer df.columns ## array of string df.columns.values ## array of string df.values ## array of list ## (5, 5) ## RangeIndex(start=0, stop=5, step=1) ## array([0, 1, 2, 3, 4], dtype=int64) ## Index([&#39;empID&#39;, &#39;year1&#39;, &#39;year2&#39;, &#39;salary&#39;, &#39;name&#39;], dtype=&#39;object&#39;) ## array([&#39;empID&#39;, &#39;year1&#39;, &#39;year2&#39;, &#39;salary&#39;, &#39;name&#39;], dtype=object) ## array([[100, 2017, 2027, 40000, &#39;Alice&#39;], ## [101, 2017, 2027, 24000, &#39;Bob&#39;], ## [102, 2017, 2027, 31000, &#39;Charles&#39;], ## [103, 2018, 2028, 20000, &#39;David&#39;], ## [104, 2018, 2028, 30000, &#39;Eric&#39;]], dtype=object) 15.7.4 Index Manipulation 15.7.4.1 Sample Data df ## empID year1 year2 salary name ## 0 100 2017 2027 40000 Alice ## 1 101 2017 2027 24000 Bob ## 2 102 2017 2027 31000 Charles ## 3 103 2018 2028 20000 David ## 4 104 2018 2028 30000 Eric 15.7.4.2 Convert Column To Index set_index(&#39;column_name&#39;, inplace=False) Specified column will turn into Index inplace=True means don’t create a new dataframe. Modify existing DataFrame. inplace=False means return a new DataFrame df.index ## 0,1,2,3,... df.set_index(&#39;empID&#39;,inplace=True) df.index ## 100,101,102,etc df ## RangeIndex(start=0, stop=5, step=1) ## Int64Index([100, 101, 102, 103, 104], dtype=&#39;int64&#39;, name=&#39;empID&#39;) ## year1 year2 salary name ## empID ## 100 2017 2027 40000 Alice ## 101 2017 2027 24000 Bob ## 102 2017 2027 31000 Charles ## 103 2018 2028 20000 David ## 104 2018 2028 30000 Eric 15.7.4.3 Convert Index To Column Resetting index will re-sequence the index as 0,1,2 etc. Old index column will be converted back as normal column, its name retained as column name. Operation support inplace option. df.reset_index(inplace=True) df ## empID year1 year2 salary name ## 0 100 2017 2027 40000 Alice ## 1 101 2017 2027 24000 Bob ## 2 102 2017 2027 31000 Charles ## 3 103 2018 2028 20000 David ## 4 104 2018 2028 30000 Eric 15.7.4.4 Updating Index ( .index= ) This simply replace all index with the new values. Number of elements in the new index must match length of DataFrame, otherwise error. Although same label are allowed to repeat, it will be deprecated in future. This operation not reversible. df.index = [201, 202, 203, 204, 205] df ## empID year1 year2 salary name ## 201 100 2017 2027 40000 Alice ## 202 101 2017 2027 24000 Bob ## 203 102 2017 2027 31000 Charles ## 204 103 2018 2028 20000 David ## 205 104 2018 2028 30000 Eric 15.7.4.5 Reordering Index (.reindex ) Returns new DataFrame according to the order specified. NO ‘inplace’ option. Doesn’t work if duplicate labels exists (Error) Only matching rows are returned. Non matching index number will results in NaN, without Error. Change the order of Index, always return a new dataframe df.reindex([203,202,300]) ## empID year1 year2 salary name ## 203 102.0 2017.0 2027.0 31000.0 Charles ## 202 101.0 2017.0 2027.0 24000.0 Bob ## 300 NaN NaN NaN NaN NaN 15.7.4.6 Rename Axis Example below renamed the axis of index. Use axis=0 for row index (default), use axis=1 for column index. = Use inplace option to apply changes to the DataFrame, otherwise it will return the a new DataFrame df.rename_axis(&#39;super_id&#39;, axis=0, inplace=True) df ## empID year1 year2 salary name ## super_id ## 201 100 2017 2027 40000 Alice ## 202 101 2017 2027 24000 Bob ## 203 102 2017 2027 31000 Charles ## 204 103 2018 2028 20000 David ## 205 104 2018 2028 30000 Eric 15.7.5 Subsetting Rows dataframe.loc[ row_label ] # return series, single row dataframe.loc[ row_label_list ] # multiple rows dataframe.loc[ boolean_list ] # multiple rows dataframe.iloc[ row_number ] # return series, single row dataframe.iloc[ row_number_list ] # multiple rows dataframe.iloc[ number_range ] # multiple rows dataframe.sample(frac=) # frac = 0.6 means sampling 60% of rows randomly 15.7.5.1 Sample Data df = pd.DataFrame(data).set_index(&#39;empID&#39;) df df2 = pd.DataFrame(data, index = [&#39;row2&#39;,&#39;row3&#39;,&#39;row1&#39;,&#39;row5&#39;,&#39;row4&#39;]) df2 ## year1 year2 salary name ## empID ## 100 2017 2027 40000 Alice ## 101 2017 2027 24000 Bob ## 102 2017 2027 31000 Charles ## 103 2018 2028 20000 David ## 104 2018 2028 30000 Eric ## empID year1 year2 salary name ## row2 100 2017 2027 40000 Alice ## row3 101 2017 2027 24000 Bob ## row1 102 2017 2027 31000 Charles ## row5 103 2018 2028 20000 David ## row4 104 2018 2028 30000 Eric 15.7.5.2 Using Row Label (loc) Non matching single row label will result in KeyError. Non matching range of row labels will not trigger error. Row Label is number df.loc[ 101] # by single row label, return Series ## year1 2017 ## year2 2027 ## salary 24000 ## name Bob ## Name: 101, dtype: object df.loc[ [100,103] ] # by multiple row labels, returns DataFrame ## year1 year2 salary name ## empID ## 100 2017 2027 40000 Alice ## 103 2018 2028 20000 David df.loc[ 100:103 ] # by range of row labels, returns DataFrame ## year1 year2 salary name ## empID ## 100 2017 2027 40000 Alice ## 101 2017 2027 24000 Bob ## 102 2017 2027 31000 Charles ## 103 2018 2028 20000 David df.loc[ 999:9999 ] # invalid range, No Error !! ## Empty DataFrame ## Columns: [year1, year2, salary, name] ## Index: [] df.loc[ 999 ] # invalid key, KeyError ## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: 999 Row Label is string df2.loc[ &#39;row3&#39; ] # by single row label, return Series ## empID 101 ## year1 2017 ## year2 2027 ## salary 24000 ## name Bob ## Name: row3, dtype: object df2.loc[ [&#39;row1&#39;,&#39;row3&#39;] ] # by multiple row labels, returns DataFrame ## empID year1 year2 salary name ## row1 102 2017 2027 31000 Charles ## row3 101 2017 2027 24000 Bob df2.loc[ &#39;row1&#39;:&#39;row3&#39; ] # by range of row labels, return empty DataFrame because there is no row3 after row1 ## Empty DataFrame ## Columns: [empID, year1, year2, salary, name] ## Index: [] df2.loc[ &#39;row1&#39;:&#39;row4&#39; ] # by range of row labels, returns DataFrame ## empID year1 year2 salary name ## row1 102 2017 2027 31000 Charles ## row5 103 2018 2028 20000 David ## row4 104 2018 2028 30000 Eric df2.loc[ &#39;row1&#39;:&#39;baba&#39; ] # Invalid range, KeyError ## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: &#39;baba&#39; 15.7.5.3 Using Row Number (iloc) Multiple rows returned as dataframe object df.iloc[1] # by single row number df.iloc[ [0,3] ] # by row numbers df.iloc[ 0:3 ] # by row number range df.iloc[ 0:999 ] # invalid range, no error df.iloc[999] # invalid row, IndexError ## year1 2017 ## year2 2027 ## salary 24000 ## name Bob ## Name: 101, dtype: object ## year1 year2 salary name ## empID ## 100 2017 2027 40000 Alice ## 103 2018 2028 20000 David ## year1 year2 salary name ## empID ## 100 2017 2027 40000 Alice ## 101 2017 2027 24000 Bob ## 102 2017 2027 31000 Charles ## year1 year2 salary name ## empID ## 100 2017 2027 40000 Alice ## 101 2017 2027 24000 Bob ## 102 2017 2027 31000 Charles ## 103 2018 2028 20000 David ## 104 2018 2028 30000 Eric ## Error in py_call_impl(callable, dots$args, dots$keywords): IndexError: single positional indexer is out-of-bounds 15.7.5.4 Using Boolean List criteria = (df.salary &gt; 30000) &amp; (df.year1==2017) print (criteria) print (df.loc[criteria]) ## empID ## 100 True ## 101 False ## 102 True ## 103 False ## 104 False ## dtype: bool ## year1 year2 salary name ## empID ## 100 2017 2027 40000 Alice ## 102 2017 2027 31000 Charles 15.7.5.5 Using Expression (.query) .query(expr, inplace=False) num1 = 31000 num2 = 2017 df.query(f&#39;salary&lt;={num1} and year1=={num2}&#39;) ## year1 year2 salary name ## empID ## 101 2017 2027 24000 Bob ## 102 2017 2027 31000 Charles 15.7.5.6 Sampling (.sample) Specify percentage of random rows to be returned. np.random.seed(15) ## ensure consistentcy of result df.sample(frac=0.7) ## randomly pick 70% of rows, without replacement ## year1 year2 salary name ## empID ## 102 2017 2027 31000 Charles ## 103 2018 2028 20000 David ## 104 2018 2028 30000 Eric ## 101 2017 2027 24000 Bob 15.7.6 Row Manipulation 15.7.6.1 Sample Data df ## year1 year2 salary name ## empID ## 100 2017 2027 40000 Alice ## 101 2017 2027 24000 Bob ## 102 2017 2027 31000 Charles ## 103 2018 2028 20000 David ## 104 2018 2028 30000 Eric 15.7.6.2 Append Rows .append() has been deprecated. Refer to Concatenating section. 15.7.6.3 Drop Rows (.drop) .drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise') By Row Label(s) df.drop(index=100) # drop single row df.drop(index=[100,103], columns=&#39;salary&#39;) # drop selected rows and columns ## year1 year2 salary name ## empID ## 101 2017 2027 24000 Bob ## 102 2017 2027 31000 Charles ## 103 2018 2028 20000 David ## 104 2018 2028 30000 Eric ## year1 year2 name ## empID ## 101 2017 2027 Bob ## 102 2017 2027 Charles ## 104 2018 2028 Eric 15.7.7 Column Manipulation 15.7.7.1 Sample Data df = pd.DataFrame(data) df ## empID year1 year2 salary name ## 0 100 2017 2027 40000 Alice ## 1 101 2017 2027 24000 Bob ## 2 102 2017 2027 31000 Charles ## 3 103 2018 2028 20000 David ## 4 104 2018 2028 30000 Eric 15.7.7.2 Renaming Columns Method 1 : Rename All Columns (.columns =). Construct the new column names, ensure there is no missing column names, which will result in error. new_columns = [&#39;empID&#39;, &#39;year.1&#39;,&#39;year.2&#39;,&#39;salary&#39;, &#39;glamour&#39;] df.columns = new_columns df.head(2) ## empID year.1 year.2 salary glamour ## 0 100 2017 2027 40000 Alice ## 1 101 2017 2027 24000 Bob Method 2 : Renaming Specific Column (.rename (columns=) ) Change column name through rename function. Support inplace option. Specify only columns that require name change. Non matching columns will not return error. That is great. df.rename( columns={&#39;year.1&#39;:&#39;year1&#39;, &#39;year.2&#39;:&#39;year2&#39;}, inplace=True) df.head(2) ## empID year1 year2 salary glamour ## 0 100 2017 2027 40000 Alice ## 1 101 2017 2027 24000 Bob 15.7.7.3 Reordering Columns Reordering columns is basically returning a new DataFrame with the speicified column list in order preferred. Refer to Sub-setting Column section. 15.7.7.4 Create New Column New Column will be created instantly using [] notation DO NOT USE dot Notation because it is view only attribute df[&#39;year3&#39;] = df.year1 df ## empID year1 year2 salary glamour year3 ## 0 100 2017 2027 40000 Alice 2017 ## 1 101 2017 2027 24000 Bob 2017 ## 2 102 2017 2027 31000 Charles 2017 ## 3 103 2018 2028 20000 David 2018 ## 4 104 2018 2028 30000 Eric 2018 15.7.7.5 Dropping Columns (.drop) dataframe.drop( columns=&#39;column_name&#39;, inplace=True/False) # delete single column dataframe.drop( columns=list_of_colnames, inplace=True/False) # delete multiple column dataframe.drop( index=&#39;row_label&#39;, inplace=True/False) # delete single row dataframe.drop( index= list_of_row_labels, inplace=True/False) # delete multiple rows inplace=True means column will be deleted from original dataframe. Default is False, which return a copy of dataframe By Column Name(s) df.drop( columns=&#39;year1&#39;) # drop single column df.drop( columns=[&#39;year1&#39;,&#39;year2&#39;]) # drop multiple columns ## empID year2 salary glamour year3 ## 0 100 2027 40000 Alice 2017 ## 1 101 2027 24000 Bob 2017 ## 2 102 2027 31000 Charles 2017 ## 3 103 2028 20000 David 2018 ## 4 104 2028 30000 Eric 2018 ## empID salary glamour year3 ## 0 100 40000 Alice 2017 ## 1 101 24000 Bob 2017 ## 2 102 31000 Charles 2017 ## 3 103 20000 David 2018 ## 4 104 30000 Eric 2018 By Column Number(s) Use dataframe.columns to produce interim list of column names ## before dropping columns df df.drop( columns=df.columns[[1,2]]) # drop second and third columns df.drop( columns=df.columns[0:3] ) # drop first, second and third columns ## empID year1 year2 salary glamour year3 ## 0 100 2017 2027 40000 Alice 2017 ## 1 101 2017 2027 24000 Bob 2017 ## 2 102 2017 2027 31000 Charles 2017 ## 3 103 2018 2028 20000 David 2018 ## 4 104 2018 2028 30000 Eric 2018 ## empID salary glamour year3 ## 0 100 40000 Alice 2017 ## 1 101 24000 Bob 2017 ## 2 102 31000 Charles 2017 ## 3 103 20000 David 2018 ## 4 104 30000 Eric 2018 ## salary glamour year3 ## 0 40000 Alice 2017 ## 1 24000 Bob 2017 ## 2 31000 Charles 2017 ## 3 20000 David 2018 ## 4 30000 Eric 2018 15.7.8 Subsetting Columns Select Single Column Return Series dataframe.columnName # single column, name based, return Series object dataframe[ single_col_name ] # single column, name based, return Series object Select Single/Multiple Columns Return DataFrame dataframe [ list_of_col_names ] # name based, return Dataframe object dataframe.loc[ : , single_col_name ] # single column, series dataframe.loc[ : , col_name_list ] # multiple columns, dataframe dataframe.loc[ : , col_name_range ] # multiple columns, dataframe dataframe.iloc[ : , col_number ] # single column, series dataframe.iloc[ : , col_number_list ] # multiple columns, dataframe dataframe.iloc[ : , number_range ] # multiple columns, dataframe 15.7.8.1 Select Single Column (.loc, iloc) Selecting single column always return as panda::Series, except using [[ ]] notation. df = pd.DataFrame(data) df.name df[&#39;name&#39;] df.loc[:, &#39;name&#39;] df.iloc[:, 3] ## 0 Alice ## 1 Bob ## 2 Charles ## 3 David ## 4 Eric ## Name: name, dtype: object ## 0 Alice ## 1 Bob ## 2 Charles ## 3 David ## 4 Eric ## Name: name, dtype: object ## 0 Alice ## 1 Bob ## 2 Charles ## 3 David ## 4 Eric ## Name: name, dtype: object ## 0 40000 ## 1 24000 ## 2 31000 ## 3 20000 ## 4 30000 ## Name: salary, dtype: int64 15.7.8.2 Select Multiple Columns (.loc, iloc) Multiple columns return as panda::Dataframe object` Example below returns DataFrame with Single Column df[[&#39;name&#39;]] # return one column dataframe df[[&#39;name&#39;,&#39;year1&#39;]] df.loc[:,[&#39;name&#39;,&#39;year1&#39;]] ## name ## 0 Alice ## 1 Bob ## 2 Charles ## 3 David ## 4 Eric ## name year1 ## 0 Alice 2017 ## 1 Bob 2017 ## 2 Charles 2017 ## 3 David 2018 ## 4 Eric 2018 ## name year1 ## 0 Alice 2017 ## 1 Bob 2017 ## 2 Charles 2017 ## 3 David 2018 ## 4 Eric 2018 Select Range of Columns df.loc [ : , &#39;year1&#39;:&#39;year2&#39;] ## by range of column names df.loc[ : , [&#39;empID&#39;,&#39;year2&#39;]] ## select two columns only df.iloc[ : , 1:4] ## by range of column number df.iloc[ : , [0,3]] ## select two columns only ## year1 year2 ## 0 2017 2027 ## 1 2017 2027 ## 2 2017 2027 ## 3 2018 2028 ## 4 2018 2028 ## empID year2 ## 0 100 2027 ## 1 101 2027 ## 2 102 2027 ## 3 103 2028 ## 4 104 2028 ## year1 year2 salary ## 0 2017 2027 40000 ## 1 2017 2027 24000 ## 2 2017 2027 31000 ## 3 2018 2028 20000 ## 4 2018 2028 30000 ## empID salary ## 0 100 40000 ## 1 101 24000 ## 2 102 31000 ## 3 103 20000 ## 4 104 30000 15.7.8.3 By Column Name (.filter, .reindex) .filter(items=None, like=None, regex=None, axis=1) Filter by like - Sub string Matching, always return DataFrame. df.filter( like=&#39;year&#39;, axis=1) ## or axis = 1 ## year1 year2 ## 0 2017 2027 ## 1 2017 2027 ## 2 2017 2027 ## 3 2018 2028 ## 4 2018 2028 df.filter( like=&#39;name&#39;) ## name ## 0 Alice ## 1 Bob ## 2 Charles ## 3 David ## 4 Eric Filter by items - list of exact column names df.filter( items=(&#39;year1&#39;,&#39;year2&#39;, &#39;name&#39;, &#39;not_exist&#39;), axis=1) ## year1 year2 name ## 0 2017 2027 Alice ## 1 2017 2027 Bob ## 2 2017 2027 Charles ## 3 2018 2028 David ## 4 2018 2028 Eric Filter by regex = Regular Expression Matching column names Select column names that contain integer. df.filter(regex=&#39;\\d&#39;) ## default axis=1 if DataFrame ## year1 year2 ## 0 2017 2027 ## 1 2017 2027 ## 2 2017 2027 ## 3 2018 2028 ## 4 2018 2028 .reindex(columns = .. ) It returns a new dataframe. There is no inplace option for reordering columns Non Matching columns names will result in NA values new_colorder = [ &#39;salary&#39;, &#39;year1&#39;, &#39;baba&#39;] df.reindex(columns = new_colorder) ## Non matching column name returns as NaN ## salary year1 baba ## 0 40000 2017 NaN ## 1 24000 2017 NaN ## 2 31000 2017 NaN ## 3 20000 2018 NaN ## 4 30000 2018 NaN 15.7.8.4 By Data Types (.select_dtypes) Always return panda::DataFrame, even though only single column matches. Allowed types are: number (integer and float) integer / int64 float datetime timedelta category bool object (string) Use .dtypes.value_counts() to insepct available data types. df.dtypes.value_counts() ## inspect available data types df.select_dtypes( exclude=&#39;integer&#39;) ## exclude bool cols df.select_dtypes( include=[&#39;number&#39;,&#39;object&#39;]) ## include only number cols ## int64 4 ## object 1 ## dtype: int64 ## name ## 0 Alice ## 1 Bob ## 2 Charles ## 3 David ## 4 Eric ## empID year1 year2 salary name ## 0 100 2017 2027 40000 Alice ## 1 101 2017 2027 24000 Bob ## 2 102 2017 2027 31000 Charles ## 3 103 2018 2028 20000 David ## 4 104 2018 2028 30000 Eric 15.7.9 Concatenating Concatenating Two DataFrames When ignore_index=True, pandas will create new index in the result with 0,1,2,3… It is recommended to ignore index IF the data source index is not unique. New columns or rows will be added in the result if non-matching. my_df = pd.DataFrame( data= {&#39;Id&#39;: [10,20,30], &#39;Name&#39;: [&#39;Aaa&#39;,&#39;Bbb&#39;,&#39;Ccc&#39;]}) # .set_index(&#39;Id&#39;) my_df_new = pd.DataFrame( data= {&#39;Id&#39;: [40,50], &#39;Name&#39;: [&#39;Ddd&#39;,&#39;Eee&#39;], &#39;Age&#39;: [12,13]}) #.set_index(&#39;Id&#39;) my_df_append = pd.concat( [my_df, my_df_new]) my_df_noindex = pd.concat( [my_df, my_df_new], ignore_index=True) print(&quot;Original DataFrame:\\n&quot;, my_df, &quot;\\n\\nTo Be Appended DataFrame:\\n&quot;, my_df_new, &quot;\\n\\nAppended DataFrame (index maintained):\\n&quot;, my_df_append, &quot;\\n\\nAppended DataFrame (index ignored):\\n&quot;, my_df_noindex) ## Original DataFrame: ## Id Name ## 0 10 Aaa ## 1 20 Bbb ## 2 30 Ccc ## ## To Be Appended DataFrame: ## Id Name Age ## 0 40 Ddd 12 ## 1 50 Eee 13 ## ## Appended DataFrame (index maintained): ## Id Name Age ## 0 10 Aaa NaN ## 1 20 Bbb NaN ## 2 30 Ccc NaN ## 0 40 Ddd 12.0 ## 1 50 Eee 13.0 ## ## Appended DataFrame (index ignored): ## Id Name Age ## 0 10 Aaa NaN ## 1 20 Bbb NaN ## 2 30 Ccc NaN ## 3 40 Ddd 12.0 ## 4 50 Eee 13.0 15.7.10 Slicing 15.7.10.1 Sample Data df = pd.DataFrame(data).set_index(&#39;empID&#39;) df ## year1 year2 salary name ## empID ## 100 2017 2027 40000 Alice ## 101 2017 2027 24000 Bob ## 102 2017 2027 31000 Charles ## 103 2018 2028 20000 David ## 104 2018 2028 30000 Eric 15.7.10.2 Getting One Cell By Row Label and Column Name (loc) dataframe.loc [ row_label , col_name ] # by row label and column names dataframe.loc [ bool_list , col_name ] # by row label and column names dataframe.iloc[ row_number, col_number ] # by row and column number df.loc [102,&#39;year1&#39;] ## row label 102, column &#39;year1&#39; df.iloc[2,0] ## same result as above ## 2017 ## 2017 15.7.10.3 Getting Range of Cells Specify rows and columns (by individual or range) dataframe.loc [ list/range_of_row_labels , list/range_col_names ] # by row label and column names dataframe.iloc[ list/range_row_numbers, list/range_col_numbers ] # by row number By Index and Column Name (loc) df.loc[ [101,103], [&#39;name&#39;, &#39;year1&#39;] ] # by list of row label and column names df.loc[ 101:104 , &#39;year1&#39;:&#39;year2&#39; ] # by range of row label and column names ## name year1 ## empID ## 101 Bob 2017 ## 103 David 2018 ## year1 year2 ## empID ## 101 2017 2027 ## 102 2017 2027 ## 103 2018 2028 ## 104 2018 2028 By Boolean Row and Column Names (loc) b = df.year1==2017 df.loc[ b ] ## year1 year2 salary name ## empID ## 100 2017 2027 40000 Alice ## 101 2017 2027 24000 Bob ## 102 2017 2027 31000 Charles By Row and Column Number (iloc) print (df.iloc[ [1,4], [0,3]],&#39;\\n&#39; ) # by individual rows/columns ## year1 name ## empID ## 101 2017 Bob ## 104 2018 Eric print (df.iloc[ 1:4 , 0:3], &#39;\\n&#39;) # by range ## year1 year2 salary ## empID ## 101 2017 2027 24000 ## 102 2017 2027 31000 ## 103 2018 2028 20000 15.7.11 Chained Indexing Chained Index Method creates a copy of dataframe, any modification of data on original dataframe does not affect the copy dataframe.loc [...] [...] dataframe.iloc [...] [...] Suggesting, never use chain indexing df = pd.DataFrame( { &#39;empID&#39;: [100, 101, 102, 103, 104], &#39;year1&#39;: [2017, 2017, 2017, 2018, 2018], &#39;name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charles&#39;,&#39;David&#39;, &#39;Eric&#39;], &#39;year2&#39;: [2001, 1907, 2003, 1998, 2011], &#39;salary&#39;: [40000, 24000, 31000, 20000, 30000]}, columns = [&#39;year1&#39;,&#39;salary&#39;,&#39;year2&#39;,&#39;empID&#39;,&#39;name&#39;]).set_index([&#39;empID&#39;]) df ## year1 salary year2 name ## empID ## 100 2017 40000 2001 Alice ## 101 2017 24000 1907 Bob ## 102 2017 31000 2003 Charles ## 103 2018 20000 1998 David ## 104 2018 30000 2011 Eric df.loc[100][&#39;year&#39;] =2000 ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy ## &lt;string&gt;:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df ## notice row label 100 had not been updated, because data was updated on a copy due to chain indexing ## year1 salary year2 name ## empID ## 100 2017 40000 2001 Alice ## 101 2017 24000 1907 Bob ## 102 2017 31000 2003 Charles ## 103 2018 20000 1998 David ## 104 2018 30000 2011 Eric 15.7.12 Find and Replace Slicing deals with square cells selection. Use mask or where to select specific cell(s). These function respect column and row names. 15.7.12.1 mask() mask() replace cell value with other= when condition is met. data={&#39;x&#39;: [1,4,7], &#39;y&#39;: [2,5,8], &#39;z&#39;: [3,6,9]} df = pd.DataFrame(data) df.mask(df&gt;4, other=999) ## x y z ## 0 1 2 3 ## 1 4 999 999 ## 2 999 999 999 15.7.12.2 where() This is reverse of mask(), it keep cell value where condition is met. If not met, replace with other value. df.where(df&gt;4, other=0) ## x y z ## 0 0 0 0 ## 1 0 5 6 ## 2 7 8 9 15.7.13 Iteration 15.7.13.1 Iterating Rows (.iterrows) df = pd.DataFrame(data= { &#39;empID&#39;: [100, 101, 102, 103, 104], &#39;Name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charles&#39;,&#39;David&#39;, &#39;Eric&#39;], &#39;Year&#39;: [1999, 1988, 2001, 2010, 2020]}).set_index([&#39;empID&#39;]) for idx, row in df.iterrows(): print(idx, row.Name) ## 100 Alice ## 101 Bob ## 102 Charles ## 103 David ## 104 Eric 15.7.13.2 Iterating Columns (.items) for label, content in df.items(): print(&#39;Column Label:&#39;, label, &#39;\\n\\n&#39;, &#39;Column Content (Series):\\n&#39;, content, &#39;\\n\\n&#39;) ## Column Label: Name ## ## Column Content (Series): ## empID ## 100 Alice ## 101 Bob ## 102 Charles ## 103 David ## 104 Eric ## Name: Name, dtype: object ## ## ## Column Label: Year ## ## Column Content (Series): ## empID ## 100 1999 ## 101 1988 ## 102 2001 ## 103 2010 ## 104 2020 ## Name: Year, dtype: int64 15.7.14 Data Structure 15.7.14.1 Inspect Structure Find out the column names, data type in a summary. Output is for display only, not a data object df.info() # return text output ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## Int64Index: 5 entries, 100 to 104 ## Data columns (total 2 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Name 5 non-null object ## 1 Year 5 non-null int64 ## dtypes: int64(1), object(1) ## memory usage: 120.0+ bytes df.dtypes.value_counts() # return Series ## object 1 ## int64 1 ## dtype: int64 15.7.14.2 Format Conversion .to_dict() df.to_dict(&#39;dict&#39;) ## dict of dict by column ## {&#39;Name&#39;: {100: &#39;Alice&#39;, 101: &#39;Bob&#39;, 102: &#39;Charles&#39;, 103: &#39;David&#39;, 104: &#39;Eric&#39;}, &#39;Year&#39;: {100: 1999, 101: 1988, 102: 2001, 103: 2010, 104: 2020}} df.to_dict(&#39;index&#39;) ## dict of dict by row index ## {100: {&#39;Name&#39;: &#39;Alice&#39;, &#39;Year&#39;: 1999}, 101: {&#39;Name&#39;: &#39;Bob&#39;, &#39;Year&#39;: 1988}, 102: {&#39;Name&#39;: &#39;Charles&#39;, &#39;Year&#39;: 2001}, 103: {&#39;Name&#39;: &#39;David&#39;, &#39;Year&#39;: 2010}, 104: {&#39;Name&#39;: &#39;Eric&#39;, &#39;Year&#39;: 2020}} df.to_dict(&#39;list&#39;) ## dict of list ## {&#39;Name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charles&#39;, &#39;David&#39;, &#39;Eric&#39;], &#39;Year&#39;: [1999, 1988, 2001, 2010, 2020]} df.to_dict(&#39;records&#39;) ## list of dict ## [{&#39;Name&#39;: &#39;Alice&#39;, &#39;Year&#39;: 1999}, {&#39;Name&#39;: &#39;Bob&#39;, &#39;Year&#39;: 1988}, {&#39;Name&#39;: &#39;Charles&#39;, &#39;Year&#39;: 2001}, {&#39;Name&#39;: &#39;David&#39;, &#39;Year&#39;: 2010}, {&#39;Name&#39;: &#39;Eric&#39;, &#39;Year&#39;: 2020}] 15.7.15 Import/Export 15.7.15.1 CSV df.to_csv() ## &#39;empID,Name,Year\\r\\n100,Alice,1999\\r\\n101,Bob,1988\\r\\n102,Charles,2001\\r\\n103,David,2010\\r\\n104,Eric,2020\\r\\n&#39; 15.8 Class: MultiIndex MultiIndexing are columns with few levels of headers. 15.8.1 The Data df = pd.DataFrame({ &#39;myindex&#39;: [0, 1, 2], &#39;One_X&#39;: [1.1, 1.1, 1.1], &#39;One_Y&#39;: [1.2, 1.2, 1.2], &#39;Two_X&#39;: [1.11, 1.11, 1.11], &#39;Two_Y&#39;: [1.22, 1.22, 1.22]}) df.set_index(&#39;myindex&#39;,inplace=True) df ## One_X One_Y Two_X Two_Y ## myindex ## 0 1.1 1.2 1.11 1.22 ## 1 1.1 1.2 1.11 1.22 ## 2 1.1 1.2 1.11 1.22 15.8.2 Creating MultiIndex Object 15.8.2.1 Create From Tuples MultiIndex can easily created from typles: - Step 1: Create a MultiIndex object by splitting column name into tuples - Step 2: Assign the MultiIndex Object to dataframe columns property. my_tuples = [tuple(c.split(&#39;_&#39;)) for c in df.columns] df.columns = pd.MultiIndex.from_tuples(my_tuples) print(&#39; Column Headers :\\n\\n&#39;, my_tuples, &#39;\\n\\nNew Columns: \\n\\n&#39;, df.columns, &#39;\\n\\nTwo Layers Header DF:\\n\\n&#39;, df) ## Column Headers : ## ## [(&#39;One&#39;, &#39;X&#39;), (&#39;One&#39;, &#39;Y&#39;), (&#39;Two&#39;, &#39;X&#39;), (&#39;Two&#39;, &#39;Y&#39;)] ## ## New Columns: ## ## MultiIndex([(&#39;One&#39;, &#39;X&#39;), ## (&#39;One&#39;, &#39;Y&#39;), ## (&#39;Two&#39;, &#39;X&#39;), ## (&#39;Two&#39;, &#39;Y&#39;)], ## ) ## ## Two Layers Header DF: ## ## One Two ## X Y X Y ## myindex ## 0 1.1 1.2 1.11 1.22 ## 1 1.1 1.2 1.11 1.22 ## 2 1.1 1.2 1.11 1.22 15.8.3 MultiIndex Object 15.8.3.1 Levels MultiIndex object contain multiple leveels, each level (header) is an Index object. Use MultiIndex.get_level_values() to the entire header for the desired level. Note that each level is an Index object print(df.columns.get_level_values(0), &#39;\\n&#39;, df.columns.get_level_values(1)) ## Index([&#39;One&#39;, &#39;One&#39;, &#39;Two&#39;, &#39;Two&#39;], dtype=&#39;object&#39;) ## Index([&#39;X&#39;, &#39;Y&#39;, &#39;X&#39;, &#39;Y&#39;], dtype=&#39;object&#39;) MultiIndex.levels return the unique values of each level. print(df.columns.levels[0], &#39;\\n&#39;, df.columns.levels[1]) ## Index([&#39;One&#39;, &#39;Two&#39;], dtype=&#39;object&#39;) ## Index([&#39;X&#39;, &#39;Y&#39;], dtype=&#39;object&#39;) 15.8.3.2 Convert MultiIndex Back To Tuples df.columns.to_list() ## [(&#39;One&#39;, &#39;X&#39;), (&#39;One&#39;, &#39;Y&#39;), (&#39;Two&#39;, &#39;X&#39;), (&#39;Two&#39;, &#39;Y&#39;)] 15.8.4 Selecting Column(s) 15.8.4.1 Sample Data import itertools test_df = pd.DataFrame max_age = 100 ### Create The Columns Tuple level0_sex = [&#39;Male&#39;,&#39;Female&#39;,&#39;Pondan&#39;] level1_age = [&#39;Medium&#39;,&#39;High&#39;,&#39;Low&#39;] my_columns = list(itertools.product(level0_sex, level1_age)) test_df = pd.DataFrame([ [1,2,3,4,5,6,7,8,9], [11,12,13,14,15,16,17,18,19], [21,22,23,24,25,26,27,28,29]], index=[&#39;row1&#39;,&#39;row2&#39;,&#39;row3&#39;]) ### Create Multiindex From Tuple test_df.columns = pd.MultiIndex.from_tuples(my_columns) print( test_df ) ## Male Female Pondan ## Medium High Low Medium High Low Medium High Low ## row1 1 2 3 4 5 6 7 8 9 ## row2 11 12 13 14 15 16 17 18 19 ## row3 21 22 23 24 25 26 27 28 29 15.8.4.2 Select Level0 Header(s) Use [L0] notation, where L0 is list of header names print( test_df[[&#39;Male&#39;,&#39;Pondan&#39;]] ,&#39;\\n\\n&#39;, ## Include multiple Level0 Header test_df[&#39;Male&#39;] , &#39;\\n\\n&#39;, ## Include single Level0 Header test_df.Male ) ## Same as above ## Male Pondan ## Medium High Low Medium High Low ## row1 1 2 3 7 8 9 ## row2 11 12 13 17 18 19 ## row3 21 22 23 27 28 29 ## ## Medium High Low ## row1 1 2 3 ## row2 11 12 13 ## row3 21 22 23 ## ## Medium High Low ## row1 1 2 3 ## row2 11 12 13 ## row3 21 22 23 Using .loc[] Use .loc[ :, L0 ], where L0 is list of headers names print( test_df.loc[:, [&#39;Male&#39;,&#39;Pondan&#39;]] , &#39;\\n\\n&#39;, ## Multiple Level0 Header test_df.loc[:, &#39;Male&#39;] ) ## Single Level0 Header ## Male Pondan ## Medium High Low Medium High Low ## row1 1 2 3 7 8 9 ## row2 11 12 13 17 18 19 ## row3 21 22 23 27 28 29 ## ## Medium High Low ## row1 1 2 3 ## row2 11 12 13 ## row3 21 22 23 15.8.4.3 Selecting Level 1 Header(s) Use .loc[ :, (All, L1)], where L1 are list of headers names All = slice(None) print( test_df.loc[ : , (All, &#39;High&#39;)], &#39;\\n\\n&#39;, ## Signle L1 header test_df.loc[ : , (All, [&#39;High&#39;,&#39;Low&#39;])] ) ## Multiple L1 headers ## Male Female Pondan ## High High High ## row1 2 5 8 ## row2 12 15 18 ## row3 22 25 28 ## ## Male Female Pondan Male Female Pondan ## High High High Low Low Low ## row1 2 5 8 3 6 9 ## row2 12 15 18 13 16 19 ## row3 22 25 28 23 26 29 15.8.4.4 Select Level 0 and Level1 Headers Use .loc[ :, (L0, L1)], where L0 and L1 are list of headers names test_df.loc[ : , ([&#39;Male&#39;,&#39;Pondan&#39;], [&#39;Medium&#39;,&#39;High&#39;])] ## Male Pondan ## Medium High Medium High ## row1 1 2 7 8 ## row2 11 12 17 18 ## row3 21 22 27 28 15.8.4.5 Select single L0,L1 Header Use .loc[:, (L0, L1) ], result is a Series Use .loc[:, (L0 ,[L1])], result is a DataFrame print( test_df.loc[ : , (&#39;Female&#39;, &#39;High&#39;)], &#39;\\n\\n&#39;, test_df.loc[ : , (&#39;Female&#39;, [&#39;High&#39;])]) ## row1 5 ## row2 15 ## row3 25 ## Name: (Female, High), dtype: int64 ## ## Female ## High ## row1 5 ## row2 15 ## row3 25 15.8.5 Headers Ordering Note that columns order specifeid by [ ] selection were not respected. This can be remediated either by Sorting and rearranging. 15.8.5.1 Sort Headers Use .sort_index() on DataFrame to sort the headers. Note that when level1 is sorted, it jumble up level0 headers. test_df_sorted_l0 = test_df.sort_index(axis=1, level=0) test_df_sorted_l1 = test_df.sort_index(axis=1, level=1, ascending=False) print(test_df, &#39;\\n\\n&#39;,test_df_sorted_l0, &#39;\\n\\n&#39;, test_df_sorted_l1) ## Male Female Pondan ## Medium High Low Medium High Low Medium High Low ## row1 1 2 3 4 5 6 7 8 9 ## row2 11 12 13 14 15 16 17 18 19 ## row3 21 22 23 24 25 26 27 28 29 ## ## Female Male Pondan ## High Low Medium High Low Medium High Low Medium ## row1 5 6 4 2 3 1 8 9 7 ## row2 15 16 14 12 13 11 18 19 17 ## row3 25 26 24 22 23 21 28 29 27 ## ## Pondan Male Female Pondan Male Female Pondan Male Female ## Medium Medium Medium Low Low Low High High High ## row1 7 1 4 9 3 6 8 2 5 ## row2 17 11 14 19 13 16 18 12 15 ## row3 27 21 24 29 23 26 28 22 25 15.8.5.2 Rearranging Headers Use **.reindex()** on arrange columns in specific order. Example below shows how to control the specific order for level1 headers. cats = [&#39;Low&#39;,&#39;Medium&#39;,&#39;High&#39;] test_df.reindex(cats, level=1, axis=1) ## Male Female Pondan ## Low Medium High Low Medium High Low Medium High ## row1 3 1 2 6 4 5 9 7 8 ## row2 13 11 12 16 14 15 19 17 18 ## row3 23 21 22 26 24 25 29 27 28 15.8.6 Stacking and Unstacking df.stack() ## One Two ## myindex ## 0 X 1.1 1.11 ## Y 1.2 1.22 ## 1 X 1.1 1.11 ## Y 1.2 1.22 ## 2 X 1.1 1.11 ## Y 1.2 1.22 15.8.6.1 Stacking Columns to Rows Stacking with DataFrame.stack(level_no) is moving wide columns into row. print(&#39;Stacking Header Level 0: \\n\\n&#39;, df.stack(0), &#39;\\n\\nStacking Header Level 1: \\n\\n&#39;, df.stack(1)) ## Stacking Header Level 0: ## ## X Y ## myindex ## 0 One 1.10 1.20 ## Two 1.11 1.22 ## 1 One 1.10 1.20 ## Two 1.11 1.22 ## 2 One 1.10 1.20 ## Two 1.11 1.22 ## ## Stacking Header Level 1: ## ## One Two ## myindex ## 0 X 1.1 1.11 ## Y 1.2 1.22 ## 1 X 1.1 1.11 ## Y 1.2 1.22 ## 2 X 1.1 1.11 ## Y 1.2 1.22 15.8.7 Exploratory Analysis 15.8.7.1 Sample Data df ## One Two ## X Y X Y ## myindex ## 0 1.1 1.2 1.11 1.22 ## 1 1.1 1.2 1.11 1.22 ## 2 1.1 1.2 1.11 1.22 15.8.7.2 All Stats in One - .describe() df.describe(include=&#39;number&#39;) # default df.describe(include=&#39;object&#39;) # display for non-numeric columns df.describe(include=&#39;all&#39;) # display both numeric and non-numeric When applied to DataFrame object, describe shows all basic statistic for all numeric columns: - Count (non-NA) - Unique (for string) - Top (for string) - Frequency (for string) - Percentile - Mean - Min / Max - Standard Deviation For Numeric Columns only You can customize the percentiles requred. Notice 0.5 percentile is always there although not specified df.describe() ## One Two ## X Y X Y ## count 3.0 3.0 3.00 3.00 ## mean 1.1 1.2 1.11 1.22 ## std 0.0 0.0 0.00 0.00 ## min 1.1 1.2 1.11 1.22 ## 25% 1.1 1.2 1.11 1.22 ## 50% 1.1 1.2 1.11 1.22 ## 75% 1.1 1.2 1.11 1.22 ## max 1.1 1.2 1.11 1.22 df.describe(percentiles=[0.9,0.3,0.2,0.1]) ## One Two ## X Y X Y ## count 3.0 3.0 3.00 3.00 ## mean 1.1 1.2 1.11 1.22 ## std 0.0 0.0 0.00 0.00 ## min 1.1 1.2 1.11 1.22 ## 10% 1.1 1.2 1.11 1.22 ## 20% 1.1 1.2 1.11 1.22 ## 30% 1.1 1.2 1.11 1.22 ## 50% 1.1 1.2 1.11 1.22 ## 90% 1.1 1.2 1.11 1.22 ## max 1.1 1.2 1.11 1.22 For both Numeric and Object df.describe(include=&#39;all&#39;) ## One Two ## X Y X Y ## count 3.0 3.0 3.00 3.00 ## mean 1.1 1.2 1.11 1.22 ## std 0.0 0.0 0.00 0.00 ## min 1.1 1.2 1.11 1.22 ## 25% 1.1 1.2 1.11 1.22 ## 50% 1.1 1.2 1.11 1.22 ## 75% 1.1 1.2 1.11 1.22 ## max 1.1 1.2 1.11 1.22 15.8.7.3 min/max/mean/median df.min() # default axis=0, column-wise ## One X 1.10 ## Y 1.20 ## Two X 1.11 ## Y 1.22 ## dtype: float64 df.min(axis=1) # axis=1, row-wise ## myindex ## 0 1.1 ## 1 1.1 ## 2 1.1 ## dtype: float64 Observe, sum on string will concatenate column-wise, whereas row-wise only sum up numeric fields df.sum(0) ## One X 3.30 ## Y 3.60 ## Two X 3.33 ## Y 3.66 ## dtype: float64 df.sum(1) ## myindex ## 0 4.63 ## 1 4.63 ## 2 4.63 ## dtype: float64 15.8.8 Plotting 15.9 Class: Categories 15.9.1 Creating 15.9.1.1 From List Basic (Auto Category Mapping) Basic syntax return categorical index with sequence with code 0,1,2,3… mapping to first found category In this case, low(0), high(1), medium(2) temp = [&#39;low&#39;,&#39;high&#39;,&#39;medium&#39;,&#39;high&#39;,&#39;high&#39;,&#39;low&#39;,&#39;medium&#39;,&#39;medium&#39;,&#39;high&#39;] temp_cat = pd.Categorical(temp) temp_cat ## [&#39;low&#39;, &#39;high&#39;, &#39;medium&#39;, &#39;high&#39;, &#39;high&#39;, &#39;low&#39;, &#39;medium&#39;, &#39;medium&#39;, &#39;high&#39;] ## Categories (3, object): [&#39;high&#39;, &#39;low&#39;, &#39;medium&#39;] type( temp_cat ) ## &lt;class &#39;pandas.core.arrays.categorical.Categorical&#39;&gt; Manual Category Mapping During creation, we can specify mapping of codes to category: low(0), medium(1), high(2) temp_cat = pd.Categorical(temp, categories=[&#39;low&#39;,&#39;medium&#39;,&#39;high&#39;]) temp_cat ## [&#39;low&#39;, &#39;high&#39;, &#39;medium&#39;, &#39;high&#39;, &#39;high&#39;, &#39;low&#39;, &#39;medium&#39;, &#39;medium&#39;, &#39;high&#39;] ## Categories (3, object): [&#39;low&#39;, &#39;medium&#39;, &#39;high&#39;] 15.9.1.2 From Series We can ‘add’ categorical structure into a Series. With these methods, additional property (.cat) is added as a categorical accessor Through this accessor, you gain access to various properties of the category such as .codes, .categories. But not .get_values() as the information is in the Series itself Can we manual map category ????? temp = [&#39;low&#39;,&#39;high&#39;,&#39;medium&#39;,&#39;high&#39;,&#39;high&#39;,&#39;low&#39;,&#39;medium&#39;,&#39;medium&#39;,&#39;high&#39;] temp_cat = pd.Series(temp, dtype=&#39;category&#39;) print (type(temp_cat)) # Series object ## &lt;class &#39;pandas.core.series.Series&#39;&gt; print (type(temp_cat.cat)) # Categorical Accessor ## &lt;class &#39;pandas.core.arrays.categorical.CategoricalAccessor&#39;&gt; Method below has the same result as above by using .astype(‘category’) It is useful adding category structure into existing series. temp_ser = pd.Series(temp) temp_cat = pd.Series(temp).astype(&#39;category&#39;) print (type(temp_cat)) # Series object ## &lt;class &#39;pandas.core.series.Series&#39;&gt; print (type(temp_cat.cat)) # Categorical Accessor ## &lt;class &#39;pandas.core.arrays.categorical.CategoricalAccessor&#39;&gt; temp_cat.cat.categories ## Index([&#39;high&#39;, &#39;low&#39;, &#39;medium&#39;], dtype=&#39;object&#39;) 15.9.1.3 Ordering Category temp = [&#39;low&#39;,&#39;high&#39;,&#39;medium&#39;,&#39;high&#39;,&#39;high&#39;,&#39;low&#39;,&#39;medium&#39;,&#39;medium&#39;,&#39;high&#39;] temp_cat = pd.Categorical(temp, categories=[&#39;low&#39;,&#39;medium&#39;,&#39;high&#39;], ordered=True) temp_cat ## [&#39;low&#39;, &#39;high&#39;, &#39;medium&#39;, &#39;high&#39;, &#39;high&#39;, &#39;low&#39;, &#39;medium&#39;, &#39;medium&#39;, &#39;high&#39;] ## Categories (3, object): [&#39;low&#39; &lt; &#39;medium&#39; &lt; &#39;high&#39;] temp_cat.codes ## array([0, 2, 1, 2, 2, 0, 1, 1, 2], dtype=int8) temp_cat[0] &lt; temp_cat[3] ## False 15.9.2 Properties 15.9.2.1 .categories first element’s code = 0 second element’s code = 1 third element’s code = 2 temp_cat.categories ## Index([&#39;low&#39;, &#39;medium&#39;, &#39;high&#39;], dtype=&#39;object&#39;) 15.9.2.2 .codes Codes are actual integer value stored as array. 1 represent ‘high’, temp_cat.codes ## array([0, 2, 1, 2, 2, 0, 1, 1, 2], dtype=int8) 15.9.3 Rename Category 15.9.3.1 Renamce To New Category Object .rename_categories() method return a new category object with new changed categories temp = [&#39;low&#39;,&#39;high&#39;,&#39;medium&#39;,&#39;high&#39;,&#39;high&#39;,&#39;low&#39;,&#39;medium&#39;,&#39;medium&#39;,&#39;high&#39;] new_temp_cat = temp_cat.rename_categories([&#39;sejuk&#39;,&#39;sederhana&#39;,&#39;panas&#39;]) new_temp_cat ## [&#39;sejuk&#39;, &#39;panas&#39;, &#39;sederhana&#39;, &#39;panas&#39;, &#39;panas&#39;, &#39;sejuk&#39;, &#39;sederhana&#39;, &#39;sederhana&#39;, &#39;panas&#39;] ## Categories (3, object): [&#39;sejuk&#39; &lt; &#39;sederhana&#39; &lt; &#39;panas&#39;] temp_cat # original category object categories not changed ## [&#39;low&#39;, &#39;high&#39;, &#39;medium&#39;, &#39;high&#39;, &#39;high&#39;, &#39;low&#39;, &#39;medium&#39;, &#39;medium&#39;, &#39;high&#39;] ## Categories (3, object): [&#39;low&#39; &lt; &#39;medium&#39; &lt; &#39;high&#39;] 15.9.3.2 Rename Inplace Observe the original categories had been changed using .rename() temp_cat.categories = [&#39;sejuk&#39;,&#39;sederhana&#39;,&#39;panas&#39;] temp_cat # original category object categories is changed ## [&#39;sejuk&#39;, &#39;panas&#39;, &#39;sederhana&#39;, &#39;panas&#39;, &#39;panas&#39;, &#39;sejuk&#39;, &#39;sederhana&#39;, &#39;sederhana&#39;, &#39;panas&#39;] ## Categories (3, object): [&#39;sejuk&#39; &lt; &#39;sederhana&#39; &lt; &#39;panas&#39;] 15.9.4 Adding New Category This return a new category object with added categories temp_cat_more = temp_cat.add_categories([&#39;susah&#39;,&#39;senang&#39;]) temp_cat_more ## [&#39;sejuk&#39;, &#39;panas&#39;, &#39;sederhana&#39;, &#39;panas&#39;, &#39;panas&#39;, &#39;sejuk&#39;, &#39;sederhana&#39;, &#39;sederhana&#39;, &#39;panas&#39;] ## Categories (5, object): [&#39;sejuk&#39; &lt; &#39;sederhana&#39; &lt; &#39;panas&#39; &lt; &#39;susah&#39; &lt; &#39;senang&#39;] 15.9.5 Removing Category This is not in place, hence return a new categorical object 15.9.5.1 Remove Specific Categor(ies) Elements with its category removed will become NaN temp = [&#39;low&#39;,&#39;high&#39;,&#39;medium&#39;,&#39;high&#39;,&#39;high&#39;,&#39;low&#39;,&#39;medium&#39;,&#39;medium&#39;,&#39;high&#39;] temp_cat = pd.Categorical(temp) temp_cat_removed = temp_cat.remove_categories(&#39;low&#39;) temp_cat_removed ## [NaN, &#39;high&#39;, &#39;medium&#39;, &#39;high&#39;, &#39;high&#39;, NaN, &#39;medium&#39;, &#39;medium&#39;, &#39;high&#39;] ## Categories (2, object): [&#39;high&#39;, &#39;medium&#39;] 15.9.5.2 Remove Unused Category Since categories removed are not used, there is no impact to the element print (temp_cat_more) ## [&#39;sejuk&#39;, &#39;panas&#39;, &#39;sederhana&#39;, &#39;panas&#39;, &#39;panas&#39;, &#39;sejuk&#39;, &#39;sederhana&#39;, &#39;sederhana&#39;, &#39;panas&#39;] ## Categories (5, object): [&#39;sejuk&#39; &lt; &#39;sederhana&#39; &lt; &#39;panas&#39; &lt; &#39;susah&#39; &lt; &#39;senang&#39;] temp_cat_more.remove_unused_categories() ## [&#39;sejuk&#39;, &#39;panas&#39;, &#39;sederhana&#39;, &#39;panas&#39;, &#39;panas&#39;, &#39;sejuk&#39;, &#39;sederhana&#39;, &#39;sederhana&#39;, &#39;panas&#39;] ## Categories (3, object): [&#39;sejuk&#39; &lt; &#39;sederhana&#39; &lt; &#39;panas&#39;] 15.9.6 Add and Remove Categories In One Step - Set() temp = [&#39;low&#39;,&#39;high&#39;,&#39;medium&#39;,&#39;high&#39;,&#39;high&#39;,&#39;low&#39;,&#39;medium&#39;,&#39;medium&#39;,&#39;high&#39;] temp_cat = pd.Categorical(temp, ordered=True) temp_cat ## [&#39;low&#39;, &#39;high&#39;, &#39;medium&#39;, &#39;high&#39;, &#39;high&#39;, &#39;low&#39;, &#39;medium&#39;, &#39;medium&#39;, &#39;high&#39;] ## Categories (3, object): [&#39;high&#39; &lt; &#39;low&#39; &lt; &#39;medium&#39;] temp_cat.set_categories([&#39;low&#39;,&#39;medium&#39;,&#39;sederhana&#39;,&#39;susah&#39;,&#39;senang&#39;]) ## [&#39;low&#39;, NaN, &#39;medium&#39;, NaN, NaN, &#39;low&#39;, &#39;medium&#39;, &#39;medium&#39;, NaN] ## Categories (5, object): [&#39;low&#39; &lt; &#39;medium&#39; &lt; &#39;sederhana&#39; &lt; &#39;susah&#39; &lt; &#39;senang&#39;] 15.9.7 Categorical Descriptive Analysis 15.9.7.1 At One Glance temp_cat.describe() ## counts freqs ## categories ## high 4 0.444444 ## low 2 0.222222 ## medium 3 0.333333 15.9.7.2 Frequency Count temp_cat.value_counts() ## high 4 ## low 2 ## medium 3 ## dtype: int64 15.9.7.3 Least Frequent Category, Most Frequent Category, and Most Frequent Category ( temp_cat.min(), temp_cat.max(), temp_cat.mode() ) ## (&#39;high&#39;, &#39;medium&#39;, [&#39;high&#39;] ## Categories (3, object): [&#39;high&#39; &lt; &#39;low&#39; &lt; &#39;medium&#39;]) ## ## &lt;string&gt;:1: FutureWarning: Categorical.mode is deprecated and will be removed in a future version. Use Series.mode instead. 15.9.8 Other Methods 15.9.8.1 .get_values() Since actual value stored by categorical object are integer codes, get_values() function return values translated from *.codes** property temp_cat#array ## [&#39;low&#39;, &#39;high&#39;, &#39;medium&#39;, &#39;high&#39;, &#39;high&#39;, &#39;low&#39;, &#39;medium&#39;, &#39;medium&#39;, &#39;high&#39;] ## Categories (3, object): [&#39;high&#39; &lt; &#39;low&#39; &lt; &#39;medium&#39;] 15.10 Dummies get_dummies creates columns for each categories The underlying data can be string or pd.Categorical It produces a new pd.DataFrame 15.10.1 Sample Data df = pd.DataFrame ( {&#39;A&#39;: [&#39;A1&#39;,&#39;A2&#39;,&#39;A3&#39;,&#39;A1&#39;,&#39;A3&#39;,&#39;A1&#39;], &#39;B&#39;: [&#39;B1&#39;,&#39;B2&#39;,&#39;B3&#39;,&#39;B1&#39;,&#39;B1&#39;,&#39;B3&#39;], &#39;C&#39;: [&#39;C1&#39;,&#39;C2&#39;,&#39;C3&#39;,&#39;C1&#39;,np.nan,np.nan]}) df ## A B C ## 0 A1 B1 C1 ## 1 A2 B2 C2 ## 2 A3 B3 C3 ## 3 A1 B1 C1 ## 4 A3 B1 NaN ## 5 A1 B3 NaN 15.10.2 Dummies on Array-Like Data pd.get_dummies(df.A) ## A1 A2 A3 ## 0 1 0 0 ## 1 0 1 0 ## 2 0 0 1 ## 3 1 0 0 ## 4 0 0 1 ## 5 1 0 0 15.10.3 Dummies on DataFrame (multiple columns) 15.10.3.1 All Columns pd.get_dummies(df) ## A_A1 A_A2 A_A3 B_B1 B_B2 B_B3 C_C1 C_C2 C_C3 ## 0 1 0 0 1 0 0 1 0 0 ## 1 0 1 0 0 1 0 0 1 0 ## 2 0 0 1 0 0 1 0 0 1 ## 3 1 0 0 1 0 0 1 0 0 ## 4 0 0 1 1 0 0 0 0 0 ## 5 1 0 0 0 0 1 0 0 0 15.10.3.2 Selected Columns cols = [&#39;A&#39;,&#39;B&#39;] pd.get_dummies(df[cols]) ## A_A1 A_A2 A_A3 B_B1 B_B2 B_B3 ## 0 1 0 0 1 0 0 ## 1 0 1 0 0 1 0 ## 2 0 0 1 0 0 1 ## 3 1 0 0 1 0 0 ## 4 0 0 1 1 0 0 ## 5 1 0 0 0 0 1 15.10.4 Dummies with na By default, nan values are ignored pd.get_dummies(df.C) ## C1 C2 C3 ## 0 1 0 0 ## 1 0 1 0 ## 2 0 0 1 ## 3 1 0 0 ## 4 0 0 0 ## 5 0 0 0 Make NaN as a dummy variable pd.get_dummies(df.C,dummy_na=True) ## C1 C2 C3 NaN ## 0 1 0 0 0 ## 1 0 1 0 0 ## 2 0 0 1 0 ## 3 1 0 0 0 ## 4 0 0 0 1 ## 5 0 0 0 1 15.10.5 Specify Prefixes pd.get_dummies(df.A, prefix=&#39;col&#39;) ## col_A1 col_A2 col_A3 ## 0 1 0 0 ## 1 0 1 0 ## 2 0 0 1 ## 3 1 0 0 ## 4 0 0 1 ## 5 1 0 0 pd.get_dummies(df[cols], prefix=[&#39;colA&#39;,&#39;colB&#39;]) ## colA_A1 colA_A2 colA_A3 colB_B1 colB_B2 colB_B3 ## 0 1 0 0 1 0 0 ## 1 0 1 0 0 1 0 ## 2 0 0 1 0 0 1 ## 3 1 0 0 1 0 0 ## 4 0 0 1 1 0 0 ## 5 1 0 0 0 0 1 15.10.6 Dropping First Column Dummies cause colinearity issue for regression as it has redundant column. Dropping a column does not loose any information technically pd.get_dummies(df[cols],drop_first=True) ## A_A2 A_A3 B_B2 B_B3 ## 0 0 0 0 0 ## 1 1 0 1 0 ## 2 0 1 0 1 ## 3 0 0 0 0 ## 4 0 1 0 0 ## 5 0 0 0 1 15.11 DataFrameGroupBy groupby() is a DataFrame method, it returns DataFrameGroupBy object DataFrameGroupBy object open doors for dataframe aggregation and summarization DataFrameGroupBy object is a very flexible abstraction. In many ways, you can simply treat DataFrameGroup as if it’s a collection of DataFrames, and it does the difficult things under the hood 15.11.1 Sample Data company = pd.read_csv(&#39;data/company.csv&#39;) company ## Company Department Name Age Salary Birthdate ## 0 C1 D1 Yong 45 15000 1/1/1970 ## 1 C1 D1 Chew 35 12000 2/1/1980 ## 2 C1 D2 Lim 34 8000 2/19/1977 ## 3 C1 D3 Jessy 23 2500 3/15/1990 ## 4 C1 D3 Hoi Ming 55 25000 4/15/1987 ## .. ... ... ... ... ... ... ## 13 C3 D3 Chang 32 7900 7/26/1973 ## 14 C3 D1 Ong 44 17500 8/21/1980 ## 15 C3 D2 Lily 41 15300 7/17/1990 ## 16 C3 D3 Sally 54 21000 7/19/1968 ## 17 C3 D3 Esther 37 13500 3/16/1969 ## ## [18 rows x 6 columns] 15.11.2 Creating Groups Group can be created for single or multiple columns com_grp = company.groupby(&#39;Company&#39;) ## Single Column com_dep_grp = company.groupby([&#39;Company&#39;,&#39;Department&#39;]) ## Multiple Column type(com_dep_grp) ## &lt;class &#39;pandas.core.groupby.generic.DataFrameGroupBy&#39;&gt; 15.11.3 Properties 15.11.3.1 Number of Groups com_dep_grp.ngroups ## 9 15.11.3.2 Row Numbers Association .groups property is a dictionary containing group key (identifying the group) and its values (underlying row indexes for the group) gdict = com_dep_grp.groups # return Dictionary print( gdict.keys() , &#39;\\n\\n&#39;, # group identifier gdict.values() ) # group row indexes ## dict_keys([(&#39;C1&#39;, &#39;D1&#39;), (&#39;C1&#39;, &#39;D2&#39;), (&#39;C1&#39;, &#39;D3&#39;), (&#39;C2&#39;, &#39;D1&#39;), (&#39;C2&#39;, &#39;D2&#39;), (&#39;C2&#39;, &#39;D3&#39;), (&#39;C3&#39;, &#39;D1&#39;), (&#39;C3&#39;, &#39;D2&#39;), (&#39;C3&#39;, &#39;D3&#39;)]) ## ## dict_values([Int64Index([0, 1], dtype=&#39;int64&#39;), Int64Index([2], dtype=&#39;int64&#39;), Int64Index([3, 4, 5], dtype=&#39;int64&#39;), Int64Index([6], dtype=&#39;int64&#39;), Int64Index([7, 8, 9], dtype=&#39;int64&#39;), Int64Index([10, 11, 12], dtype=&#39;int64&#39;), Int64Index([14], dtype=&#39;int64&#39;), Int64Index([15], dtype=&#39;int64&#39;), Int64Index([13, 16, 17], dtype=&#39;int64&#39;)]) 15.11.4 Methods 15.11.4.1 Number of Rows In Each Group com_dep_grp.size() # return panda Series object ## Company Department ## C1 D1 2 ## D2 1 ## D3 3 ## C2 D1 1 ## D2 3 ## D3 3 ## C3 D1 1 ## D2 1 ## D3 3 ## dtype: int64 15.11.5 Retrieve Rows 15.11.5.1 Retrieve n-th Row Of Each Grou Row number is 0-based For First row, use .first() or nth(0) print( com_dep_grp.nth(0) , &#39;\\n&#39;, com_dep_grp.first()) ## Name Age Salary Birthdate ## Company Department ## C1 D1 Yong 45 15000 1/1/1970 ## D2 Lim 34 8000 2/19/1977 ## D3 Jessy 23 2500 3/15/1990 ## C2 D1 Anne 18 400 7/15/1997 ## D2 Deborah 30 8600 8/15/1984 ## D3 Michael 38 17000 11/30/1997 ## C3 D1 Ong 44 17500 8/21/1980 ## D2 Lily 41 15300 7/17/1990 ## D3 Chang 32 7900 7/26/1973 ## Name Age Salary Birthdate ## Company Department ## C1 D1 Yong 45 15000 1/1/1970 ## D2 Lim 34 8000 2/19/1977 ## D3 Jessy 23 2500 3/15/1990 ## C2 D1 Anne 18 400 7/15/1997 ## D2 Deborah 30 8600 8/15/1984 ## D3 Michael 38 17000 11/30/1997 ## C3 D1 Ong 44 17500 8/21/1980 ## D2 Lily 41 15300 7/17/1990 ## D3 Chang 32 7900 7/26/1973 For Last row, use .last() or nth(-1)` print( com_dep_grp.nth(-1) , &#39;\\n&#39;, com_dep_grp.last()) ## Name Age Salary Birthdate ## Company Department ## C1 D1 Chew 35 12000 2/1/1980 ## D2 Lim 34 8000 2/19/1977 ## D3 Sui Wei 56 3000 6/15/1990 ## C2 D1 Anne 18 400 7/15/1997 ## D2 Jimmy 46 14000 10/31/1988 ## D3 Bernard 29 9800 12/1/1963 ## C3 D1 Ong 44 17500 8/21/1980 ## D2 Lily 41 15300 7/17/1990 ## D3 Esther 37 13500 3/16/1969 ## Name Age Salary Birthdate ## Company Department ## C1 D1 Chew 35 12000 2/1/1980 ## D2 Lim 34 8000 2/19/1977 ## D3 Sui Wei 56 3000 6/15/1990 ## C2 D1 Anne 18 400 7/15/1997 ## D2 Jimmy 46 14000 10/31/1988 ## D3 Bernard 29 9800 12/1/1963 ## C3 D1 Ong 44 17500 8/21/1980 ## D2 Lily 41 15300 7/17/1990 ## D3 Esther 37 13500 3/16/1969 15.11.5.2 Retrieve N Rows Of Each Groups Example below retrieve 2 rows from each group com_dep_grp.head(2) ## Company Department Name Age Salary Birthdate ## 0 C1 D1 Yong 45 15000 1/1/1970 ## 1 C1 D1 Chew 35 12000 2/1/1980 ## 2 C1 D2 Lim 34 8000 2/19/1977 ## 3 C1 D3 Jessy 23 2500 3/15/1990 ## 4 C1 D3 Hoi Ming 55 25000 4/15/1987 ## .. ... ... ... ... ... ... ## 11 C2 D3 Jeannie 30 12500 12/31/1980 ## 13 C3 D3 Chang 32 7900 7/26/1973 ## 14 C3 D1 Ong 44 17500 8/21/1980 ## 15 C3 D2 Lily 41 15300 7/17/1990 ## 16 C3 D3 Sally 54 21000 7/19/1968 ## ## [14 rows x 6 columns] 15.11.5.3 Retrieve All Rows Of Specific Group get_group() retrieves all rows within the specified group. com_dep_grp.get_group((&#39;C1&#39;,&#39;D3&#39;)) ## Company Department Name Age Salary Birthdate ## 3 C1 D3 Jessy 23 2500 3/15/1990 ## 4 C1 D3 Hoi Ming 55 25000 4/15/1987 ## 5 C1 D3 Sui Wei 56 3000 6/15/1990 15.11.6 Single Statistic Per Group 15.11.6.1 count() count() for valid data (not null) for each fields within the group com_dep_grp.count() # return panda DataFrame object ## Name Age Salary Birthdate ## Company Department ## C1 D1 2 2 2 2 ## D2 1 1 1 1 ## D3 3 3 3 3 ## C2 D1 1 1 1 1 ## D2 3 3 3 3 ## D3 3 3 3 3 ## C3 D1 1 1 1 1 ## D2 1 1 1 1 ## D3 3 3 3 3 15.11.6.2 sum() This sums up all numeric columns for each group com_dep_grp.sum() ## Age Salary ## Company Department ## C1 D1 80 27000 ## D2 34 8000 ## D3 134 30500 ## C2 D1 18 400 ## D2 127 34600 ## D3 97 39300 ## C3 D1 44 17500 ## D2 41 15300 ## D3 123 42400 To sum specific columns of each group, use ['columnName'] to select the column. When single column is selected, output is a Series com_dep_grp[&#39;Age&#39;].sum() ## Company Department ## C1 D1 80 ## D2 34 ## D3 134 ## C2 D1 18 ## D2 127 ## D3 97 ## C3 D1 44 ## D2 41 ## D3 123 ## Name: Age, dtype: int64 15.11.6.3 mean() This average up all numeric columns for each group com_dep_grp.mean() ## Age Salary ## Company Department ## C1 D1 40.000000 13500.000000 ## D2 34.000000 8000.000000 ## D3 44.666667 10166.666667 ## C2 D1 18.000000 400.000000 ## D2 42.333333 11533.333333 ## D3 32.333333 13100.000000 ## C3 D1 44.000000 17500.000000 ## D2 41.000000 15300.000000 ## D3 41.000000 14133.333333 To average specific columns of each group, use ['columnName'] to select the column. When single column is selected, output is a Series com_dep_grp[&#39;Age&#39;].mean() ## Company Department ## C1 D1 40.000000 ## D2 34.000000 ## D3 44.666667 ## C2 D1 18.000000 ## D2 42.333333 ## D3 32.333333 ## C3 D1 44.000000 ## D2 41.000000 ## D3 41.000000 ## Name: Age, dtype: float64 15.11.7 Multi Statistic Per Group 15.11.7.1 Single Function To Column(s) Instructions for aggregation are provided in the form of a dictionary. Dictionary keys specifies the column name, and value as the function to run Can use lambda x: to customize the calclulation on entire column (x) Python built-in function names does can be supplied without wrapping in string 'function' com_dep_grp.agg({ &#39;Age&#39;: sum , ## Total age of the group &#39;Salary&#39;: lambda x: max(x), ## Highest salary of the group &#39;Birthdate&#39;: &#39;first&#39; ## First birthday of the group }) ## Age Salary Birthdate ## Company Department ## C1 D1 80 15000 1/1/1970 ## D2 34 8000 2/19/1977 ## D3 134 25000 3/15/1990 ## C2 D1 18 400 7/15/1997 ## D2 127 14000 8/15/1984 ## D3 97 17000 11/30/1997 ## C3 D1 44 17500 8/21/1980 ## D2 41 15300 7/17/1990 ## D3 123 21000 7/26/1973 15.11.7.2 Multiple Function to Column(s) Use list of function names to specify functions to be applied on a particular column Notice that output columns are MultiIndex , indicating the name of funcitons appled on level 1 ag = com_dep_grp.agg({ &#39;Age&#39;: [&#39;mean&#39;, sum ], ## Average age of the group &#39;Salary&#39;: lambda x: max(x), ## Highest salary of the group &#39;Birthdate&#39;: &#39;first&#39; ## First birthday of the group }) print (ag, &#39;\\n\\n&#39;, ag.columns) ## Age Salary Birthdate ## mean sum &lt;lambda&gt; first ## Company Department ## C1 D1 40.000000 80 15000 1/1/1970 ## D2 34.000000 34 8000 2/19/1977 ## D3 44.666667 134 25000 3/15/1990 ## C2 D1 18.000000 18 400 7/15/1997 ## D2 42.333333 127 14000 8/15/1984 ## D3 32.333333 97 17000 11/30/1997 ## C3 D1 44.000000 44 17500 8/21/1980 ## D2 41.000000 41 15300 7/17/1990 ## D3 41.000000 123 21000 7/26/1973 ## ## MultiIndex([( &#39;Age&#39;, &#39;mean&#39;), ## ( &#39;Age&#39;, &#39;sum&#39;), ## ( &#39;Salary&#39;, &#39;&lt;lambda&gt;&#39;), ## (&#39;Birthdate&#39;, &#39;first&#39;)], ## ) 15.11.7.3 Column Relabling Introduced in Pandas 0.25.0, groupby aggregation with relabelling is supported using “named aggregation” with simple tuples com_dep_grp.agg( max_age = (&#39;Age&#39;, max), salary_m100 = (&#39;Salary&#39;, lambda x: max(x)+100), first_bd = (&#39;Birthdate&#39;, &#39;first&#39;) ) ## max_age salary_m100 first_bd ## Company Department ## C1 D1 45 15100 1/1/1970 ## D2 34 8100 2/19/1977 ## D3 56 25100 3/15/1990 ## C2 D1 18 500 7/15/1997 ## D2 51 14100 8/15/1984 ## D3 38 17100 11/30/1997 ## C3 D1 44 17600 8/21/1980 ## D2 41 15400 7/17/1990 ## D3 54 21100 7/26/1973 15.11.8 Iteration DataFrameGroupBy object can be thought as a collection of named groups def print_groups (g): for name,group in g: print (name) print (group[:2]) print_groups (com_grp) ## C1 ## Company Department Name Age Salary Birthdate ## 0 C1 D1 Yong 45 15000 1/1/1970 ## 1 C1 D1 Chew 35 12000 2/1/1980 ## C2 ## Company Department Name Age Salary Birthdate ## 6 C2 D1 Anne 18 400 7/15/1997 ## 7 C2 D2 Deborah 30 8600 8/15/1984 ## C3 ## Company Department Name Age Salary Birthdate ## 13 C3 D3 Chang 32 7900 7/26/1973 ## 14 C3 D1 Ong 44 17500 8/21/1980 com_grp ## &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x000002137B2ECCD0&gt; 15.11.9 Transform Transform is an operation used combined with DataFrameGroupBy object transform() return a new DataFrame object grp = company.groupby(&#39;Company&#39;) grp.size() ## Company ## C1 6 ## C2 7 ## C3 5 ## dtype: int64 transform() perform a function to a group, and expands and replicate it to multiple rows according to original DataFrame grp[[&#39;Age&#39;,&#39;Salary&#39;]].transform(&#39;sum&#39;) ## Age Salary ## 0 248 65500 ## 1 248 65500 ## 2 248 65500 ## 3 248 65500 ## 4 248 65500 ## .. ... ... ## 13 208 75200 ## 14 208 75200 ## 15 208 75200 ## 16 208 75200 ## 17 208 75200 ## ## [18 rows x 2 columns] grp.transform( lambda x:x+10 ) ## Age Salary ## 0 55 15010 ## 1 45 12010 ## 2 44 8010 ## 3 33 2510 ## 4 65 25010 ## .. ... ... ## 13 42 7910 ## 14 54 17510 ## 15 51 15310 ## 16 64 21010 ## 17 47 13510 ## ## [18 rows x 2 columns] ## ## &lt;string&gt;:1: FutureWarning: Dropping invalid columns in DataFrameGroupBy.transform is deprecated. In a future version, a TypeError will be raised. Before calling .transform, select only columns which should be valid for the function. ## &lt;string&gt;:1: FutureWarning: Dropping invalid columns in DataFrameGroupBy.transform is deprecated. In a future version, a TypeError will be raised. Before calling .transform, select only columns which should be valid for the function. 15.12 Fundamental Analysis 15.13 Missing Data 15.13.1 Sample Data df = pd.DataFrame( np.random.randn(5, 3), index =[&#39;a&#39;, &#39;c&#39;, &#39;e&#39;, &#39;f&#39;, &#39;h&#39;], columns =[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]) df[&#39;four&#39;] = &#39;bar&#39; df[&#39;five&#39;] = df[&#39;one&#39;] &gt; 0 #df df.reindex([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;]) ## one two three four five ## a -0.155909 -0.501790 0.235569 bar False ## b NaN NaN NaN NaN NaN ## c -1.763605 -1.095862 -1.087766 bar False ## d NaN NaN NaN NaN NaN ## e -0.305170 -0.473748 -0.200595 bar False ## f 0.355197 0.689518 0.410590 bar True ## g NaN NaN NaN NaN NaN ## h -0.564978 0.599391 -0.162936 bar False How Missing Data For Each Column ? df.count() ## one 5 ## two 5 ## three 5 ## four 5 ## five 5 ## dtype: int64 len(df.index) - df.count() ## one 0 ## two 0 ## three 0 ## four 0 ## five 0 ## dtype: int64 df.isnull() ## one two three four five ## a False False False False False ## c False False False False False ## e False False False False False ## f False False False False False ## h False False False False False df.describe() ## one two three ## count 5.000000 5.000000 5.000000 ## mean -0.486893 -0.156498 -0.161028 ## std 0.788635 0.772882 0.579752 ## min -1.763605 -1.095862 -1.087766 ## 25% -0.564978 -0.501790 -0.200595 ## 50% -0.305170 -0.473748 -0.162936 ## 75% -0.155909 0.599391 0.235569 ## max 0.355197 0.689518 0.410590 "],["plydata-dplyr-for-python.html", "Chapter 16 Plydata (dplyr for Python) 16.1 Sample Data 16.2 Column Manipulation 16.3 Sorting (arrange) 16.4 Grouping 16.5 Summarization", " Chapter 16 Plydata (dplyr for Python) 16.1 Sample Data import pandas as pd import numpy as np from plydata import define, query, if_else, ply, select, rename, arrange, group_by, summarize n = 200 comp = [&#39;C&#39; + i for i in np.random.randint( 1,4, size = n).astype(str)] # 3x Company dept = [&#39;D&#39; + i for i in np.random.randint( 1,6, size = n).astype(str)] # 5x Department grp = [&#39;G&#39; + i for i in np.random.randint( 1,3, size = n).astype(str)] # 2x Groups value1 = np.random.normal( loc=50 , scale=5 , size = n) value2 = np.random.normal( loc=20 , scale=3 , size = n) #value3 = np.random.normal( loc=5 , scale=30 , size = n) mydf = pd.DataFrame({ &#39;comp&#39;:comp, &#39;dept&#39;:dept, &#39;grp&#39;: grp, &#39;value1&#39;:value1, &#39;value2&#39;:value2 #&#39;value3&#39;:value3 }) mydf.head() ## comp dept grp value1 value2 ## 0 C2 D5 G1 58.154636 19.606496 ## 1 C1 D1 G2 57.886281 21.870195 ## 2 C3 D4 G1 55.574762 19.010111 ## 3 C3 D2 G1 52.501182 17.915301 ## 4 C1 D2 G1 54.974855 21.116552 16.2 Column Manipulation 16.2.1 Copy Column mydf &gt;&gt; define(newcol = &#39;value1&#39;) # simple method for one column ## comp dept grp value1 value2 newcol ## 0 C2 D5 G1 58.154636 19.606496 58.154636 ## 1 C1 D1 G2 57.886281 21.870195 57.886281 ## 2 C3 D4 G1 55.574762 19.010111 55.574762 ## 3 C3 D2 G1 52.501182 17.915301 52.501182 ## 4 C1 D2 G1 54.974855 21.116552 54.974855 ## .. ... ... .. ... ... ... ## 195 C1 D2 G2 47.299345 13.625440 47.299345 ## 196 C1 D4 G1 54.456964 19.673677 54.456964 ## 197 C2 D3 G2 45.197945 22.006431 45.197945 ## 198 C2 D3 G2 46.494690 19.140572 46.494690 ## 199 C1 D5 G2 48.742266 19.927472 48.742266 ## ## [200 rows x 6 columns] mydf &gt;&gt; define ((&#39;newcol1&#39;, &#39;value1&#39;), newcol2=&#39;value2&#39;) # method for muiltiple new columns ## comp dept grp value1 value2 newcol1 newcol2 ## 0 C2 D5 G1 58.154636 19.606496 58.154636 19.606496 ## 1 C1 D1 G2 57.886281 21.870195 57.886281 21.870195 ## 2 C3 D4 G1 55.574762 19.010111 55.574762 19.010111 ## 3 C3 D2 G1 52.501182 17.915301 52.501182 17.915301 ## 4 C1 D2 G1 54.974855 21.116552 54.974855 21.116552 ## .. ... ... .. ... ... ... ... ## 195 C1 D2 G2 47.299345 13.625440 47.299345 13.625440 ## 196 C1 D4 G1 54.456964 19.673677 54.456964 19.673677 ## 197 C2 D3 G2 45.197945 22.006431 45.197945 22.006431 ## 198 C2 D3 G2 46.494690 19.140572 46.494690 19.140572 ## 199 C1 D5 G2 48.742266 19.927472 48.742266 19.927472 ## ## [200 rows x 7 columns] 16.2.2 New Column from existing Column Without specify the new column name, it will be derived from expression mydf &gt;&gt; define (&#39;value1*2&#39;) ## comp dept grp value1 value2 value1*2 ## 0 C2 D5 G1 58.154636 19.606496 116.309272 ## 1 C1 D1 G2 57.886281 21.870195 115.772561 ## 2 C3 D4 G1 55.574762 19.010111 111.149525 ## 3 C3 D2 G1 52.501182 17.915301 105.002363 ## 4 C1 D2 G1 54.974855 21.116552 109.949711 ## .. ... ... .. ... ... ... ## 195 C1 D2 G2 47.299345 13.625440 94.598689 ## 196 C1 D4 G1 54.456964 19.673677 108.913928 ## 197 C2 D3 G2 45.197945 22.006431 90.395889 ## 198 C2 D3 G2 46.494690 19.140572 92.989380 ## 199 C1 D5 G2 48.742266 19.927472 97.484533 ## ## [200 rows x 6 columns] Specify the new column name mydf &gt;&gt; define(value3 = &#39;value1*2&#39;) ## comp dept grp value1 value2 value3 ## 0 C2 D5 G1 58.154636 19.606496 116.309272 ## 1 C1 D1 G2 57.886281 21.870195 115.772561 ## 2 C3 D4 G1 55.574762 19.010111 111.149525 ## 3 C3 D2 G1 52.501182 17.915301 105.002363 ## 4 C1 D2 G1 54.974855 21.116552 109.949711 ## .. ... ... .. ... ... ... ## 195 C1 D2 G2 47.299345 13.625440 94.598689 ## 196 C1 D4 G1 54.456964 19.673677 108.913928 ## 197 C2 D3 G2 45.197945 22.006431 90.395889 ## 198 C2 D3 G2 46.494690 19.140572 92.989380 ## 199 C1 D5 G2 48.742266 19.927472 97.484533 ## ## [200 rows x 6 columns] Define multiple new columns in one go. Observe there are three ways to specify the new columns mydf &gt;&gt; define(&#39;value1*2&#39;,(&#39;newcol2&#39;,&#39;value2*2&#39;),newcol3=&#39;value2*3&#39;) ## comp dept grp value1 value2 value1*2 newcol2 newcol3 ## 0 C2 D5 G1 58.154636 19.606496 116.309272 39.212991 58.819487 ## 1 C1 D1 G2 57.886281 21.870195 115.772561 43.740391 65.610586 ## 2 C3 D4 G1 55.574762 19.010111 111.149525 38.020221 57.030332 ## 3 C3 D2 G1 52.501182 17.915301 105.002363 35.830602 53.745903 ## 4 C1 D2 G1 54.974855 21.116552 109.949711 42.233104 63.349656 ## .. ... ... .. ... ... ... ... ... ## 195 C1 D2 G2 47.299345 13.625440 94.598689 27.250880 40.876320 ## 196 C1 D4 G1 54.456964 19.673677 108.913928 39.347354 59.021031 ## 197 C2 D3 G2 45.197945 22.006431 90.395889 44.012862 66.019294 ## 198 C2 D3 G2 46.494690 19.140572 92.989380 38.281144 57.421716 ## 199 C1 D5 G2 48.742266 19.927472 97.484533 39.854945 59.782417 ## ## [200 rows x 8 columns] 16.2.3 Select Column(s) mydf2 = mydf &gt;&gt; define(newcol1=&#39;value1&#39;,newcol2=&#39;value2&#39;) mydf2.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 200 entries, 0 to 199 ## Data columns (total 7 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 comp 200 non-null object ## 1 dept 200 non-null object ## 2 grp 200 non-null object ## 3 value1 200 non-null float64 ## 4 value2 200 non-null float64 ## 5 newcol1 200 non-null float64 ## 6 newcol2 200 non-null float64 ## dtypes: float64(4), object(3) ## memory usage: 11.1+ KB 16.2.3.1 By Column Names Exact Coumn Name mydf2 &gt;&gt; select (&#39;comp&#39;,&#39;dept&#39;,&#39;value1&#39;) ## comp dept value1 ## 0 C2 D5 58.154636 ## 1 C1 D1 57.886281 ## 2 C3 D4 55.574762 ## 3 C3 D2 52.501182 ## 4 C1 D2 54.974855 ## .. ... ... ... ## 195 C1 D2 47.299345 ## 196 C1 D4 54.456964 ## 197 C2 D3 45.197945 ## 198 C2 D3 46.494690 ## 199 C1 D5 48.742266 ## ## [200 rows x 3 columns] Column Name Starts With … mydf2 &gt;&gt; select (&#39;comp&#39;, startswith=&#39;val&#39;) ## comp value1 value2 ## 0 C2 58.154636 19.606496 ## 1 C1 57.886281 21.870195 ## 2 C3 55.574762 19.010111 ## 3 C3 52.501182 17.915301 ## 4 C1 54.974855 21.116552 ## .. ... ... ... ## 195 C1 47.299345 13.625440 ## 196 C1 54.456964 19.673677 ## 197 C2 45.197945 22.006431 ## 198 C2 46.494690 19.140572 ## 199 C1 48.742266 19.927472 ## ## [200 rows x 3 columns] Column Name Ends With … mydf2 &gt;&gt; select (&#39;comp&#39;,endswith=(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;)) ## comp value1 value2 newcol1 newcol2 ## 0 C2 58.154636 19.606496 58.154636 19.606496 ## 1 C1 57.886281 21.870195 57.886281 21.870195 ## 2 C3 55.574762 19.010111 55.574762 19.010111 ## 3 C3 52.501182 17.915301 52.501182 17.915301 ## 4 C1 54.974855 21.116552 54.974855 21.116552 ## .. ... ... ... ... ... ## 195 C1 47.299345 13.625440 47.299345 13.625440 ## 196 C1 54.456964 19.673677 54.456964 19.673677 ## 197 C2 45.197945 22.006431 45.197945 22.006431 ## 198 C2 46.494690 19.140572 46.494690 19.140572 ## 199 C1 48.742266 19.927472 48.742266 19.927472 ## ## [200 rows x 5 columns] Column Name Contains … mydf2 &gt;&gt; select(&#39;comp&#39;, contains=(&#39;col&#39;,&#39;val&#39;)) ## comp value1 value2 newcol1 newcol2 ## 0 C2 58.154636 19.606496 58.154636 19.606496 ## 1 C1 57.886281 21.870195 57.886281 21.870195 ## 2 C3 55.574762 19.010111 55.574762 19.010111 ## 3 C3 52.501182 17.915301 52.501182 17.915301 ## 4 C1 54.974855 21.116552 54.974855 21.116552 ## .. ... ... ... ... ... ## 195 C1 47.299345 13.625440 47.299345 13.625440 ## 196 C1 54.456964 19.673677 54.456964 19.673677 ## 197 C2 45.197945 22.006431 45.197945 22.006431 ## 198 C2 46.494690 19.140572 46.494690 19.140572 ## 199 C1 48.742266 19.927472 48.742266 19.927472 ## ## [200 rows x 5 columns] 16.2.3.2 Specify Column Range mydf2 &gt;&gt; select (&#39;comp&#39;, slice(&#39;value1&#39;,&#39;newcol2&#39;)) ## comp value1 value2 newcol1 newcol2 ## 0 C2 58.154636 19.606496 58.154636 19.606496 ## 1 C1 57.886281 21.870195 57.886281 21.870195 ## 2 C3 55.574762 19.010111 55.574762 19.010111 ## 3 C3 52.501182 17.915301 52.501182 17.915301 ## 4 C1 54.974855 21.116552 54.974855 21.116552 ## .. ... ... ... ... ... ## 195 C1 47.299345 13.625440 47.299345 13.625440 ## 196 C1 54.456964 19.673677 54.456964 19.673677 ## 197 C2 45.197945 22.006431 45.197945 22.006431 ## 198 C2 46.494690 19.140572 46.494690 19.140572 ## 199 C1 48.742266 19.927472 48.742266 19.927472 ## ## [200 rows x 5 columns] 16.2.4 Drop Column(s) mydf2 &gt;&gt; select(&#39;newcol1&#39;,&#39;newcol2&#39;,drop=True) ## comp dept grp value1 value2 ## 0 C2 D5 G1 58.154636 19.606496 ## 1 C1 D1 G2 57.886281 21.870195 ## 2 C3 D4 G1 55.574762 19.010111 ## 3 C3 D2 G1 52.501182 17.915301 ## 4 C1 D2 G1 54.974855 21.116552 ## .. ... ... .. ... ... ## 195 C1 D2 G2 47.299345 13.625440 ## 196 C1 D4 G1 54.456964 19.673677 ## 197 C2 D3 G2 45.197945 22.006431 ## 198 C2 D3 G2 46.494690 19.140572 ## 199 C1 D5 G2 48.742266 19.927472 ## ## [200 rows x 5 columns] mydf &gt;&gt; rename( {&#39;val.1&#39; : &#39;value1&#39;, &#39;val.2&#39; : &#39;value2&#39; }) ## comp dept grp val.1 val.2 ## 0 C2 D5 G1 58.154636 19.606496 ## 1 C1 D1 G2 57.886281 21.870195 ## 2 C3 D4 G1 55.574762 19.010111 ## 3 C3 D2 G1 52.501182 17.915301 ## 4 C1 D2 G1 54.974855 21.116552 ## .. ... ... .. ... ... ## 195 C1 D2 G2 47.299345 13.625440 ## 196 C1 D4 G1 54.456964 19.673677 ## 197 C2 D3 G2 45.197945 22.006431 ## 198 C2 D3 G2 46.494690 19.140572 ## 199 C1 D5 G2 48.742266 19.927472 ## ## [200 rows x 5 columns] Combined Method Combine both assignment and dictionary method mydf &gt;&gt; rename( {&#39;val.1&#39; : &#39;value1&#39;, &#39;val.2&#39; : &#39;value2&#39; }, group = &#39;grp&#39; ) ## comp dept group val.1 val.2 ## 0 C2 D5 G1 58.154636 19.606496 ## 1 C1 D1 G2 57.886281 21.870195 ## 2 C3 D4 G1 55.574762 19.010111 ## 3 C3 D2 G1 52.501182 17.915301 ## 4 C1 D2 G1 54.974855 21.116552 ## .. ... ... ... ... ... ## 195 C1 D2 G2 47.299345 13.625440 ## 196 C1 D4 G1 54.456964 19.673677 ## 197 C2 D3 G2 45.197945 22.006431 ## 198 C2 D3 G2 46.494690 19.140572 ## 199 C1 D5 G2 48.742266 19.927472 ## ## [200 rows x 5 columns] 16.3 Sorting (arrange) Use ‘-colName’ for decending mydf &gt;&gt; arrange(&#39;comp&#39;, &#39;-value1&#39;) ## comp dept grp value1 value2 ## 24 C1 D2 G1 59.492155 17.723157 ## 1 C1 D1 G2 57.886281 21.870195 ## 149 C1 D1 G1 57.717468 18.031205 ## 54 C1 D5 G1 57.172568 18.368049 ## 170 C1 D1 G2 56.611681 21.602959 ## .. ... ... .. ... ... ## 38 C3 D3 G2 42.363468 18.466057 ## 110 C3 D3 G2 41.886945 17.573542 ## 18 C3 D4 G2 40.371106 22.077577 ## 147 C3 D5 G2 39.872349 28.687536 ## 62 C3 D2 G2 36.559909 18.899219 ## ## [200 rows x 5 columns] 16.4 Grouping mydf.info() ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 200 entries, 0 to 199 ## Data columns (total 5 columns): ## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 comp 200 non-null object ## 1 dept 200 non-null object ## 2 grp 200 non-null object ## 3 value1 200 non-null float64 ## 4 value2 200 non-null float64 ## dtypes: float64(2), object(3) ## memory usage: 7.9+ KB gdf = mydf &gt;&gt; group_by(&#39;comp&#39;,&#39;dept&#39;) type(gdf) ## &lt;class &#39;plydata.types.GroupedDataFrame&#39;&gt; 16.5 Summarization 16.5.1 Simple Method Passing Multiple Expressions gdf &gt;&gt; summarize(&#39;n()&#39;,&#39;sum(value1)&#39;,&#39;mean(value2)&#39;) ## comp dept n() sum(value1) mean(value2) ## 0 C2 D5 16 859.712686 19.872350 ## 1 C1 D1 15 780.839425 18.973080 ## 2 C3 D4 17 866.578096 19.989500 ## 3 C3 D2 16 804.736768 20.236558 ## 4 C1 D2 14 670.369267 18.839716 ## 5 C2 D4 10 496.987652 19.698027 ## 6 C3 D3 8 407.062893 17.275397 ## 7 C3 D5 14 676.593657 21.364989 ## 8 C1 D4 10 503.749639 19.259360 ## 9 C1 D5 15 752.126742 21.243452 ## 10 C2 D2 14 698.304608 21.182864 ## 11 C3 D1 15 773.374816 19.536946 ## 12 C1 D3 11 550.998497 19.150402 ## 13 C2 D3 15 757.429609 19.763727 ## 14 C2 D1 10 483.481779 19.433901 16.5.2 Specify Summarized Column Name Assignment Method - Passing colName=‘expression’** - Column name cannot contain special character gdf &gt;&gt; summarize(count=&#39;n()&#39;,v1sum=&#39;sum(value1)&#39;,v2_mean=&#39;mean(value2)&#39;) ## comp dept count v1sum v2_mean ## 0 C2 D5 16 859.712686 19.872350 ## 1 C1 D1 15 780.839425 18.973080 ## 2 C3 D4 17 866.578096 19.989500 ## 3 C3 D2 16 804.736768 20.236558 ## 4 C1 D2 14 670.369267 18.839716 ## 5 C2 D4 10 496.987652 19.698027 ## 6 C3 D3 8 407.062893 17.275397 ## 7 C3 D5 14 676.593657 21.364989 ## 8 C1 D4 10 503.749639 19.259360 ## 9 C1 D5 15 752.126742 21.243452 ## 10 C2 D2 14 698.304608 21.182864 ## 11 C3 D1 15 773.374816 19.536946 ## 12 C1 D3 11 550.998497 19.150402 ## 13 C2 D3 15 757.429609 19.763727 ## 14 C2 D1 10 483.481779 19.433901 Tuple Method (‘colName’,‘expression’) Use when the column name contain special character gdf &gt;&gt; summarize((&#39;count&#39;,&#39;n()&#39;),(&#39;v1.sum&#39;,&#39;sum(value1)&#39;),(&#39;s2.sum&#39;,&#39;sum(value2)&#39;),v2mean=np.mean(value2)) ## comp dept count v1.sum s2.sum v2mean ## 0 C2 D5 16 859.712686 317.957600 19.832593 ## 1 C1 D1 15 780.839425 284.596205 19.832593 ## 2 C3 D4 17 866.578096 339.821498 19.832593 ## 3 C3 D2 16 804.736768 323.784925 19.832593 ## 4 C1 D2 14 670.369267 263.756020 19.832593 ## 5 C2 D4 10 496.987652 196.980269 19.832593 ## 6 C3 D3 8 407.062893 138.203177 19.832593 ## 7 C3 D5 14 676.593657 299.109844 19.832593 ## 8 C1 D4 10 503.749639 192.593603 19.832593 ## 9 C1 D5 15 752.126742 318.651787 19.832593 ## 10 C2 D2 14 698.304608 296.560090 19.832593 ## 11 C3 D1 15 773.374816 293.054183 19.832593 ## 12 C1 D3 11 550.998497 210.654419 19.832593 ## 13 C2 D3 15 757.429609 296.455909 19.832593 ## 14 C2 D1 10 483.481779 194.339014 19.832593 16.5.3 Number of Rows in Group n() : total rows in group n_unique() : total of rows with unique value gdf &gt;&gt; summarize(count=&#39;n()&#39;, va11_unique=&#39;n_unique(value1)&#39;) ## comp dept count va11_unique ## 0 C2 D5 16 16 ## 1 C1 D1 15 15 ## 2 C3 D4 17 17 ## 3 C3 D2 16 16 ## 4 C1 D2 14 14 ## 5 C2 D4 10 10 ## 6 C3 D3 8 8 ## 7 C3 D5 14 14 ## 8 C1 D4 10 10 ## 9 C1 D5 15 15 ## 10 C2 D2 14 14 ## 11 C3 D1 15 15 ## 12 C1 D3 11 11 ## 13 C2 D3 15 15 ## 14 C2 D1 10 10 "],["sklearn.html", "Chapter 17 sklearn 17.1 The Library 17.2 Model Fitting 17.3 Model Tuning 17.4 High Level ML Process 17.5 Built-in Datasets 17.6 Train Test Data Splitting 17.7 Polynomial Transform 17.8 Imputation of Missing Data 17.9 Scaling 17.10 Pipeline 17.11 Cross Validation", " Chapter 17 sklearn This is a machine learning library. from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:75% !important; margin-left:350px; }&lt;/style&gt;&quot;)) #matplotlib inline ## &lt;IPython.core.display.HTML object&gt; import numpy as np import pandas as pd import matplotlib.pyplot as plt import math pd.set_option( &#39;display.notebook_repr_html&#39;, False) # render Series and DataFrame as text, not HTML pd.set_option( &#39;display.max_column&#39;, 10) # number of columns pd.set_option( &#39;display.max_rows&#39;, 10) # number of rows pd.set_option( &#39;display.width&#39;, 90) # number of characters per row import os os.environ[&#39;QT_QPA_PLATFORM_PLUGIN_PATH&#39;] = &quot;C:\\ProgramData\\Anaconda3\\Library\\plugins\\platforms&quot; #import matplotlib #matplotlib.use(&#39;Qt5Agg&#39;) 17.1 The Library sklearn does not automatically import its subpackages. Therefore all subpakcages must be specifically loaded before use. # Sample Data from sklearn import datasets # Model Selection from sklearn.model_selection import train_test_split from sklearn.model_selection import KFold from sklearn.model_selection import LeaveOneOut from sklearn.model_selection import cross_validate # Preprocessing from sklearn.impute import SimpleImputer from sklearn.preprocessing import MinMaxScaler from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import Normalizer from sklearn.preprocessing import PolynomialFeatures # Model and Pipeline from sklearn.linear_model import LinearRegression,Lasso from sklearn.pipeline import make_pipeline # Measurement from sklearn.metrics import * import statsmodels.formula.api as smf 17.2 Model Fitting split 17.2.1 Underfitting The model does not fit the training data and therefore misses the trends in the data The model cannot be generalized to new data, this is usually the result of a very simple model (not enough predictors/independent variables) The model will have poor predictive ability For example, we fit a linear model (like linear regression) to data that is not linear 17.2.2 Overfitting The model has trained ?too well? and is now, well, fit too closely to the training dataset The model is too complex (i.e. too many features/variables compared to the number of observations) The model will be very accurate on the training data but will probably be very not accurate on untrained or new data The model is not generalized (or not AS generalized), meaning you can generalize the results The model learns or describes the ?noise? in the training data instead of the actual relationships between variables in the data 17.2.3 Just Right It is worth noting the underfitting is not as prevalent as overfitting Nevertheless, we want to avoid both of those problems in data analysis We want to find the middle ground between under and overfitting our model 17.3 Model Tuning A highly complex model tend to overfit A too flexible model tend to underfit Complexity can be reduced by: - Less features - Less degree of polynomial features - Apply generalization (tuning hyperparameters) split 17.4 High Level ML Process split 17.5 Built-in Datasets sklearn included some popular datasets to play with Each dataset is of type Bunch. It has useful data (array) in the form of properties: - keys (display all data availabe within the dataset) - data (common) - target (common) - DESCR (common) - feature_names (some dataset) - target_names (some dataset) - images (some dataset) 17.5.1 diabetes (regression) 17.5.1.1 Load Dataset diabetes = datasets.load_diabetes() print (type(diabetes)) ## &lt;class &#39;sklearn.utils._bunch.Bunch&#39;&gt; 17.5.1.2 keys diabetes.keys() ## dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;data_filename&#39;, &#39;target_filename&#39;, &#39;data_module&#39;]) 17.5.1.3 Features and Target .data = features - two dimension array .target = target - one dimension array print (type(diabetes.data)) ## &lt;class &#39;numpy.ndarray&#39;&gt; print (type(diabetes.target)) ## &lt;class &#39;numpy.ndarray&#39;&gt; print (diabetes.data.shape) ## (442, 10) print (diabetes.target.shape) ## (442,) 17.5.1.4 Load with X,y (Convenient Method) using return_X_y = True, data is loaded into X, target is loaded into y X,y = datasets.load_diabetes(return_X_y=True) print (X.shape) ## (442, 10) print (y.shape) ## (442,) 17.5.2 digits (Classification) This is a copy of the test set of the UCI ML hand-written digits datasets digits = datasets.load_digits() print (type(digits)) ## &lt;class &#39;sklearn.utils._bunch.Bunch&#39;&gt; print (type(digits.data)) ## &lt;class &#39;numpy.ndarray&#39;&gt; digits.keys() ## dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;feature_names&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;]) digits.target_names ## array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 17.5.2.1 data digits.data.shape # features ## (1797, 64) digits.target.shape # target ## (1797,) 17.5.2.2 Images images is 3 dimensional array There are 1797 samples, each sample is 8x8 pixels digits.images.shape ## (1797, 8, 8) type(digits.images) ## &lt;class &#39;numpy.ndarray&#39;&gt; Each element represent the data that make its target print (digits.target[100]) ## 4 print (digits.images[100]) ## [[ 0. 0. 0. 2. 13. 0. 0. 0.] ## [ 0. 0. 0. 8. 15. 0. 0. 0.] ## [ 0. 0. 5. 16. 5. 2. 0. 0.] ## [ 0. 0. 15. 12. 1. 16. 4. 0.] ## [ 0. 4. 16. 2. 9. 16. 8. 0.] ## [ 0. 0. 10. 14. 16. 16. 4. 0.] ## [ 0. 0. 0. 0. 13. 8. 0. 0.] ## [ 0. 0. 0. 0. 13. 6. 0. 0.]] plt.matshow(digits.images[100]) 17.5.2.3 Loading Into X,y (Convenient Method) X,y = datasets.load_digits(return_X_y=True) X.shape ## (1797, 64) y.shape ## (1797,) 17.5.3 iris (Classification) iris = datasets.load_iris() iris.keys() ## dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;, &#39;data_module&#39;]) 17.5.3.1 Feature Names iris.feature_names ## [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] 17.5.3.2 target iris.target_names ## array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) iris.target ## array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ## 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ## 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ## 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ## 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 17.6 Train Test Data Splitting 17.6.1 Sample Data Generate 100 rows of data, with 3x features (X1,X2,X3), and one dependant variable (Y) n = 21 # number of samples I = 5 # intercept value E = np.random.randint( 1,20, n) # Error x1 = np.random.randint( 1,n+1, n) x2 = np.random.randint( 1,n+1, n) x3 = np.random.randint( 1,n+1, n) y = 0.1*x1 + 0.2*x2 + 0.3*x3 + E + I mydf = pd.DataFrame({ &#39;y&#39;:y, &#39;x1&#39;:x1, &#39;x2&#39;:x2, &#39;x3&#39;:x3 }) mydf.shape ## (21, 4) 17.6.2 One Time Split sklearn::train_test_split() has two forms: - Take one DF, split into 2 DF (most of sklearn modeling use this method - Take two DFs, split into 4 DF mydf.head() ## y x1 x2 x3 ## 0 24.4 17 4 3 ## 1 12.9 5 2 10 ## 2 12.8 7 15 7 ## 3 28.8 1 14 13 ## 4 20.3 5 15 16 17.6.2.1 Method 1: Split One Dataframe Into Two (Train &amp; Test) traindf, testdf = train_test_split( df, test_size=, random_state= ) # random_state : seed number (integer), optional # test_size : fraction of 1, 0.2 means 20% split traindf, testdf = train_test_split(mydf,test_size=0.2, random_state=25) print (len(traindf)) ## 16 print (len(testdf)) ## 5 17.6.2.2 Method 2: Split Two DataFrame (X,Y) into Four x_train/test, y_train/test x_train, x_test, y_train, y_test = train_test_split( X,Y, test_size=, random_state= ) # random_state : seed number (integer), optional # test_size : fraction of 1, 0.2 means 20% split Split DataFrame into X and Y First feature_cols = [&#39;x1&#39;,&#39;x2&#39;,&#39;x3&#39;] X = mydf[feature_cols] Y = mydf.y Then Split X/Y into x_train/test, y_train/test x_train, x_test, y_train, y_test = train_test_split( X,Y, test_size=0.2, random_state=25) print (len(x_train)) ## 16 print (len(x_test)) ## 5 17.6.3 K-Fold KFold(n_splits=3, shuffle=False, random_state=None) split suffle=False (default), meaning index number is taken continously kf = KFold(n_splits=7) for train_index, test_index in kf.split(X): print (train_index, test_index) ## [ 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [0 1 2] ## [ 0 1 2 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [3 4 5] ## [ 0 1 2 3 4 5 9 10 11 12 13 14 15 16 17 18 19 20] [6 7 8] ## [ 0 1 2 3 4 5 6 7 8 12 13 14 15 16 17 18 19 20] [ 9 10 11] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 15 16 17 18 19 20] [12 13 14] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 18 19 20] [15 16 17] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17] [18 19 20] shuffle=True kf = KFold(n_splits=7, shuffle=True) for train_index, test_index in kf.split(X): print (train_index, test_index) ## [ 0 1 2 4 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [3 5 6] ## [ 0 1 2 3 4 5 6 7 10 11 12 14 15 16 17 18 19 20] [ 8 9 13] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 16 19 20] [15 17 18] ## [ 0 1 2 3 5 6 7 8 9 10 11 12 13 15 16 17 18 19] [ 4 14 20] ## [ 0 1 3 4 5 6 7 8 9 10 11 12 13 14 15 17 18 20] [ 2 16 19] ## [ 0 1 2 3 4 5 6 7 8 9 13 14 15 16 17 18 19 20] [10 11 12] ## [ 2 3 4 5 6 8 9 10 11 12 13 14 15 16 17 18 19 20] [0 1 7] 17.6.4 Leave One Out For a dataset of N rows, Leave One Out will split N-1 times, each time leaving one row as test, remaning as training set. Due to the high number of test sets (which is the same as the number of samples-1) this cross-validation method can be very costly. For large datasets one should favor KFold. loo = LeaveOneOut() for train_index, test_index in loo.split(X): print (train_index, test_index) ## [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [0] ## [ 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [1] ## [ 0 1 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [2] ## [ 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [3] ## [ 0 1 2 3 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [4] ## [ 0 1 2 3 4 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [5] ## [ 0 1 2 3 4 5 7 8 9 10 11 12 13 14 15 16 17 18 19 20] [6] ## [ 0 1 2 3 4 5 6 8 9 10 11 12 13 14 15 16 17 18 19 20] [7] ## [ 0 1 2 3 4 5 6 7 9 10 11 12 13 14 15 16 17 18 19 20] [8] ## [ 0 1 2 3 4 5 6 7 8 10 11 12 13 14 15 16 17 18 19 20] [9] ## [ 0 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 19 20] [10] ## [ 0 1 2 3 4 5 6 7 8 9 10 12 13 14 15 16 17 18 19 20] [11] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 13 14 15 16 17 18 19 20] [12] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 14 15 16 17 18 19 20] [13] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 15 16 17 18 19 20] [14] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 16 17 18 19 20] [15] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17 18 19 20] [16] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 18 19 20] [17] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 19 20] [18] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 20] [19] ## [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19] [20] X ## x1 x2 x3 ## 0 17 4 3 ## 1 5 2 10 ## 2 7 15 7 ## 3 1 14 13 ## 4 5 15 16 ## .. .. .. .. ## 16 20 18 18 ## 17 5 16 6 ## 18 16 15 14 ## 19 5 13 18 ## 20 6 8 11 ## ## [21 rows x 3 columns] 17.7 Polynomial Transform This can be used as part of feature engineering, to introduce new features for data that seems to fit with quadradic model. 17.7.1 Single Variable 17.7.1.1 Sample Data Data must be 2-D before polynomial features can be applied. Code below convert 1D array into 2D array. x = np.array([1, 2, 3, 4, 5]) X = x[:,np.newaxis] X ## array([[1], ## [2], ## [3], ## [4], ## [5]]) 17.7.1.2 Degree 1 One Degree means maintain original features. No new features is created. PolynomialFeatures(degree=1, include_bias=False).fit_transform(X) ## array([[1.], ## [2.], ## [3.], ## [4.], ## [5.]]) 17.7.1.3 Degree 2 Degree-1 original feature: x Degree-2 additional features: x^2 PolynomialFeatures(degree=2, include_bias=False).fit_transform(X) ## array([[ 1., 1.], ## [ 2., 4.], ## [ 3., 9.], ## [ 4., 16.], ## [ 5., 25.]]) 17.7.1.4 Degree 3 Degree-1 original feature: x Degree-2 additional features: x^2 Degree-3 additional features: x^3 PolynomialFeatures(degree=3, include_bias=False).fit_transform(X) ## array([[ 1., 1., 1.], ## [ 2., 4., 8.], ## [ 3., 9., 27.], ## [ 4., 16., 64.], ## [ 5., 25., 125.]]) 17.7.1.5 Degree 4 Degree-1 original feature: x Degree-2 additional features: x^2 Degree-3 additional features: x^3 Degree-3 additional features: x^4 PolynomialFeatures(degree=4, include_bias=False).fit_transform(X) ## array([[ 1., 1., 1., 1.], ## [ 2., 4., 8., 16.], ## [ 3., 9., 27., 81.], ## [ 4., 16., 64., 256.], ## [ 5., 25., 125., 625.]]) 17.7.2 Two Variables 17.7.2.1 Sample Data X = pd.DataFrame( {&#39;x1&#39;: [1, 2, 3, 4, 5 ], &#39;x2&#39;: [6, 7, 8, 9, 10]}) X ## x1 x2 ## 0 1 6 ## 1 2 7 ## 2 3 8 ## 3 4 9 ## 4 5 10 17.7.2.2 Degree 2 Degree-1 original features: x1, x2 Degree-2 additional features: x1^2, x2^2, x1:x2 PolynomialFeatures(degree=2, include_bias=False).fit_transform(X) ## array([[ 1., 6., 1., 6., 36.], ## [ 2., 7., 4., 14., 49.], ## [ 3., 8., 9., 24., 64.], ## [ 4., 9., 16., 36., 81.], ## [ 5., 10., 25., 50., 100.]]) 17.7.2.3 Degree 3 Degree-1 original features: x1, x2 Degree-2 additional features: x1^2, x2^2, x1:x2 Degree-3 additional features: x1^3, x2^3 x1:x2^2 x2:x1^2 PolynomialFeatures(degree=3, include_bias=False).fit_transform(X) ## array([[ 1., 6., 1., 6., 36., 1., 6., 36., 216.], ## [ 2., 7., 4., 14., 49., 8., 28., 98., 343.], ## [ 3., 8., 9., 24., 64., 27., 72., 192., 512.], ## [ 4., 9., 16., 36., 81., 64., 144., 324., 729.], ## [ 5., 10., 25., 50., 100., 125., 250., 500., 1000.]]) 17.8 Imputation of Missing Data 17.8.1 Sample Data from numpy import nan X = np.array([[ nan, 0, 3 ], [ 3, 7, 9 ], [ 3, 5, 2 ], [ 4, nan, 6 ], [ 8, 8, 1 ]]) y = np.array([14, 16, -1, 8, -5]) 17.8.2 Imputer 17.8.2.1 mean strategy imp = SimpleImputer(strategy=&#39;mean&#39;) X2 = imp.fit_transform(X) X2 ## array([[4.5, 0. , 3. ], ## [3. , 7. , 9. ], ## [3. , 5. , 2. ], ## [4. , 5. , 6. ], ## [8. , 8. , 1. ]]) 17.9 Scaling It is possible that some insignificant variable with larger range will be dominating the objective function. We can remove this problem by scaling down all the features to a same range. 17.9.1 Sample Data X=mydf.filter(like=&#39;x&#39;)[:5] X ## x1 x2 x3 ## 0 17 4 3 ## 1 5 2 10 ## 2 7 15 7 ## 3 1 14 13 ## 4 5 15 16 17.9.2 MinMax Scaler MinMaxScaler( feature_range(0,1), copy=True ) # default feature range (output result) from 0 to 1 # default return a copy of new array, copy=False will inplace original array Define Scaler Object scaler = MinMaxScaler() Transform Data scaler.fit_transform(X) ## array([[1. , 0.15384615, 0. ], ## [0.25 , 0. , 0.53846154], ## [0.375 , 1. , 0.30769231], ## [0. , 0.92307692, 0.76923077], ## [0.25 , 1. , 1. ]]) Scaler Attributes data_min_: minimum value of the feature (before scaling) data_max_: maximum value of the feature (before scaling) pd.DataFrame(list(zip(scaler.data_min_, scaler.data_max_)), columns=[&#39;data_min&#39;,&#39;data_max&#39;], index=X.columns) ## data_min data_max ## x1 1.0 17.0 ## x2 2.0 15.0 ## x3 3.0 16.0 17.9.3 Standard Scaler It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better with rescaled data, such as linear regression, logistic regression and linear discriminate analysis. StandardScaler(copy=True, with_mean=True, with_std=True) # copy=True : return a copy of data, instead of inplace # with_mean=True : centre all features by substracting with its mean # with_std=True : centre all features by dividing with its std Define Scaler Object scaler = StandardScaler() Transform Data scaler.fit_transform(X) ## array([[ 1.86338998, -1.0413152 , -1.49967571], ## [-0.372678 , -1.38842027, 0.04410811], ## [ 0. , 0.86776267, -0.61751353], ## [-1.11803399, 0.69421013, 0.70572975], ## [-0.372678 , 0.86776267, 1.36735138]]) Scaler Attributes After the data transformation step above, scaler will have the mean and variance information for each feature. pd.DataFrame(list(zip(scaler.mean_, scaler.var_)), columns=[&#39;mean&#39;,&#39;variance&#39;], index=X.columns) ## mean variance ## x1 7.0 28.80 ## x2 10.0 33.20 ## x3 9.8 20.56 17.10 Pipeline With any of the preceding examples, it can quickly become tedious to do the transformations by hand, especially if you wish to string together multiple steps. For example, we might want a processing pipeline that looks something like this: Impute missing values using the mean Transform features to quadratic Fit a linear regression make_pipeline takes list of functions as parameters. When calling fit() on a pipeline object, these functions will be performed in sequential with data flow from one function to another. make_pipeline ( function_1 (), function_2 (), function_3 () ) 17.10.1 Sample Data X ## x1 x2 x3 ## 0 17 4 3 ## 1 5 2 10 ## 2 7 15 7 ## 3 1 14 13 ## 4 5 15 16 y ## array([14, 16, -1, 8, -5]) 17.10.2 Create Pipeline my_pipe = make_pipeline ( SimpleImputer (strategy=&#39;mean&#39;), PolynomialFeatures (degree=2), LinearRegression () ) type(my_pipe) ## &lt;class &#39;sklearn.pipeline.Pipeline&#39;&gt; my_pipe #sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}Pipeline(steps=[(&#x27;simpleimputer&#x27;, SimpleImputer()), (&#x27;polynomialfeatures&#x27;, PolynomialFeatures()), (&#x27;linearregression&#x27;, LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;simpleimputer&#x27;, SimpleImputer()), (&#x27;polynomialfeatures&#x27;, PolynomialFeatures()), (&#x27;linearregression&#x27;, LinearRegression())])SimpleImputerSimpleImputer()PolynomialFeaturesPolynomialFeatures()LinearRegressionLinearRegression() 17.10.3 Executing Pipeline my_pipe.fit( X, y) # execute the pipeline #sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}Pipeline(steps=[(&#x27;simpleimputer&#x27;, SimpleImputer()), (&#x27;polynomialfeatures&#x27;, PolynomialFeatures()), (&#x27;linearregression&#x27;, LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[(&#x27;simpleimputer&#x27;, SimpleImputer()), (&#x27;polynomialfeatures&#x27;, PolynomialFeatures()), (&#x27;linearregression&#x27;, LinearRegression())])SimpleImputerSimpleImputer()PolynomialFeaturesPolynomialFeatures()LinearRegressionLinearRegression() print (y) ## [14 16 -1 8 -5] print (my_pipe.predict(X)) ## [14. 16. -1. 8. -5.] type(my_pipe) ## &lt;class &#39;sklearn.pipeline.Pipeline&#39;&gt; 17.11 Cross Validation 17.11.1 Load Data X,y = datasets.load_diabetes(return_X_y=True) 17.11.2 Choose An Cross Validator kf = KFold(n_splits=5) 17.11.3 Run Cross Validation Single Scorer Use default scorer of the estimator (if available) lasso = Lasso() cv_results1 = cross_validate(lasso, X,y,cv=kf, return_train_score=False) Multiple Scorer Specify the scorer http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter cv_results2 = cross_validate(lasso, X,y,cv=kf, scoring=(&quot;neg_mean_absolute_error&quot;,&quot;neg_mean_squared_error&quot;,&quot;r2&quot;), return_train_score=False) 17.11.4 The Result Result is a dictionary cv_results1.keys() ## dict_keys([&#39;fit_time&#39;, &#39;score_time&#39;, &#39;test_score&#39;]) cv_results2.keys() ## dict_keys([&#39;fit_time&#39;, &#39;score_time&#39;, &#39;test_neg_mean_absolute_error&#39;, &#39;test_neg_mean_squared_error&#39;, &#39;test_r2&#39;]) cv_results1 ## {&#39;fit_time&#39;: array([0., 0., 0., 0., 0.]), &#39;score_time&#39;: array([0. , 0. , 0. , 0.00703287, 0. ]), &#39;test_score&#39;: array([0.28349006, 0.35157952, 0.35338233, 0.33481253, 0.36453239])} cv_results2 ## {&#39;fit_time&#39;: array([0. , 0. , 0. , 0.00132012, 0. ]), &#39;score_time&#39;: array([0. , 0. , 0.01453161, 0.00165081, 0. ]), &#39;test_neg_mean_absolute_error&#39;: array([-50.09006473, -52.54118496, -55.02819607, -50.8112893 , ## -55.60479053]), &#39;test_neg_mean_squared_error&#39;: array([-3491.74208572, -4113.86049974, -4046.91135088, -3489.75176794, ## -4111.92674103]), &#39;test_r2&#39;: array([0.28349006, 0.35157952, 0.35338233, 0.33481253, 0.36453239])} "],["web-scrapping.html", "Chapter 18 Web Scrapping 18.1 requests 18.2 BeautifulSoup", " Chapter 18 Web Scrapping 18.1 requests 18.1.1 Creating A Session import requests from requests.adapters import HTTPAdapter from urllib3.util.retry import Retry import random _retries = Retry(connect=10,read=10,backoff_factor=1) # backoff is incremental interval in seconds between retries _timeout = (10,10) ## connect, read timeout in seconds rqs = requests.Session() rqs.mount( &#39;http://&#39; , HTTPAdapter(max_retries= _retries)) rqs.mount( &#39;https://&#39; , HTTPAdapter(max_retries= _retries)) link1 = &#39;https://www.yahoo.com&#39; link2 = &#39;http://mamamia777.com.au&#39; #user_agent = {&#39;User-Agent&#39;: random.choice(_USER_AGENTS)} #response1 = rqs.get(link1, timeout=_timeout) #response2 = rqs.get(link2, timeout=_timeout) #print (page1.status_code) 18.1.2 Rotating Broswer _USER_AGENTS = [ #Chrome &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36&#39;, &#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36&#39;, #Firefox &#39;Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&#39;, &#39;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko&#39;, &#39;Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko&#39;, &#39;Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko&#39;, &#39;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)&#39;, &#39;Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko&#39;, &#39;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko&#39;, &#39;Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)&#39;, &#39;Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)&#39;, &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)&#39;] 18.2 BeautifulSoup 18.2.1 Module Import from bs4 import BeautifulSoup 18.2.2 HTML Tag Parsing 18.2.2.1 Sample Data my_html = &#39;&#39;&#39; &lt;div id=&quot;my-id1&quot; class=&#39;title&#39;&gt; &lt;p&gt;This Is My Title&lt;/p&gt; &lt;div id=&quot;my-id2&quot; class=&#39;subtitle&#39; custom_attr=&#39;funny&#39;&gt; &lt;p&gt;This is Subtitle&lt;/p&gt; &lt;/div&gt; &lt;div id=&quot;my-id3&quot; class=&#39;title&#39;, custom_attr=&#39;funny&#39;&gt; &lt;p&gt;This is paragraph1&lt;/p&gt; &lt;p&gt;This is paragraph2&lt;/p&gt; &lt;h3&gt;This is paragraph3&lt;/h3&gt; &lt;/div&gt; &lt;/div&gt; &#39;&#39;&#39; soup = BeautifulSoup(my_html) 18.2.2.2 First Match ID Selector Everthing under the selected tag will be returned. soup.find(id=&#39;my-id1&#39;) ## &lt;div class=&quot;title&quot; id=&quot;my-id1&quot;&gt; ## &lt;p&gt;This Is My Title&lt;/p&gt; ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; ## &lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt; ## &lt;/div&gt; Class Selector soup.find(class_=&#39;subtitle&#39;) ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; Attribute Selector soup.find(custom_attr=&#39;funny&#39;) ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; soup.find( custom_attr=&#39;funny&#39;) ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; soup.find(&#39;div&#39;, custom_attr=&#39;funny&#39;) ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; 18.2.2.3 Find All Matches find_all soup = BeautifulSoup(my_html) multiple_result = soup.find_all(class_=&#39;title&#39;) print( &#39;Item 0: \\n&#39;, multiple_result[0], &#39;\\n\\nItem 1: \\n&#39;, multiple_result[1]) ## Item 0: ## &lt;div class=&quot;title&quot; id=&quot;my-id1&quot;&gt; ## &lt;p&gt;This Is My Title&lt;/p&gt; ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; ## &lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt; ## &lt;/div&gt; ## ## Item 1: ## &lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt; CSS Selector using select() Above can be achieved using css selector. It return an array of result (multiple matches). multiple_result = soup.select(&#39;.title&#39;) print( &#39;Item 0: \\n&#39;, multiple_result[0], &#39;\\n\\nItem 1: \\n&#39;, multiple_result[1]) ## Item 0: ## &lt;div class=&quot;title&quot; id=&quot;my-id1&quot;&gt; ## &lt;p&gt;This Is My Title&lt;/p&gt; ## &lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt; ## &lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt; ## &lt;/div&gt; ## ## Item 1: ## &lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt; More granular exmaple of css selector. soup.select(&#39;#my-id1 div.subtitle&#39;) ## [&lt;div class=&quot;subtitle&quot; custom_attr=&quot;funny&quot; id=&quot;my-id2&quot;&gt; ## &lt;p&gt;This is Subtitle&lt;/p&gt; ## &lt;/div&gt;] Using contains() soup.select(&quot;p:contains(&#39;This is paragraph&#39;)&quot;) ## [&lt;p&gt;This is paragraph1&lt;/p&gt;, &lt;p&gt;This is paragraph2&lt;/p&gt;] ## ## C:\\PROGRA~3\\Anaconda3\\lib\\site-packages\\soupsieve\\css_parser.py:876: FutureWarning: The pseudo class &#39;:contains&#39; is deprecated, &#39;:-soup-contains&#39; should be used moving forward. ## warnings.warn( Combining ID, Class and Custom Attribute in the selector soup.select(&quot;div#my-id3.title[custom_attr=&#39;funny&#39;]:contains(&#39;This is paragraph&#39;)&quot;) ## [&lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt;] 18.2.3 Meta Parsing my_meta = &#39;&#39;&#39; &lt;meta property=&quot;description&quot; content=&quot;KUALA LUMPUR: blah blah&quot; category=&quot;Malaysia&quot;&gt; &lt;meta property=&quot;publish-date&quot; content=&quot;2012-01-03&quot;&gt; &#39;&#39;&#39; soup = BeautifulSoup(my_meta) soup.find(&#39;meta&#39;, property=&#39;description&#39;)[&#39;content&#39;] ## &#39;KUALA LUMPUR: blah blah&#39; soup.find(&#39;meta&#39;, property=&#39;description&#39;)[&#39;category&#39;] ## &#39;Malaysia&#39; soup.find(&#39;meta&#39;, property=&#39;publish-date&#39;)[&#39;content&#39;] ## &#39;2012-01-03&#39; soup.find(&#39;meta&#39;, category=&#39;Malaysia&#39;)[&#39;property&#39;] ## &#39;description&#39; 18.2.4 Getting Content 18.2.4.1 Get Content get_text(strip=, separator=) Use strip=True to strip whitespace from the beginning and end of each bit of text Use `separator=‘’ to specify a string to be used to join the bits of text together It is recommended to use strip=True, separator='\\n' so that result from different operating system will be consistant soup = BeautifulSoup(my_html) elem = soup.find(id = &quot;my-id3&quot;) elem.get_text(strip=False) ## &#39;\\nThis is paragraph1\\nThis is paragraph2\\nThis is paragraph3\\n&#39; strip=True combine with separator will retain only the user readable text portion of each tag, with separator seperating them elem.get_text(strip=True, separator=&#39;\\n&#39;) ## &#39;This is paragraph1\\nThis is paragraph2\\nThis is paragraph3&#39; 18.2.4.2 Splitting Content It is useful to split using separator into list of string. elem = soup.find(id = &quot;my-id3&quot;) elem.get_text(strip=True, separator=&#39;\\n&#39;).split(&#39;\\n&#39;) ## [&#39;This is paragraph1&#39;, &#39;This is paragraph2&#39;, &#39;This is paragraph3&#39;] 18.2.5 Traversing 18.2.5.1 Get The Element elems = soup.select(&quot;div#my-id3.title[custom_attr=&#39;funny&#39;]:contains(&#39;This is paragraph&#39;)&quot;) elem = elems[0] elem ## &lt;div class=&quot;title&quot; custom_attr=&quot;funny&quot; id=&quot;my-id3&quot;&gt; ## &lt;p&gt;This is paragraph1&lt;/p&gt; ## &lt;p&gt;This is paragraph2&lt;/p&gt; ## &lt;h3&gt;This is paragraph3&lt;/h3&gt; ## &lt;/div&gt; 18.2.5.2 Traversing Children All Children In List findChildren() elem.findChildren() ## [&lt;p&gt;This is paragraph1&lt;/p&gt;, &lt;p&gt;This is paragraph2&lt;/p&gt;, &lt;h3&gt;This is paragraph3&lt;/h3&gt;] Next Children findNext() If the element has children, this will get the immediate child If the element has no children, this will find the next element in the hierechy first_child = elem.fin print( elem.findNext().get_text(strip=True), &#39;\\n&#39;, elem.findNext().findNext().get_text(strip=True), &#39;\\n&#39;) ## This is paragraph1 ## This is paragraph2 18.2.5.3 Traversing To Parent parent() elem_parent = elem.parent elem_parent.attrs ## {&#39;id&#39;: &#39;my-id1&#39;, &#39;class&#39;: [&#39;title&#39;]} 18.2.5.4 Get The Sibling findPreviousSibling() Sibling is element at the same level of hierachy elem_prev_sib = elem.findPreviousSibling() elem_prev_sib.attrs ## {&#39;id&#39;: &#39;my-id2&#39;, &#39;class&#39;: [&#39;subtitle&#39;], &#39;custom_attr&#39;: &#39;funny&#39;} "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
