
# pandas

```{r,setup, include=FALSE}
library(reticulate)
use_condaenv('base')    #conda_list() - to find out the name of conda environment
```

## Modules Import

```{python}
import pandas as pd

## Other Libraries
import numpy as np
import datetime as dt
from datetime import datetime
from datetime import date
```

```{python include=FALSE, results='hide'}
from IPython.core.display import display, HTML
display(HTML("<style>.container { width:75% !important; margin-left:350px; }</style>"))
pd.set_option( 'display.notebook_repr_html', False)  # render Series and DataFrame as text, not HTML
pd.set_option( 'display.max_column', 10)    # number of columns
pd.set_option( 'display.max_rows', 10)      # number of rows
pd.set_option( 'display.width', 90)         # number of characters per row
```


## Pandas Objects

### Types of Objects

- pandas.Timestamp
- pandas.Timedelta
- pandas.Period
- pandas.Interval
- pandas.DateTimeIndex
- pandas.DataFrame
- pandas.Series

### Series and DataFrame

|Type        | Dimension | Size      | Value   | Constructor
|:---------- |:----------|:----------|:--------|:----------------------------------------------------
|Series      | 1         | Immutable | Mutable | pandas.DataFrame( data, index, dtype, copy)  
|DataFrame   | 2         | Mutable   | Mutable | pandas.DataFrame( data, index, columns, dtype, copy)
|Panel       | 3         | Mutable   | Mutable | 

**data** can be ndarray, list, constants  
**index** must be unique and same length as data. Can be integer or string
**dtype** if none, it will be inferred  
**copy** copy data. Default false

## Class Method

### Creating Timestamp

Pandas **`to_datetime()`** can:  
- Convert list of dates to **DateTimeIndex**  
- Convert list of dates to **Series of Timestamps**  
- Convert single date into **Timestamp** Object
. Source can be **string, date, datetime object**

#### From List to `DateTimeIndex`

```{python}
dti = pd.to_datetime(['2011-01-03',             # from string
                       date(2018,4,13),         # from date
                       datetime(2018,3,1,7,30)] # from datetime
              )
print( dti,
      '\nObject Type:  ', type(dti),
      '\nObject dtype: ', dti.dtype,
      '\nElement Type: ', type(dti[1]))
```

#### From List to Series of Timestamps

```{python}
sdt = pd.to_datetime(pd.Series(['2011-01-03',      # from string
                                date(2018,4,13),        # from date
                                datetime(2018,3,1,7,30)]# from datetime
              ))
print(sdt,
      '\nObject Type:  ',type(sdt),
      '\nObject dtype: ', sdt.dtype,
      '\nElement Type: ',type(sdt[1]))
```

#### From Scalar to Timestamp

```{python}
print( pd.to_datetime('2011-01-03'), '\n',
       pd.to_datetime(date(2011,1,3)), '\n',
       pd.to_datetime(datetime(2011,1,3,5,30)), '\n',
       '\nElement Type: ', type(pd.to_datetime(datetime(2011,1,3,5,30))))
```

### Generate Timestamp Sequence

The function `date_range()` return **`DateTimeIndex`** object. Use `Series()` to convert into Series if desired.

#### Hourly

If start time not specified, default to 00:00:00.  
If start time specified, it will be honored on all subsequent Timestamp elements.  
Specify **start** and **end**, sequence will automatically distribute Timestamp according to **frequency**.  

```{python}
print(
  pd.date_range('2018-01-01', periods=3, freq='H'),
  pd.date_range(datetime(2018,1,1,12,30), periods=3, freq='H'),
  pd.date_range(start='2018-01-03-1230', end='2018-01-03-18:30', freq='H'))
```

#### Daily

When the **frequency is Day and time is not specified**, output is date distributed.  
When time is specified, output will honor the time.

```{python}
print(
  pd.date_range(date(2018,1,2), periods=3, freq='D'),
  pd.date_range('2018-01-01-1230', periods=4, freq='D'))
```

#### First Day Of Month

Use `freq=MS`, M stands for montly, S stand for Start. If the **day** specified, the sequence start from first day of following month.

```{python, jupyter_meta = list(scrolled = TRUE)}
print(
  pd.date_range('2018-01', periods=4, freq='MS'),
  pd.date_range('2018-01-09', periods=4, freq='MS'),
  pd.date_range('2018-01-09 12:30:00', periods=4, freq='MS') )
```
#### Last Day of Month

Sequence always starts from the end of the specified month.

```{python}
print(
  pd.date_range('2018-01', periods=4, freq='M'),
  pd.date_range('2018-01-09', periods=4, freq='M'),
  pd.date_range('2018-01-09 12:30:00', periods=4, freq='M'))
```

### Frequency Table (crosstab)

crosstab returns **Dataframe** Object
```
crosstab( index = <SeriesObj>, columns = <new_colName> )                # one dimension table
crosstab( index = <SeriesObj>, columns = <SeriesObj> )                  # two dimension table
crosstab( index = <SeriesObj>, columns = [<SeriesObj1>, <SeriesObj2>] ) # multi dimension table   
crosstab( index = <SeriesObj>, columns = <SeriesObj>, margines=True )   # add column and row margins
```

#### Sample Data

```{python}
n = 200
comp = ['C' + i for i in np.random.randint( 1,4, size  = n).astype(str)] # 3x Company
dept = ['D' + i for i in np.random.randint( 1,6, size  = n).astype(str)] # 5x Department
grp =  ['G' + i for i in np.random.randint( 1,3, size  = n).astype(str)] # 2x Groups
value1 = np.random.normal( loc=50 , scale=5 , size = n)
value2 = np.random.normal( loc=20 , scale=3 , size = n)
value3 = np.random.normal( loc=5 , scale=30 , size = n)

mydf = pd.DataFrame({
    'comp':comp, 
    'dept':dept, 
    'grp': grp,
    'value1':value1, 
    'value2':value2,
    'value3':value3 })
mydf.head()
```

#### One DimensionTable

```{python}
## Frequency Countn For Company, Department
print(
  pd.crosstab(index=mydf.comp, columns='counter'),'\n\n',
  pd.crosstab(index=mydf.dept, columns='counter'))
```

#### Two Dimension Table

```{python}
pd.crosstab(index=mydf.comp, columns=mydf.dept)
```

#### Higher Dimension Table

Crosstab header is **multi-levels index** when more than one column specified.

```{python}
tb = pd.crosstab(index=mydf.comp, columns=[mydf.dept, mydf.grp])
print( tb, '\n\n',
       tb.columns )
```

Select **sub-dataframe** using multi-level referencing.

```{python}
print( 'Under D2:\n', tb['D2'], '\n\n',
       'Under D2-G2:\n',tb['D2','G1'])
```

#### Getting Margin
Extend the crosstab with 'margin=True' to have sum of rows/columns, presented in **new column/row named 'All'**.

```{python}
tb = pd.crosstab(index=mydf.dept, columns=mydf.grp, margins=True)
tb
```

```{python}
print(
  'Row Sums:     \n', tb.loc[:,'All'],
  '\n\nColumn Sums:\n', tb.loc['All'])
```

#### Getting Proportion
Use matrix operation divide each row with its respective column sum.

```{python}
tb/tb.loc['All']
```

### Concatination

#### Sample Data

```{python}
s1 = pd.Series(['A1','A2','A3','A4'])
s2 = pd.Series(['B1','B2','B3','B4'], name='B')
s3 = pd.Series(['C1','C2','C3','C4'], name='C')
```

#### Column-Wise

**Combining Multiple Series Into A New DataFrame**  

- Series name will become column name in DataFrame.
- If Series has no name, default column names in DataFrame will be 0,1,2,3
- `axis=1` means column-wise

```{python}
pd.concat([s1,s2,s3, None], axis=1)  ## observed that None is ignored
```

**Add Multiple Series Into An Existing DataFrame**  

- No change to original DataFrame column name  
- Added columns from series will have 0,1,2,3,.. column name

```{python}
df = pd.DataFrame({ 'A': s1, 'B': s2})
df
pd.concat([df,s3,s2,s1, None],axis=1)
```

#### Row-Wise

### External Data

#### `html_table` Parser

This method require **html5lib** library.  
- Read the web page, create a list: which contain one or more dataframes that maps to each html table found  
- Scrap all detectable html tables  
- Auto detect column header  
- Auto create index using number starting from 0  

```
read_html(url)  # return list of dataframe(s) that maps to web table(s) structure
```

```{python}
df_list = pd.read_html('https://www.malaysiastock.biz/Listed-Companies.aspx?type=S&s1=18')  ## read all tables
df = df_list[6]  ## get the specific table

print ('Total Table(s) Found : ', len(df_list), '\n',
       'First Table Found:      ',df)
```

#### CSV Writing

**Syntax**  

```
DataFrame.to_csv(
  path_or_buf=None,   ## if not provided, result is returned as string
  sep=', ', 
  na_rep='', 
  float_format=None, 
  columns=None,       ## list of columns name to write, if not provided, all columns are written
  header=True,        ## write out column names
  index=True,         ## write row label
  index_label=None, 
  mode='w', 
  encoding=None,      ## if not provided, default to 'utf-8'
  quoting=None, quotechar='"', 
  line_terminator=None, 
  chunksize=None, 
  date_format=None, 
  doublequote=True, 
  escapechar=None, 
  decimal='.')

```

Example below shows column value containing different special character. Note that pandas handles these very well by default.

```{python}
mydf = pd.DataFrame({'Id':[10,20,30,40], 
                     'Name':  ['Aaa','Bbb','Ccc','Ddd'],
                     'Funny': ["world's most \clever", 
                     "Bloody, damn, good", 
                     "many\nmany\nline", 
                     'Quoting "is" tough']})
mydf.set_index('Id', inplace=True)
mydf.to_csv('data/csv_test.csv', index=True)
mydf
```

**This is the file saved**

```{r}
system('more data\\csv_test.csv')
```

**All content retained when reading back by Pandas**

```{python}
pd.read_csv('data/csv_test.csv', index_col='Id')
```

#### CSV Reading

**Syntax**  

```
pandas.read_csv( 
    'url or filePath',                     # path to file or url 
    encoding    = 'utf_8',                 # optional: default is 'utf_8'
    index_col   = ['colName1', ...],       # optional: specify one or more index column
    parse_dates = ['dateCol1', ...],       # optional: specify multiple string column to convert to date
    na_values   = ['.','na','NA','N/A'],   # optional: values that is considered NA
    names       = ['newColName1', ... ],   # optional: overwrite column names
    thousands   = '.',                     # optional: thousand seperator symbol
    nrows       = n,                       # optional: load only first n rows
    skiprows    = 0,                       # optional: don't load first n rows
    parse_dates = False,                   # List of date column names
    infer_datetime_format = False          # automatically parse dates
)
```
Refer to full codec [Python Codec](https://docs.python.org/3/library/codecs.html#standard-encodings).

**Default Import**  

- index is sequence of integer 0,1,2...   
- only two data types detection; **number (float64/int64) and string (object)**  
- **date is not parsed**, hence stayed as string  

```{python}
goo = pd.read_csv('data/goog.csv', encoding='utf_8')
print(goo.head(), '\n\n',
      goo.info())
```

**Specify Data Types**  

- To customize the data type, use **`dtype`** parameter with a **dict** of definition.  

```{python}
d_types = {'Volume': str}
pd.read_csv('data/goog.csv', dtype=d_types).info()
```

**Parse Datetime**

You can specify multiple date-alike column for parsing

```{python}
pd.read_csv('data/goog.csv', parse_dates=['Date']).info()
```

**Parse Datetime, Then Set as Index**  
- Specify names of date column in `parse_dates=`  
- When date is set as index, the type is **`DateTimeIndex`**  

```{python}
goo3 = pd.read_csv('data/goog.csv',index_col='Date', parse_dates=['Date'])
goo3.info()
```

### Inspection

#### Structure `info`

**info()** is a function that print information to screen. It doesn't return any object

```
dataframe.info()  # display columns and number of rows (that has no missing data)
```

```{python, jupyter_meta = list(hidden = TRUE)}
goo.info()
```

#### `head`

```{python, jupyter_meta = list(hidden = TRUE)}
goo.head()
```

## Class: Timestamp

This is an enhanced version to datetime standard library.  
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html#pandas.Timestamp

### Constructor

#### From Number

```{python}
print( pd.Timestamp(year=2017, month=1, day=1),'\n',  #date-like numbers
       pd.Timestamp(2017,1,1), '\n',                  # date-like numbers
       pd.Timestamp(2017,12,11,5,45),'\n',            # datetime-like numbers
       pd.Timestamp(2017,12,11,5,45,55,999),'\n',     # + microseconds
       pd.Timestamp(2017,12,11,5,45,55,999,8),'\n',   # + nanoseconds
       type(pd.Timestamp(2017,12,11,5,45,55,999,8)),'\n')
```

#### From String

Observe that pandas support many string input format  

**Year Month Day**, default has no timezone
```{python}
print( pd.Timestamp('2017-12-11'),'\n',   # date-like string: year-month-day
       pd.Timestamp('2017 12 11'),'\n',   # date-like string: year-month-day
       pd.Timestamp('2017 Dec 11'),'\n',  # date-like string: year-month-day
       pd.Timestamp('Dec 11, 2017'))      # date-like string: year-month-day
```

**YMD Hour Minute Second Ms**

```{python}
print( pd.Timestamp('2017-12-11 0545'),'\n',     ## hour minute
       pd.Timestamp('2017-12-11-05:45'),'\n',
       pd.Timestamp('2017-12-11T0545'),'\n',
       pd.Timestamp('2017-12-11 054533'),'\n',   ## hour minute seconds
       pd.Timestamp('2017-12-11 05:45:33'))
```

**With Timezone** can be included in various ways.

```{python}
print( pd.Timestamp('2017-01-01T0545Z'),'\n',  # GMT 
       pd.Timestamp('2017-01-01T0545+9'),'\n', # GMT+9
       pd.Timestamp('2017-01-01T0545+0800'),'\n',   # GMT+0800
       pd.Timestamp('2017-01-01 0545', tz='Asia/Singapore'),'\n')
```

#### From Standard Library ```datetime``` and ```date``` Object

```{python}
print( pd.Timestamp(date(2017,3,5)),'\n',           # from date
       pd.Timestamp(datetime(2017,3,5,4,30)),'\n',  # from datetime
       pd.Timestamp(datetime(2017,3,5,4,30), tz='Asia/Kuala_Lumpur')) # from datetime, + tz
```

### Attributes

We can tell many things about a Timestamp object.

```{python}
ts = pd.Timestamp('2017-01-01T054533+0800') # GMT+0800
print( ts.month, '\n',
       ts.day, '\n',
       ts.year, '\n',
       ts.hour, '\n',
       ts.minute, '\n',
       ts.second, '\n',
       ts.microsecond, '\n',
       ts.nanosecond, '\n',
       ts.tz, '\n',
       ts.daysinmonth,'\n',
       ts.dayofyear, '\n',
       ts.is_leap_year, '\n',
       ts.is_month_end, '\n',
       ts.is_month_start, '\n',
       ts.dayofweek)
```

Note that timezone (tz) is a **pytz object**.

```{python}
ts1 = pd.Timestamp(datetime(2017,3,5,4,30), tz='Asia/Kuala_Lumpur')   # from datetime, + tz
ts2 = pd.Timestamp('2017-01-01T054533+0800') # GMT+0800
ts3 = pd.Timestamp('2017-01-01T0545')

print( ts1.tz, 'Type:', type(ts1.tz), '\n',
       ts2.tz, 'Type:', type(ts2.tz), '\n',
       ts3.tz, 'Type:', type(ts3.tz)  )
```

### Instance Methods

#### Atribute-like Methods

```{python}
ts = pd.Timestamp(2017,1,1)
print( ' Weekday:    ', ts.weekday(), '\n',
       'ISO Weekday:',  ts.isoweekday(), '\n',
       'Day Name:   ',  ts.day_name(), '\n',
       'ISO Calendar:',  ts.isocalendar()
       )
```

#### Timezones

**Adding Timezones and Clock Shifting**  

- `tz_localize` will add the timezone, however will not shift the clock.  
- Once a timestamp had gotten a timezone, you can easily shift the clock to another timezone using `tz_convert()`  

```{python}
ts = pd.Timestamp(2017,1,10,10,34)        ## No timezone
ts1 = ts.tz_localize('Asia/Kuala_Lumpur')  ## Add timezone
ts2 = ts1.tz_convert('UTC')                 ## Convert timezone
print(' Origininal Timestamp           :', ts,  '\n',
      'Loacalized Timestamp (added TZ):', ts1, '\n',
      'Converted Timestamp (shifted)  :',ts2)
```

**Removing Timezone**

Just apply **None with `tz_localize`** to remove TZ infomration.

```{python}
ts = pd.Timestamp(2017,1,10,10,34)        ## No timezone
ts = ts.tz_localize('Asia/Kuala_Lumpur')  ## Add timezone
ts = ts.tz_localize(None)                 ## Convert timezone
ts
```

#### Formatting 

**`strftime`**  

Use **```strftime()```** to customize string format. For complete directive, see below: https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior

```{python}
ts = pd.Timestamp(2017,1,10,10,34)        ## No timezone
ts = ts.tz_localize('Asia/Kuala_Lumpur')  ## Add timezone
ts.strftime("%m/%d")
```

**`isoformat`**  

Use **```isoformat()```** to format ISO string (**without timezone**)

```{python}
ts = pd.Timestamp(2017,1,10,10,34)        
ts1 = ts.tz_localize('Asia/Kuala_Lumpur') 
print( ' ISO Format without TZ:', ts.isoformat(), '\n',
       'ISO Format with TZ   :', ts1.isoformat())
```

#### Type Conversion

**Convert To `datetime.datetime/date`**

Use `to_pydatetime()` to convert into standard library **`datetime.datetime`**.  From the 'datetime' object, apply `date()` to get **`datetime.date`**

```{python}
ts = pd.Timestamp(2017,1,10,7,30,52)
print(
  'Datetime:',  ts.to_pydatetime(), '\n',
  'Date Only:', ts.to_pydatetime().date())
```

**Convert To `numpy.datetime64`**

Use `to_datetime64()` to convert into ```numpy.datetime64```

```{python}
ts = pd.Timestamp(2017,1,10,7,30,52)
ts.to_datetime64()
```


#### `ceil`

```{python}
print( ts.ceil(freq='D') ) # ceiling to day
```

#### Updating

`replace()`

```{python}
ts.replace(year=2000, month=1,day=1)
```

## Class: DateTimeIndex

### Creating 

Refer to Pandas class method above.

### Instance Method

#### Data Type Conversion

**Convert To datetime.datetime**  
Use **```to_pydatetime```** to convert into python standard **datetime.datetime** object

```{python}
print('Converted to List:', dti.to_pydatetime(), '\n\n',
      'Converted Type:',    type(dti.to_pydatetime()))
```

#### Structure Conversion

**Convert To Series: `to_series`**  
This creates a Series where **index and data** with the same value

```{python}
#dti = pd.date_range('2018-02', periods=4, freq='M')
dti.to_series()
```

**Convert To DataFrame: `to_frame()`**  
This convert to **single column DataFrame** with index as the same value

```{python}
dti.to_frame()
```

### Attributes

**All Timestamp Attributes** can be used upon DateTimeIndex.

```{python}
print( dti.weekday, '\n',
       dti.month, '\n',
       dti.daysinmonth)
```

## class: Series

Series allows different data types (object class) as its element

```
pandas.Series(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False)
- data array-like, iterable, dict or scalar
- If dtype not specified, it will infer from data.
```

### Constructor

#### Empty Series

Passing no data to constructor will result in empty series. By default, empty series dtype is float.

```{python}
s = pd.Series(dtype='object')
print (s, '\n',
       type(s))
```

#### From Scalar

If data is a scalar value, an **index must be provided**. The **value will be repeated** to match the length of index

```{python}
pd.Series( 99, index = ['a','b','c','d'])
```

#### From array-like

**From list**

```{python}
pd.Series(['a','b','c','d','e'])           # from Python list
```


**From numpy.array**  
If index is not specified, default to 0 and continue incrementally

```{python}
pd.Series(np.array(['a','b','c','d','e']))
```

**From DateTimeIndex**

```{python}
pd.Series(pd.date_range('2011-1-1','2011-1-3'))
```


#### From Dictionary
The **dictionary key** will be the index. Order is **not sorted**.

```{python}
pd.Series({'a' : 0., 'c' : 5., 'b' : 2.})
```

If **index sequence** is specifeid, then Series will forllow the index order  
Objerve that **missing data** (index without value) will be marked as NaN

```{python}
pd.Series({'a' : 0., 'c' : 1., 'b' : 2.},index = ['a','b','c','d'])
```

#### Specify Index

```{python}
pd.Series(['a','b','c','d','e'], index=[10,20,30,40,50])
```

#### Mix Element Types

dType will be **'object'** when there were mixture of classes

```{python}
ser = pd.Series(['a',1,2,3])
print('Object Type :  ', type(ser),'\n',
      'Object dType:  ', ser.dtype,'\n',
      'Element 1 Type: ',type(ser[0]),'\n',
      'Elmeent 2 Type: ',type(ser[1]))
```

#### Specify Data Types
By default, dtype is **inferred** from data.

```{python}
ser1 = pd.Series([1,2,3])
ser2 = pd.Series([1,2,3], dtype="int8")
ser3 = pd.Series([1,2,3], dtype="object")

print(' Inferred:        ',ser1.dtype, '\n',
      'Specified int8:  ',ser2.dtype, '\n',
      'Specified object:',ser3.dtype)
```

### Accessing Series

```
series     ( single/list/range_of_row_label/number ) # can cause confusion
series.loc ( single/list/range_of_row_label )
series.iloc( single/list/range_of_row_number )
```

#### Sample Data

```{python}
s = pd.Series([1,2,3,4,5],index=['a','b','c','d','e']) 
s
```

#### by Row Number(s)

**Single Item**. 
Notice that inputing a number and list of number give different result.

```{python}
print( 'Referencing by number:',s.iloc[1],'\n\n',
       '\nReferencing by list of number:\n',s.iloc[[1]])
```


**Multiple Items**

```{python}
s.iloc[[1,3]] 
```


**Range (First 3)**

```{python}
s.iloc[:3]
```


**Range (Last 3)**

```{python, jupyter_meta = list(scrolled = TRUE)}
s.iloc[-3:]
```

**Range (in between)**

```{python}
s.iloc[2:3]
```

#### by Index(es)

**Single Label**. Notice the difference referencing input: single index and list of index.  
**Warning**: if index is invalid, this will result in error.

```{python}
print( s.loc['c'], '\n',
       s[['c']])
```

**Multiple Labels**

If index is not found, it will return **NaN**

```{python, error=TRUE}
s.loc[['k','c']]
```

** Range of Labels **

```{python}
s.loc['b':'d']
```

#### Filtering

Use **logical array** to filter

```{python}
s = pd.Series(range(1,8))
s[s<5]
```

Use **where**  
The where method is an application of the if-then idiom. For each element in the calling Series, if `cond` is True the element is used; otherwise `other` is used.

```
.where(cond, other=nan, inplace=False)
```

```{python}
print(s.where(s<4),'\n\n',
      s.where(s<4,other=None) )
```

### Updating Series

#### by Row Number(s)

```{python}
s = pd.Series(range(1,7), index=['a','b','c','d','e','f'])
s[2] = 999
s[[3,4]] = 888,777
s
```

#### by Index(es)

```{python}
s = pd.Series(range(1,7), index=['a','b','c','d','e','f'])
s['e'] = 888
s[['c','d']] = 777,888
s
```

### Series Attributes

#### The Data

```{python}
s = pd.Series([1,2,3,4,5],index=['a','b','c','d','e'],name='SuperHero') 
s
```

#### The Attributes

```{python}
print( ' Series Index:    ',s.index, '\n',
       'Series dType:    ', s.dtype, '\n',
       'Series Size:     ', s.size, '\n',
       'Series Shape:    ', s.shape, '\n',
       'Series Dimension:', s.ndim)
```

### Instance Methods

#### Index Manipulation

**`.rename_axis()`**

```{python}
s.rename_axis('haribulan')
```

**`.reset_index()`**

Resetting index will:  
- Convert index to a normal column, with column named as **'index'**  
- Index renumbered to 1,2,3  
- Return **DataFrame** (became two columns)

```{python}
s.reset_index()
```

#### Structure Conversion

- A series structure contain **`value`** (in numpy array), its **`dtype`** (data type of the numpy array).    
- Use **`values`** to retrieve into ```numpy.ndarray``. Use **`dtype`** to understand the data type.  

```{python}
s = pd.Series([1,2,3,4,5])
print(' Series value:      ', s.values, '\n', 
      'Series value type: ',  type(s.values), '\n',
      'Series dtype:      ',  s.dtype)
```

Convert To List using **`.tolist()`**

```{python}
pd.Series.tolist(s)
```

#### DataType Conversion

Use **```astype()```** to convert to another numpy supproted datatypes, results in a new Series.  
**Warning**: casting to incompatible type will result in **error**

```{python}
s.astype('int8')
```

### Series Operators

The result of applying operator (arithmetic or logic) to Series object **returns a new Series object**

#### Arithmetic Operator

```{python}
s1 = pd.Series( [100,200,300,400,500] )
s2 = pd.Series( [10, 20, 30, 40, 50] )
```

**Apply To One Series Object**

```{python}
s1 - 100
```

**Apply To Two Series Objects**

```{python}
s1 - s2
```

#### Logic Operator

- Apply logic operator to a Series return a **new Series** of boolean result  
- This can be used for **Series or DataFrame filtering**

```{python}
bs = pd.Series(range(0,10))
bs>3
```

```{python}
~((bs>3) & (bs<8) | (bs>7))
```

### Series `.str` Accesor

If the underlying data is **str** type, then pandas exposed various properties and methos through **`str` accessor**. 

```
SeriesObj.str.operatorFunction()
``` 

**Available Functions**

Nearly all Python's built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas str methods that mirror Python string methods:  

len()	lower()	translate()	islower()
ljust()	upper()	startswith()	isupper()
rjust()	find()	endswith()	isnumeric()
center()	rfind()	isalnum()	isdecimal()
zfill()	index()	isalpha()	split()
strip()	rindex()	isdigit()	rsplit()
rstrip()	capitalize()	isspace()	partition()
lstrip()	swapcase()	istitle()	rpartition()

#### Regex Extractor

Extract capture **groups** in the regex pattern, by default in DataFrame (`expand=True`).

```
Series.str.extract(self, pat, flags=0, expand=True)
- expand=True: if result is single column, make it a Series instead of Dataframe.
```

```{python}
s = pd.Series(['a1', 'b2', 'c3'])
print( 
  ' Extracted Dataframe:\n', s.str.extract(r'([ab])(\d)'),'\n\n',
  'Extracted Dataframe witn Names:\n', s.str.extract(r'(?P<Letter>[ab])(\d)'))
```

Below ouptut single columne, use **`expand=False`** to make the result a **Series**, instead of DataFrame.

```{python}
r = s.str.extract(r'[ab](\d)', expand=False)
print( r, '\n\n', type(r) )
```
#### Character Extractor

```{python}
monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',
                   'Eric Idle', 'Terry Jones', 'Michael Palin'])
monte
```

**```startwith```**

```{python}
monte.str.startswith('T')
```

**```Slicing```**

```{python}
monte.str[0:3]
```

#### Splitting

Split strings around given separator/delimiter in either string or regex.

```
Series.str.split(self, pat=None, n=-1, expand=False)
- pat: can be string or regex
```

```{python}
s = pd.Series(['a_b_c', 'c_d_e', np.nan, 'f_g_h_i_j'])
s
```

**```str.split()```** by default, split will split each item into **array**

```{python}
s.str.split('_')
```

**```expand=True```** will return a **dataframe** instead of series. By default, expand split into all possible columns.

```{python}
print( s.str.split('_', expand=True) )
```

It is possible to limit the number of columns splitted

```{python}
print( s.str.split('_', expand=True, n=1) )
```

**```str.rsplit()```**


**```rsplit```** stands for **reverse split**, it works the same way, except it is reversed

```{python}
print( s.str.rsplit('_', expand=True, n=1) )
```


#### Case Conversion

```
SeriesObj.str.upper()
SeriesObj.str.lower()
SeriesObj.str.capitalize()
```

```{python}
s = pd.Series(['A', 'B', 'C', 'aAba', 'bBaca', np.nan, 'cCABA', 'dog', 'cat'])
print( s.str.upper(), '\n',
       s.str.capitalize())
```

#### Number of Characters

```{python}
s.str.len()
```

#### String Indexing

This return specified character from each item.

```{python}
s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan,'CABA', 'dog', 'cat'])
s.str[0].values    # first char
s.str[0:2].values  # first and second char
```

#### Series Substring Extraction

**Sample Data**

```{python}
s = pd.Series(['a1', 'b2', 'c3'])
s
```


**Extract absed on regex matching**  
... to improve ...

```{python}
type(s.str.extract('([ab])(\d)', expand=False))
```


### Series  `.dt` Accessor 

If the underlying data is **datetime64** type, then pandas exposed various properties and methos through **```dt``` accessor**. 


#### Sample Data

```{python}
s = pd.Series([
    datetime(2000,1,1,0,0,0),
    datetime(1999,12,15,12,34,55),
    datetime(2020,3,8,5,7,12),
    datetime(2018,1,1,0,0,0),
    datetime(2003,3,4,5,6,7)
])
s
```


#### Convert To 
**datetime.datetime**  
Use **```to_pydatetime()```** to convert into **```numpy.array```** of standard library **```datetime.datetime```**  

```{python}
pdt  = s.dt.to_pydatetime()
print( type(pdt) )
pdt
```


**datetime.date**  
Use **```dt.date```** to convert into **```pandas.Series```** of standard library **```datetime.date```**   
Is it possible to have a pandas.Series of datetime.datetime ? No, because Pandas want it as its own Timestamp.

```{python}
sdt = s.dt.date
print( type(sdt[1] ))
print( type(sdt))
sdt
```


#### Timestamp Attributes
A Series::DateTime object support below properties:  
- date  
- month  
- day  
- year  
- dayofweek  
- dayofyear  
- weekday  
- weekday_name  
- quarter  
- daysinmonth  
- hour
- minute

Full list below:  
https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties

```{python}
s.dt.date
```

```{python}
s.dt.month
```

```{python}
s.dt.dayofweek
```

```{python}
s.dt.weekday
```

```{python}
s.dt.quarter
```

```{python}
s.dt.daysinmonth
```

```{python}
s.dt.time   # extract time as time Object
```

```{python}
s.dt.hour  # extract hour as integer
```

```{python}
s.dt.minute # extract minute as integer
```

## Class: DataFrame

### Constructor (Creation)

- pd.DataFrame() constructor creates DataFrame object.
- When index (row label) and column names are not specified (or cannot be induced from data source), it will default to 0,1,2,3...
- Specify Row Label and Column Header with **`columns`** and **`index`** parameters in constructor.
- When **`columns`** is specified AND data source has keys that can be used as column names, pandas will select those column names and order specified in **`columns`** parameter.
- **index** and **row label** are used interchangeably in this book

#### Empty DataFrame

By default, An empty dataframe contain no columns and index.

```{python, results='hold'}
empty_df1 = pd.DataFrame()
empty_df2 = pd.DataFrame()

empty_df1
id(empty_df1)
id(empty_df2)
```
Even though it is empty, you can still initialize it with Index Columns.

```{python, results='hold'}
## empty dataframe with columns
empty_df = pd.DataFrame(columns=['A','B','C'])
empty_df  ## index is empty

## empty dataframe with columns and index
empty_df = pd.DataFrame(columns=['A','B','C'], index=[1,2,3])
empty_df

```
When initializing multiple empty DataFrame at once (in one line), all the empty DataFrame refers to **same memory location**. Meaning they contain similar data.

```{python, results='hold'}
empty_df1 = empty_df2 = pd.DataFrame()
id(empty_df1)
id(empty_df2)
```
#### From Row Oriented Data (List of Lists)

```
DataFrame( [row_list1, row_list2, row_list3] )
DataFrame( [row_list1, row_list2, row_list3], column = columnName_list )
DataFrame( [row_list1, row_list2, row_list3], index  = row_label_list )
```

- Create DataFrame object from **List of Lists**
- Use **`columns'** to select the columns and preferred order.

```{python}
data = [  [101, 'Alice',  40000, 2017],
          [102, 'Bob',    24000, 2017], 
          [103, 'Charles',31000, 2017]]

## Initialize with default row label and column names  
pd.DataFrame(data)
```

```{python}
## Initialize with column names and row index
col_names  =  ['empID','name','salary','year']
row_labels =  ['r1','r2','r3']
pd.DataFrame(data, columns=col_names, index=row_labels)
```


#### From Record Oriented Data (List of Dict)

```
DataFrame( [dict1, dict2, dict3] )
DataFrame( [dict1, dict2, dict3], column = column_list )
DataFrame( [dict1, dict2, dict3], index  = row_label_list )
```

- Create DataFrame object from List of Dicts.
- By default,  **Column Name will follow Dictionary Key**, unless manually specified.
- Certain dict object may have missing keys, these are assumed as NaN by pandas.
- Use **`columns'** to select the columns and preferred order.

```{python}
data = [
  {"name":"Yong", "id":1,"zkey":101},
  {"name":"Gan",  "id":2           }]  ## missing zkey in this record

## Default Index and include all detected columns
pd.DataFrame(data)
```

```{python}
## specify Index, must be same length of DataFrame
pd.DataFrame(data, index=['row1','row2'])
```

```{python}
## Filter Which Columns To Include, in the prefered order
pd.DataFrame(data, columns=['zkey','name'])
```

#### From Column Oriented Data (Dict of Lists)

```
DataFrame(  data = {'column1': list1,
                    'column2': list2,
                    'column3': list3 } , 
            index    = row_label_list, 
            columns  = column_list)
```
- Create DataFrame object from Dict of Lists.
- Constructor will arrange the columns according to the **dict keys order** by default, unless **`columns`** is specified. 
- Use **`columns'** to select the columns in preferred order. NaN column is returned if not exist.

```{python}
## columns arrangement default to dict's keys
data = {'empID':  [100,      101,    102,      103,     104],
        'year1':   [2017,     2017,   2017,      2018,    2018],
        'year2':   [2027,     2027,   2027,      2028,    2028],
        'salary': [40000,    24000,  31000,     20000,   30000],
        'name':   ['Alice', 'Bob',  'Charles', 'David', 'Eric']}

## Default Index and Include All Columns
pd.DataFrame(data)
```

```{python}
## Select What Columns To Show (and its order) Specify row label.
pd.DataFrame(data, columns=['name','salary','year1','year2','not_exist'], index=data.get('empID'))
```

### Operators

#### Sample Data

- Two dataframes are created, each with 3 columns and 3 rows. 
- However, there are only two **matching column and row** names. 
- We shall notice that the operator will perform cell-wise, **honoring the row/column name**.

```{python, results = 'hold'}
df1 = pd.DataFrame(data=
  {'idx': ['row1','row2','row3'],
   'x': [10, 20, 30],
   'y': [1,2,3],
   'z': [0.1, 0.2, 0.3]}).set_index('idx')
   
df2 = pd.DataFrame(data=
  {'idx': ['row1','row2','row4'],
   'x': [13, 23, 33],
   'z': [0.1, 0.2, 0.3],
   'k': [11,21,31]}).set_index('idx')
```

#### Addition

**Adding Two DataFrame**

- When Using `+` operator, non-matching row/column names will result in **NA**. 
- **`.add()`**, has the same result as **`+`** by default.
- However, `add` supports **`fill_value=`** parameter. When specified, ONE-SIDED none matching cells is assumed to have value specified in **`fill_value`**. Only BOTH-SIDED non matching cells will results in NaN. 

```{python, results='hold'}
df1 + df2
df1.add(df2)
df1.add(df2, fill_value=1)
```

**Adding Series To DataFrame**

- Specify the **appropriate `axis`** depending on the orientation of the series data. 
- Column and Row names are respected in this operation. 
- However, `fill_value` is **not applicable** is not available on Series.  
- Columns or rows in Series that are non-matching in DataFrame, they will be created as the result. This behavior is similar to adding two DataFrames.

```{python, results='hold'}
## Series to add into Rows
s3 = pd.Series([1,1,1], index=['row1','row2','row4'])

## Series to add into Columns
s4 = pd.Series([3,3,3], index=['x','y','s'])

print('Original DataFrame:\n',df1,'\n\n',
      'Add Series as Rows: \n', df1.add(s3, axis=0), '\n\n',
      'Add Series as Columns: \n', df1.add(s4, axis=1))
```
#### Substraction

Refer to `.add()` above for `fill_value` explanation.

```{python, results='hold'}
df2 - df1
df2.sub(df1,fill_value=1000)
```

#### Operator `&`

- Non matching columns returns NaN.
- Matching columns with both True value returns True
- Matching columns with both True and False returns False

```{python, results='hold'}
df1>5
df2<20
(df1>5) & (df2<20)
```

### Attributes

```{python}
df = pd.DataFrame(data)
df
```

```{python, results='hold'}
df.shape          ## tuple (rows, columns)
df.index          ## default index is RangeIndex 
df.index.values   ## array of integer
df.columns        ## array of string
df.columns.values ## array of string
df.values         ## array of list
```

### Index Manipulation

#### Sample Data

```{python}
df
```
#### Convert Column To Index

```
set_index('column_name', inplace=False)
```

- Specified column will turn into Index
- **inplace=True** means don't create a new dataframe. Modify existing DataFrame.
- **inplace=False** means return a new DataFrame

```{python, results='hold'}
df.index   ## 0,1,2,3,...
df.set_index('empID',inplace=True)
df.index   ## 100,101,102,etc
df
```

#### Convert Index To Column

- Resetting index will re-sequence the index as 0,1,2 etc.
- Old index column will be converted back as normal column, its name retained as column name.
- Operation support **inplace** option.

```{python}
df.reset_index(inplace=True)
df
```

#### Updating Index ( .index= )

- This simply **replace** all index with the new values.
- Number of elements in the new index must match length of DataFrame, otherwise **error**.
- Although same label are **allowed to repeat**, it will be deprecated in future.
- This operation not reversible.

```{python}
df.index = [201, 202, 203, 204, 205]
df
```

#### Reordering Index (.reindex )

- Returns new DataFrame according to the order specified. NO 'inplace' option.
- Doesn't work if **duplicate** labels exists (Error)
- Only matching rows are returned. 
- Non matching index number will results in NaN, without Error.

**Change the order of Index**, always return a new dataframe


```{python}
df.reindex([203,202,300])
```

#### Rename Axis

- Example below renamed the axis of index.
- Use **`axis=0`** for row index (default), use **`axis=1`** for column index.
= Use **`inplace`** option to apply changes to the DataFrame, otherwise it will return the a new DataFrame

```{python}
df.rename_axis('super_id', axis=0, inplace=True)
df
```

### Subsetting Rows

```
dataframe.loc[ row_label       ]  # return series, single row
dataframe.loc[ row_label_list  ]  # multiple rows
dataframe.loc[ boolean_list    ]  # multiple rows

dataframe.iloc[ row_number       ]  # return series, single row
dataframe.iloc[ row_number_list  ]  # multiple rows
dataframe.iloc[ number_range     ]  # multiple rows

dataframe.sample(frac=)             # frac = 0.6 means sampling 60% of rows randomly
```

#### Sample Data

```{python, results='hold'}
df  = pd.DataFrame(data).set_index('empID')
df
df2 = pd.DataFrame(data, index = ['row2','row3','row1','row5','row4'])
df2
```
#### Using Row Label (loc)

- Non matching single row label will result in **KeyError**.
- Non matching **range** of row labels will not trigger error.

Row Label is **number**

```{python, error=TRUE}
df.loc[ 101]         # by single row label, return Series
df.loc[ [100,103] ]  # by multiple row labels, returns DataFrame
df.loc[ 100:103   ]  # by range of row labels, returns DataFrame
df.loc[ 999:9999  ]  # invalid range, No Error !!
df.loc[ 999 ]        # invalid key, KeyError
```

Row Label is **string**

```{python, error=TRUE}
df2.loc[ 'row3' ]          # by single row label, return Series
df2.loc[ ['row1','row3'] ] # by multiple row labels, returns DataFrame
df2.loc[ 'row1':'row3'  ]  # by range of row labels, return empty DataFrame because there is no row3 after row1
df2.loc[ 'row1':'row4'  ]  # by range of row labels, returns DataFrame
df2.loc[ 'row1':'baba'  ]  # Invalid range, KeyError
```

#### Using Row Number (iloc)

Multiple rows **returned as dataframe** object

```{python, results='hold', error=TRUE}
df.iloc[1]          # by single row number
df.iloc[ [0,3] ]    # by row numbers
df.iloc[  0:3  ]    # by row number range
df.iloc[ 0:999 ]    # invalid range, no error
df.iloc[999]        # invalid row, IndexError
```

#### Using Boolean List

```{python, results='hold'}
criteria = (df.salary > 30000) & (df.year1==2017)
print (criteria)
print (df.loc[criteria])
```

#### Using Expression (.query)

`.query(expr, inplace=False)`

```{python}
num1 = 31000
num2 = 2017
df.query(f'salary<={num1} and year1=={num2}')
```

#### Sampling (.sample)

Specify percentage of random rows to be returned.

```{python}
np.random.seed(15)  ## ensure consistentcy of result
df.sample(frac=0.7) ## randomly pick 70% of rows, without replacement
```

### Row Manipulation

#### Sample Data

```{python, results='hold'}
df
```

#### Append Rows

**`.append()`** has been deprecated. Refer to **Concatenating** section.

#### Drop Rows (.drop)

```.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')```

**By Row Label(s)**

```{python, results='hold'}
df.drop(index=100)                         # drop single row
df.drop(index=[100,103], columns='salary') # drop selected rows and columns
```
### Column Manipulation

#### Sample Data

```{python}
df = pd.DataFrame(data)
df
```

#### Renaming Columns

**Method 1 : Rename All Columns (.columns =)**. 

Construct the new column names, **ensure there is no missing** column names, which will result in error.

```{python}
new_columns = ['empID', 'year.1','year.2','salary', 'glamour']
df.columns = new_columns
df.head(2)
```

**Method 2 : Renaming Specific Column (.rename (columns=) )** 

- Change column name through **rename** function.  
- Support **inplace** option.
- Specify only columns that require name change.
- Non matching columns **will not** return error. That is great.

```{python}
df.rename( columns={'year.1':'year1', 'year.2':'year2'}, inplace=True)
df.head(2)
```

#### Reordering Columns

- Reordering columns is basically returning a new DataFrame with the speicified column list in order preferred.
- Refer to Sub-setting Column section.

#### Create New Column

- **New Column** will be created instantly using **[] notation**  
- **DO NOT USE dot Notation** because it is view only attribute

```{python}
df['year3'] = df.year1
df
```

#### Dropping Columns (.drop)

```
dataframe.drop( columns='column_name',    inplace=True/False)   # delete single column
dataframe.drop( columns=list_of_colnames, inplace=True/False)   # delete multiple column

dataframe.drop( index='row_label',         inplace=True/False)   # delete single row
dataframe.drop( index= list_of_row_labels, inplace=True/False)   # delete multiple rows

```

**inplace=True** means column will be deleted from original dataframe. **Default is False**, which return a copy of dataframe  

**By Column Name(s)**

```{python, results='hold'}
df.drop( columns='year1')           # drop single column
df.drop( columns=['year1','year2'])  # drop multiple columns
```

**By Column Number(s)**   

Use dataframe.columns to produce interim list of column names

```{python, results='hold'}
## before dropping columns
df
df.drop( columns=df.columns[[1,2]])  # drop second and third columns
df.drop( columns=df.columns[0:3] )   # drop first, second and third columns
```
### Subsetting Columns

**Select Single Column** Return **Series**
```
dataframe.columnName               # single column, name based, return Series object
dataframe[ single_col_name ]       # single column, name based, return Series object
```

**Select Single/Multiple Columns** Return **DataFrame**
```
dataframe    [  list_of_col_names   ]  # name based, return Dataframe object
dataframe.loc[ : , single_col_name  ]  # single column, series
dataframe.loc[ : , col_name_list    ]  # multiple columns, dataframe
dataframe.loc[ : , col_name_range   ]  # multiple columns, dataframe

dataframe.iloc[ : , col_number      ]  # single column, series
dataframe.iloc[ : , col_number_list ]  # multiple columns, dataframe
dataframe.iloc[ : , number_range    ]  # multiple columns, dataframe
```

#### Select Single Column (.loc, iloc)

Selecting single column always return as **`panda::Series`**, except using **`[[ ]]`** notation.

```{python, results='hold'}
df = pd.DataFrame(data)
df.name
df['name']
df.loc[:, 'name']
df.iloc[:, 3]
```
#### Select Multiple Columns (.loc, iloc)

Multiple columns return as **panda::Dataframe** object`  

Example below returns DataFrame with Single Column

```{python, results='hold'}
df[['name']]                # return one column dataframe
df[['name','year1']] 
df.loc[:,['name','year1']]
```

**Select Range of Columns**

```{python, results='hold'}
df.loc [ : , 'year1':'year2']  ## by range of column names
df.loc[ : , ['empID','year2']] ## select two columns only
df.iloc[ : , 1:4]              ## by range of column number
df.iloc[ : , [0,3]]            ## select two columns only
```
#### By Column Name (.filter, .reindex)

**`.filter(items=None, like=None, regex=None, axis=1)`**

**Filter by `like` - Sub string Matching**, always return DataFrame.

```{python}
df.filter( like='year', axis=1)  ## or axis = 1
df.filter( like='name')
```
**Filter by `items` - list of exact column names**

```{python}
df.filter( items=('year1','year2', 'name', 'not_exist'),  axis=1)
```
**Filter by regex = Regular Expression Matching column names**   

Select column names that contain integer.

```{python}
df.filter(regex='\d')  ## default axis=1 if DataFrame
```
**`.reindex(columns = .. )`**

- It returns a new dataframe.  There is **no inplace option** for reordering columns  
- **Non Matching** columns names will result in **NA** values


```{python, results='hold', error=TRUE}
new_colorder = [ 'salary', 'year1', 'baba']
df.reindex(columns = new_colorder)  ## Non matching column name returns as NaN
```

#### By Data Types (.select_dtypes)

Always return **panda::DataFrame**, even though only single column matches.  Allowed types are:

- number (integer and float)  
- integer / int64
- float
- datetime  
- timedelta  
- category
- bool
- object (string)

Use **`.dtypes.value_counts()`** to insepct available data types.

```{python, results='hold'}

df.dtypes.value_counts()                       ## inspect available data types
df.select_dtypes( exclude='integer')           ## exclude bool cols
df.select_dtypes( include=['number','object']) ## include only number cols
```
### Concatenating

**Concatenating Two DataFrames**

- When `ignore_index=True`, pandas will create new index in the result with 0,1,2,3...  
- It is recommended to ignore index IF the data source index is **not unique**.  
- New columns or rows will be added in the result if non-matching.

```{python, results='hold'}
my_df = pd.DataFrame(
          data= {'Id':   [10,20,30],
                 'Name': ['Aaa','Bbb','Ccc']})
#                 .set_index('Id')
                 
my_df_new = pd.DataFrame(
            data= {'Id':   [40,50],
                   'Name': ['Ddd','Eee'],
                   'Age':  [12,13]})  
                   #.set_index('Id')
                   
my_df_append  = pd.concat( [my_df, my_df_new])
my_df_noindex = pd.concat( [my_df, my_df_new], ignore_index=True)

print("Original DataFrame:\n", my_df,
      "\n\nTo Be Appended DataFrame:\n", my_df_new,
      "\n\nAppended DataFrame (index maintained):\n", my_df_append,
      "\n\nAppended DataFrame (index ignored):\n", my_df_noindex)
```
### Slicing

#### Sample Data

```{python}
df = pd.DataFrame(data).set_index('empID')
df
```
#### Getting One Cell  

**By Row Label and Column Name (loc)**

```
dataframe.loc [ row_label , col_name   ]    # by row label and column names
dataframe.loc [ bool_list , col_name   ]    # by row label and column names
dataframe.iloc[ row_number, col_number ]    # by row and column number
```

```{python, results='hold'}
df.loc [102,'year1']  ## row label 102, column 'year1'
df.iloc[2,0]          ## same result as above
```
#### Getting Range of Cells

Specify rows and columns (by individual or range)

```
dataframe.loc [ list/range_of_row_labels , list/range_col_names   ]    # by row label and column names
dataframe.iloc[ list/range_row_numbers,    list/range_col_numbers ]    # by row number
```

**By Index and Column Name (loc)**

```{python, results='hold'}
df.loc[ [101,103], ['name', 'year1'] ]  # by list of row label and column names
df.loc[  101:104 ,  'year1':'year2'  ]  # by range of row label and column names
```
**By Boolean Row and Column Names (loc)**

```{python}
b = df.year1==2017
df.loc[ b ]
```

**By Row and Column Number (iloc)**

```{python}
print (df.iloc[ [1,4], [0,3]],'\n' )   # by individual rows/columns
print (df.iloc[  1:4 ,  0:3], '\n')    # by range
```
### Chained Indexing

**Chained Index** Method creates a copy of dataframe, any modification of data on original dataframe does not affect the copy  

```
dataframe.loc  [...]  [...]
dataframe.iloc [...]  [...]
```
Suggesting, **never use** chain indexing

```{python}
df = pd.DataFrame(
    { 'empID':  [100,      101,    102,      103,     104],
      'year1':   [2017,     2017,   2017,      2018,    2018],
      'name':   ['Alice',  'Bob',  'Charles','David', 'Eric'],
      'year2':   [2001,     1907,   2003,      1998,    2011],
      'salary': [40000,    24000,  31000,     20000,   30000]},
    columns = ['year1','salary','year2','empID','name']).set_index(['empID'])
df
```

```{python}
df.loc[100]['year'] =2000
df  ## notice row label 100 had not been updated, because data was updated on a copy due to chain indexing
```
### Find and Replace

Slicing deals with square cells selection. Use `mask` or `where` to select specific cell(s). These function respect column and row names.

#### `mask()`

`mask()` replace cell value with `other=` when condition is met.

```{python, results='hold'}
data={'x': [1,4,7],
      'y': [2,5,8],
      'z': [3,6,9]}
df = pd.DataFrame(data)
df.mask(df>4, other=999)
```
#### `where()`

This is reverse of `mask()`, it keep cell value where  **condition is met**. If not met, replace with `other` value.

```{python}
df.where(df>4, other=0)
```

### Iteration

#### Iterating Rows (.iterrows)

```{python}
df = pd.DataFrame(data=
    { 'empID':  [100,      101,    102,      103,     104],
      'Name':   ['Alice',  'Bob',  'Charles','David', 'Eric'],
      'Year':   [1999,     1988,   2001,     2010,     2020]}).set_index(['empID'])

for idx, row in df.iterrows():
  print(idx, row.Name)
```

#### Iterating Columns (.items)

```{python}
for label, content in df.items():
  print('Column Label:',              label,   '\n\n', 
        'Column Content (Series):\n', content, '\n\n')
```
### Data Structure

#### Inspect Structure

Find out the column names, data type in a summary. Output is for display only, not a data object

```{python}
df.info()  # return text output
```

```{python}
df.dtypes.value_counts() # return Series
```

#### Format Conversion

**`.to_dict()`**

```{python}
df.to_dict('dict')    ## dict of dict by column
df.to_dict('index')   ## dict of dict by row index
df.to_dict('list')    ## dict of list
df.to_dict('records') ## list of dict
```
### Import/Export

#### CSV

```{python}
df.to_csv()
```

## Class: MultiIndex

MultiIndexing are columns with few levels of headers.

### The Data

```{python}
df = pd.DataFrame({
     'myindex': [0, 1, 2],
     'One_X':   [1.1,  1.1,  1.1],
     'One_Y':   [1.2,  1.2,  1.2],
     'Two_X':   [1.11, 1.11, 1.11],
     'Two_Y':   [1.22, 1.22, 1.22]})
df.set_index('myindex',inplace=True)
df
```

### Creating MultiIndex Object

#### Create From Tuples

MultiIndex can easily created from typles:  
- Step 1: Create a MultiIndex object by splitting column name into tuples  
- Step 2: Assign the MultiIndex Object to dataframe `columns` property.  

```{python}
my_tuples = [tuple(c.split('_')) for c in df.columns]
df.columns = pd.MultiIndex.from_tuples(my_tuples)

print(' Column Headers :\n\n',           my_tuples,
        '\n\nNew Columns: \n\n',         df.columns,
        '\n\nTwo Layers Header DF:\n\n', df)
```



### MultiIndex Object

#### Levels

- MultiIndex object contain multiple leveels, each level (header) is an Index object.   
- Use **`MultiIndex.get_level_values()`** to the entire header for the desired level. Note that each level is an Index object

```{python}
print(df.columns.get_level_values(0), '\n',
      df.columns.get_level_values(1))
```

**`MultiIndex.levels`** return the **unique values** of each level.

```{python}
print(df.columns.levels[0], '\n',
      df.columns.levels[1])
```

#### Convert MultiIndex Back To Tuples

```{python}
df.columns.to_list()
```

### Selecting Column(s)

#### Sample Data

```{python}
import itertools
test_df = pd.DataFrame
max_age = 100

### Create The Columns Tuple
level0_sex = ['Male','Female','Pondan']
level1_age = ['Medium','High','Low']
my_columns = list(itertools.product(level0_sex, level1_age))

test_df = pd.DataFrame([
             [1,2,3,4,5,6,7,8,9],
             [11,12,13,14,15,16,17,18,19],
             [21,22,23,24,25,26,27,28,29]], index=['row1','row2','row3'])

### Create Multiindex From Tuple
test_df.columns = pd.MultiIndex.from_tuples(my_columns)
print( test_df ) 
```

#### Select Level0 Header(s)

Use **`[L0]` notation**, where `L0` is list of header names

```{python}
print( test_df[['Male','Pondan']] ,'\n\n',  ## Include multiple Level0 Header
       test_df['Male'] ,          '\n\n',   ## Include single Level0 Header
       test_df.Male )                       ## Same as above
```

**Using `.loc[]`**

Use  **`.loc[ :, L0 ]`**, where `L0` is list of headers names

```{python}
print( test_df.loc[:, ['Male','Pondan']] , '\n\n',  ## Multiple Level0 Header
       test_df.loc[:, 'Male'] )                     ## Single Level0 Header
```

#### Selecting Level 1 Header(s)

Use  **`.loc[ :, (All, L1)]`**, where `L1` are list of headers names

```{python}
All = slice(None)
print( test_df.loc[ : , (All, 'High')],  '\n\n',  ## Signle L1 header
       test_df.loc[ : , (All, ['High','Low'])] )  ## Multiple L1 headers
```

#### Select Level 0 and Level1 Headers

Use  **`.loc[ :, (L0, L1)]`**, where `L0` and `L1` are list of headers names

```{python}
test_df.loc[ : , (['Male','Pondan'], ['Medium','High'])]
```

#### Select single L0,L1 Header

Use **`.loc[:, (L0,  L1) ]`**, result is a **Series**  
Use **`.loc[:, (L0 ,[L1])]`**, result is a **DataFrame**

```{python}
print( test_df.loc[ : , ('Female', 'High')], '\n\n',
       test_df.loc[ : , ('Female', ['High'])])
```

### Headers Ordering

Note that columns **order** specifeid by `[ ]` selection were not respected. This can be remediated either by Sorting and rearranging.

#### Sort Headers

Use `.sort_index()` on DataFrame to sort the headers. Note that when level1 is sorted, it jumble up level0 headers.

```{python}
test_df_sorted_l0 = test_df.sort_index(axis=1, level=0)
test_df_sorted_l1 = test_df.sort_index(axis=1, level=1, ascending=False)
print(test_df, '\n\n',test_df_sorted_l0, '\n\n', test_df_sorted_l1)
```
#### Rearranging Headers

Use **`.reindex()**` on arrange columns in specific order. Example below shows how to control the specific order for level1 headers.

```{python}
cats = ['Low','Medium','High']
test_df.reindex(cats, level=1, axis=1)
```

### Stacking and Unstacking

```{python}
df.stack()
```

#### Stacking Columns to Rows

Stacking with **`DataFrame.stack(level_no)`** is moving wide columns into row.

```{python}
print('Stacking Header Level 0: \n\n', df.stack(0),
      '\n\nStacking Header Level 1: \n\n', df.stack(1))
```

### Exploratory Analysis


#### Sample Data

```{python}
df
```


#### All Stats in One  - .describe()


```
df.describe(include='number') # default
df.describe(include='object') # display for non-numeric columns
df.describe(include='all')    # display both numeric and non-numeric
```

When applied to DataFrame object, describe shows all **basic statistic** for **all numeric** columns:
- Count (non-NA)  
- Unique (for string)  
- Top (for string)   
- Frequency (for string)  
- Percentile  
- Mean  
- Min / Max  
- Standard Deviation  


**For Numeric Columns only**  
You can **customize the percentiles requred**. Notice 0.5 percentile is always there although not specified

```{python}
df.describe()
```

```{python}
df.describe(percentiles=[0.9,0.3,0.2,0.1])
```


**For both Numeric and Object**

```{python}
df.describe(include='all')
```


#### min/max/mean/median

```{python}
df.min()  # default axis=0, column-wise
```

```{python}
df.min(axis=1) # axis=1, row-wise
```


Observe, sum on **string will concatenate column-wise**, whereas row-wise only sum up numeric fields

```{python}
df.sum(0)
```

```{python}
df.sum(1)
```


### Plotting

```{python}

```

```{python}

```


## Class: Categories

### Creating

#### From List
**Basic (Auto Category Mapping)**  
Basic syntax return categorical index with sequence with code 0,1,2,3... mapping to first found category   
In this case, **low(0), high(1), medium(2)**

```{python}
temp = ['low','high','medium','high','high','low','medium','medium','high']
temp_cat = pd.Categorical(temp)
temp_cat
```

```{python}
type( temp_cat )
```


**Manual Category Mapping**  
During creation, we can specify mapping of codes to category: **low(0), medium(1), high(2)**

```{python}
temp_cat = pd.Categorical(temp, categories=['low','medium','high'])
temp_cat
```


#### From Series
- We can 'add' categorical structure into a Series. With these methods, additional property (.cat) is added as a **categorical accessor**  
- Through this accessor, you gain access to various properties of the category such as .codes, .categories. But not .get_values() as the information is in the Series itself  
- Can we manual map category ?????

```{python}
temp = ['low','high','medium','high','high','low','medium','medium','high']
temp_cat = pd.Series(temp, dtype='category')
print (type(temp_cat))       # Series object
print (type(temp_cat.cat))   # Categorical Accessor
```


- Method below has the same result as above by using **.astype('category')**  
- It is useful adding category structure into existing series.

```{python}
temp_ser = pd.Series(temp)
temp_cat = pd.Series(temp).astype('category')
print (type(temp_cat))       # Series object
print (type(temp_cat.cat))   # Categorical Accessor
```

```{python}
temp_cat.cat.categories
```


#### Ordering Category

```{python}
temp = ['low','high','medium','high','high','low','medium','medium','high']
temp_cat = pd.Categorical(temp, categories=['low','medium','high'], ordered=True)
temp_cat
```

```{python}
temp_cat.codes
```

```{python}
temp_cat[0] < temp_cat[3]
```

<!-- jupyter_markdown, jupyter_meta = list(heading_collapsed = TRUE) -->
### Properties

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### .categories
first element's code = 0  
second element's code = 1  
third element's code = 2

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat.categories
```

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### .codes
Codes are actual **integer** value stored as array. 1 represent 'high', 

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat.codes
```

<!-- jupyter_markdown, jupyter_meta = list(heading_collapsed = TRUE) -->
### Rename Category

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### Renamce To New Category Object
**.rename_categories()** method return a new category object with new changed categories

```{python, jupyter_meta = list(hidden = TRUE)}
temp = ['low','high','medium','high','high','low','medium','medium','high']
new_temp_cat = temp_cat.rename_categories(['sejuk','sederhana','panas'])
new_temp_cat 
```

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat   # original category object categories not changed
```

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### Rename Inplace
Observe the original categories had been changed using **.rename()**

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat.categories = ['sejuk','sederhana','panas']
temp_cat   # original category object categories is changed
```

<!-- jupyter_markdown, jupyter_meta = list(heading_collapsed = TRUE) -->
### Adding New Category
This return a new category object with added categories

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat_more = temp_cat.add_categories(['susah','senang'])
temp_cat_more
```

<!-- jupyter_markdown, jupyter_meta = list(heading_collapsed = TRUE) -->
### Removing Category
This is **not in place**, hence return a new categorical object  

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### Remove Specific Categor(ies)
Elements with its category removed will become **NaN**

```{python, jupyter_meta = list(hidden = TRUE)}
temp = ['low','high','medium','high','high','low','medium','medium','high']
temp_cat = pd.Categorical(temp)
temp_cat_removed = temp_cat.remove_categories('low')
temp_cat_removed
```

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### Remove Unused Category
Since categories removed are not used, there is no impact to the element

```{python, jupyter_meta = list(hidden = TRUE)}
print (temp_cat_more)
temp_cat_more.remove_unused_categories()
```

<!-- jupyter_markdown, jupyter_meta = list(heading_collapsed = TRUE) -->
### Add and Remove Categories In One Step - Set()

```{python, jupyter_meta = list(hidden = TRUE)}
temp = ['low','high','medium','high','high','low','medium','medium','high']
temp_cat = pd.Categorical(temp, ordered=True)
temp_cat
```

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat.set_categories(['low','medium','sederhana','susah','senang'])
```

<!-- jupyter_markdown, jupyter_meta = list(heading_collapsed = TRUE) -->
### Categorical Descriptive Analysis 

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### At One Glance

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat.describe()
```

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### Frequency Count

```{python, jupyter_meta = list(hidden = TRUE, scrolled = TRUE)}
temp_cat.value_counts()
```

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### Least Frequent Category, Most Frequent Category, and Most Frequent Category

```{python, jupyter_meta = list(hidden = TRUE)}
( temp_cat.min(), temp_cat.max(), temp_cat.mode() )
```


### Other Methods


#### .get_values()
Since actual value stored by categorical object are integer **codes**, get_values() function return values translated from *.codes** property

```{python}
temp_cat#array
```


## Dummies

- **get_dummies** creates columns for each categories 
- The underlying data can be string or pd.Categorical  
- It produces a **new pd.DataFrame**

### Sample Data

```{python}
df = pd.DataFrame (
    {'A': ['A1','A2','A3','A1','A3','A1'], 
     'B': ['B1','B2','B3','B1','B1','B3'],
     'C': ['C1','C2','C3','C1',np.nan,np.nan]})
df
```


### Dummies on Array-Like Data

```{python}
pd.get_dummies(df.A)
```


### Dummies on DataFrame (multiple columns)

#### All Columns

```{python}
pd.get_dummies(df)
```


#### Selected Columns

```{python}
cols = ['A','B']
pd.get_dummies(df[cols])
```


### Dummies with na

By default, nan values are ignored

```{python}
pd.get_dummies(df.C)
```


**Make NaN as a dummy variable**

```{python}
pd.get_dummies(df.C,dummy_na=True)
```


### Specify Prefixes

```{python}
pd.get_dummies(df.A, prefix='col')
```

```{python}
pd.get_dummies(df[cols], prefix=['colA','colB'])
```


### Dropping First Column
- Dummies cause **colinearity issue** for regression as it has redundant column.  
- Dropping a column **does not loose any information** technically

```{python}
pd.get_dummies(df[cols],drop_first=True)
```

## DataFrameGroupBy

- `groupby()` is a DataFrame method, it returns  **`DataFrameGroupBy`** object  
- **`DataFrameGroupBy`** object open doors for dataframe aggregation and summarization  
- **`DataFrameGroupBy`** object is a **very flexible abstraction**. In many ways, you can simply treat `DataFrameGroup` as if it's a **collection of DataFrames**, and it does the difficult things under the hood  


### Sample Data

```{python}
company = pd.read_csv('data/company.csv')
company
```

### Creating Groups

Group can be created for  **single or multiple** columns

```{python}
com_grp = company.groupby('Company') ## Single Column
com_dep_grp = company.groupby(['Company','Department'])  ## Multiple Column
type(com_dep_grp)
```

### Properties

#### Number of Groups

```{python}
com_dep_grp.ngroups
```

#### Row Numbers Association

`.groups` property is a dictionary containing group key (identifying the group) and its values (underlying row indexes for the group)

```{python}
gdict = com_dep_grp.groups       # return Dictionary
print( gdict.keys()   , '\n\n',  # group identifier
       gdict.values()   )        # group row indexes
```

### Methods

#### Number of Rows In Each Group

```{python}
com_dep_grp.size()  # return panda Series object
```

### Retrieve Rows

#### Retrieve n-th Row Of Each Grou

- Row number is 0-based  
- For First row, use `.first()` or `nth(0)`  

```{python}
print( com_dep_grp.nth(0)  , '\n',
       com_dep_grp.first())
```

- For Last row,  use `.last()` or `nth(`-1)`

```{python}
print( com_dep_grp.nth(-1)  , '\n',
       com_dep_grp.last())
```

#### Retrieve N Rows Of Each Groups

Example below retrieve 2 rows from each group

```{python}
com_dep_grp.head(2)
```

#### Retrieve All Rows Of  Specific Group

`get_group()` retrieves all rows within the specified group.

```{python}
com_dep_grp.get_group(('C1','D3'))
```

### Single Statistic Per Group

#### `count()` 

`count()` for valid data (not null) for each fields within the group

```{python}
com_dep_grp.count()  # return panda DataFrame object
```

#### `sum()`

This sums up all numeric columns for each group

```{python}
com_dep_grp.sum()
```

To sum specific columns of each group, use `['columnName']` to select the column.  
When single column is selected, output is a **Series**

```{python}
com_dep_grp['Age'].sum()
```

#### `mean()`

This average up all numeric columns for each group

```{python}
com_dep_grp.mean()
```

To average specific columns of each group, use `['columnName']` to select the column.  
When single column is selected, output is a **Series**

```{python}
com_dep_grp['Age'].mean()
```

### Multi Statistic Per Group

#### Single Function To Column(s)

- Instructions for aggregation are provided in the form of a dictionary. Dictionary keys specifies the **column name**, and value as the **function** to run  
- Can use **`lambda x:`** to customize the calclulation on entire column (x)  
- Python built-in function names does can be supplied without wrapping in string `'function'`

```{python}
com_dep_grp.agg({
  'Age': sum ,                 ## Total age of the group
  'Salary': lambda x: max(x),  ## Highest salary of the group
  'Birthdate': 'first'         ## First birthday of the group
})
```

#### Multiple Function to Column(s)

- Use list of function names to specify functions to be applied on a particular column  
- Notice that output columns are MultiIndex , indicating the name of funcitons appled on level 1  

```{python}
ag = com_dep_grp.agg({
      'Age': ['mean', sum ],       ## Average age of the group
      'Salary': lambda x: max(x),  ## Highest salary of the group
      'Birthdate': 'first'         ## First birthday of the group
    })
    
print (ag, '\n\n', ag.columns)
```

#### Column Relabling

Introduced in Pandas 0.25.0, groupby aggregation with relabelling is supported using “named aggregation” with **simple tuples**

```{python}
com_dep_grp.agg(
  max_age     = ('Age', max),
  salary_m100 = ('Salary',  lambda x: max(x)+100),  
  first_bd    = ('Birthdate', 'first')
)
```

### Iteration

**DataFrameGroupBy** object can be thought as a collection of named groups

```{python}
def print_groups (g):
    for name,group in g:
        print (name)
        print (group[:2])
        
print_groups (com_grp)
```

```{python}
com_grp
```

### Transform

- Transform is an operation used combined with **DataFrameGroupBy** object  
- **transform()** return a **new DataFrame object**  

```{python}
grp = company.groupby('Company')
grp.size()
```


**transform()** perform a function to a group, and **expands and replicate** it to multiple rows according to original DataFrame

```{python}
grp[['Age','Salary']].transform('sum')
```

```{python}
grp.transform( lambda x:x+10 )
```

## Fundamental Analysis

## Missing Data

### Sample Data

```{python}
df = pd.DataFrame( np.random.randn(5, 3), 
                   index   =['a', 'c', 'e', 'f', 'h'],
                   columns =['one', 'two', 'three'])
df['four'] = 'bar'
df['five'] = df['one'] > 0
#df
df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])
```

**How Missing Data For Each Column ?**

```{python}
df.count()
```

```{python}
len(df.index) - df.count()
```

```{python}
df.isnull()
```

```{python}
df.describe()
```


